@INPROCEEDINGS{9499395,
author={Marru, Suresh and Kuruvilla, Tanya and Abeysinghe, Eroma and McMullen, Donald and Pierce, Marlon and Morgan, David Gene and Tait, Steven L. and Innes, Roger W.},
booktitle={2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)}, title={User-Centric Design and Evolvable Architecture for Science Gateways: A Case Study},
year={2021},
volume={},
number={},
pages={267-276},
abstract={Scientific applications built on wide-area distributed systems such as emerging cloud based architectures and the legacy grid computing infrastructure often struggle with user adoption even though they succeed from a systems research perspective. This paper examines the coupling of user-centered design processes with modern distributed systems. Further in this paper, we describe approaches for conceptualizing a product that solves a recognized need: to develop a data gateway to serve the data management and research needs of experimentalists of electron microscopes and similar shared scientific instruments in the context of a research service laboratory. The purpose of the data gateway is to provide secure, controlled access to data generated from a wide range of scientific instruments. From the functional perspective, we focus on the basic processing of raw data that underlies the lab’s "business" processes, the movement of data from the laboratory to central access and archival storage points, and the distribution of data to respective authorized users. Through the gateway interface, users will be able to share the instrument data with collaborators or copy it to remote storage servers. Basic pipelines for extracting additional metadata (through a pluggable parser framework) will be enabled. The core contribution described in this paper, building on the aforementioned distributed data management capabilities, is the adoption of user-centered design processes for developing the scientific user interface. We describe the user-centered design methodology for exploring user needs, iteratively testing the design, learning from user experiences, and adapting what we learn to improve design and capabilities. We further conclude that user-centered design is, in turn, best enabled by an adaptable distributed systems framework. A key challenge to implementing a user-centered design is to have design tools closely linked with a software system architecture that can evolve over time while providing a highly available data gateway. A key contribution of this paper is to share the insights from crafting such an evolvable design-build-evaluate-deploy architecture and plans for iterative development and deployment.},
keywords={Cloud computing;Architecture;User centered design;Buildings;Computer architecture;Logic gates;User interfaces;Apache Airavata;User Centered Design;Flexible Architecture;Instrument Gateways;LIMS},
doi={10.1109/CCGrid51090.2021.00036},
ISSN={},
month={May},}
@INPROCEEDINGS{9498005,
author={Li, Jie and Tian, Deliang and Ding, Xiaobing and Yu, Jiang and Li, Xingjian},
booktitle={2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)}, title={The development and application of a new combined-distributed HVDC tester},
year={2021},
volume={},
number={},
pages={617-621},
abstract={In view of the present lack of debugging method for HVDC protection system, this paper proposes the method to a single device function test and the method to a distributed system test for HVDC protection system, and designs a new combined-distributed HVDC tester to realize the above two test methods. The HVDC tester includes management module, power amplifier module, I/O module and optical fiber Ethernet Module, fiber FT3 module, power supply module, and so on. Various modules include internal bus interface, general power supply interface, external fiber interface, and through the internal bus interface and general power supply interface, all modules are combined in a chassis to form a combined tester. Through the external fiber interface and general power supply interface, these modules can also form a distributed test system. The combined tester is convenient to carry, store and manage, mainly for the function and performance test of a single HVDC protection device, while the distributed tester is mainly used to fulfill the distributed system test on site. This flexible combination method not only reduces the cost and volume of tester, but also provides affluent interfaces and flexible application. Engineering practices show that the device fully meets the requirements of HVDC protection maintenance and operation on site.},
keywords={Optical fiber amplifiers;Performance evaluation;Protocols;Power supplies;HVDC transmission;Debugging;Maintenance engineering;HVDC;DC protection system;Function test;Distributed system test},
doi={10.1109/ICAICA52286.2021.9498005},
ISSN={},
month={June},}
@INPROCEEDINGS{9460234,
author={Pedro Furriel, João and Pereira, Eliseu and Reis, Joao and Gonçalves, Gil},
booktitle={2021 10th Mediterranean Conference on Embedded Computing (MECO)}, title={Jurassic Park - A Centralized Software Modules Repository for IoT Devices},
year={2021},
volume={},
number={},
pages={1-4},
abstract={One of the key issues in Cyber- Physical Productions Systems (CPPSs) is the software re-utilization across all the devices. Also, the lack of programming knowledge from factory managers has led to the introduction of the IEC61499 visual programming paradigm, where each block functionality is abstracted into function blocks (FBs), which can be reused and deployed to a distributed system. That standard facilitates the management and development of CPPS, combining FBs into complex pipelines, deployed in distributed devices. However, the implementation of this paradigm to address the aforementioned problem brings itself, other necessities such as the FBs management, execution monitorization, and their previous download by the embedded devices. This work's main goal is the development of a centralized platform, Jurassic Park, to manage and monitor FBs. Jurassic Park will enable the automatic distribution of FBs among the embedded devices in a CPPS, functioning as a central repository of software components, and having monitoring features, that allow the detection of flaws or malfunctions on those components. In the end, several tests have been performed to validate Jurassic Park, namely usability, monitoring, and performance tests.},
keywords={Performance evaluation;Visualization;Production systems;Pipelines;Programming;Software;Production facilities;Internet of Things;IEC 61499;Software Repository;Function Blocks},
doi={10.1109/MECO52532.2021.9460234},
ISSN={2637-9511},
month={June},}
@INPROCEEDINGS{9461114,
author={Paulavičius, Remigijus and Grigaitis, Saulius and Filatovas, Ernestas},
booktitle={2021 IEEE International Conference on Blockchain and Cryptocurrency (ICBC)}, title={An Overview and Current Status of Blockchain Simulators},
year={2021},
volume={},
number={},
pages={1-3},
abstract={Since the introduction of Bitcoin, blockchain has attracted tremendous interest from both academia and industry. During the last decade, various large-scale blockchain systems were developed. However, the complexity of large-scale distributed systems makes the performance evaluation process challenging and costly. Here, blockchain simulators give the possibility to repeat complex real-world processes at a low cost. Simulators are easily extensible and can test distributed ledger performance using different settings and parameter variations. This work reviews and summarizes the current status of the state-of-the-art blockchain simulators.},
keywords={Performance evaluation;Industries;Distributed ledger;Conferences;Blockchain;Consensus algorithm;Bitcoin;Blockchain;Bitcoin;simulators;review},
doi={10.1109/ICBC51069.2021.9461114},
ISSN={},
month={May},}
@INPROCEEDINGS{9453493,
author={Prabhakara, Arpitha and Steinwender, Benjamin and Elmenreich, Wilfried},
booktitle={2021 22nd IEEE International Conference on Industrial Technology (ICIT)}, title={Statistical analysis of execution time profile for temporal validation of a distributed hard real-time system},
year={2021},
volume={1},
number={},
pages={1188-1192},
abstract={This paper addresses modelling and assessing of worst-case execution times of tasks in a distributed real-time system. The presented case study is based on a stress test system for power semiconductors.Typical distributed systems involve inherent inter-dependencies that have to be handled with minimized delays. Studying the computing task’s timing behavior becomes necessary to understand the timing violations. Modelling the temporal behaviour of the system using probabilistic Worst-Case Execution Time (WCET) analysis aids to overcome the timing faults.This paper focuses on (1) the study of the temporal behaviour through the execution time profile of the distributed hard real-time system; (2) the statistical analysis by performing probability distribution modelling on the measured data. Measurement-based probabilistic timing analysis is an emerging and reliable method used to arrive at industry quality estimates. This method is used here in the paper to carry out temporal validation of the real-time computing tasks on our case study.},
keywords={Semiconductor device measurement;Analytical models;Statistical analysis;Computational modeling;Probabilistic logic;Real-time systems;Data models;Real-Time Distributed System;Timing Failure;WCET Analysis;Measurement-based Probabilistic Timing Anal-ysis;Statistical Analysis;Probability Distribution;Fault-Tolerant System},
doi={10.1109/ICIT46573.2021.9453493},
ISSN={},
month={March},}
@INPROCEEDINGS{9385244,
author={Klymash, Mykhailo and Chaikovskyi, Ihor and Syvkova, Nataliia and Hordiichuk-Bublivska, Olena and Kyryk, Marian},
booktitle={2021 IEEE 16th International Conference on the Experience of Designing and Application of CAD Systems (CADSM)}, title={Research of Distributed Data Processing in Corporate Information Systems},
year={2021},
volume={},
number={},
pages={24-27},
abstract={This article presents the results of research on the features and development of distributed information systems technology. For that purpose, a distributed system based on Java technologies was implemented. A brief description of the concept and types of a distributed system, as well as an analysis of its architecture, is given. Also, the application of the singular decomposition algorithm for processing user data in corporate systems is proposed. To test the proposed methods of data processing built a software model of a distributed system using the Java programming language and performed a comparison of the duration of data processing by users' preferences of films. The obtained results testify to the advantages of using distributed systems for more efficient data processing of large volumes and improving the quality of service provision in corporate systems.},
keywords={Solid modeling;Java;Software algorithms;Data processing;Software;Data models;Sparks;Distributed Systems;Corporate information systems (CIS);Spark},
doi={10.1109/CADSM52681.2021.9385244},
ISSN={2572-7591},
month={Feb},}
@INPROCEEDINGS{9376112,
author={Ozcelik, Ilker and Skjellum, Anthony},
booktitle={2021 IEEE 11th Annual Computing and Communication Workshop and Conference (CCWC)}, title={CryptoRevocate: A Cryptographic Accumulator based Distributed Certificate Revocation List},
year={2021},
volume={},
number={},
pages={0865-0872},
abstract={Verification of the certificate revocation status is a crucial process for Public Key Infrastructure (PKI) system reliability. Failing to detect a revoked certificate may lead to catastrophic system compromises. Existing verification systems use slow and centralized approaches like Certificate Revocation Lists (CRLs) and Online Certificate Status Protocol (OCSP). These systems are known to cause verification failures (soft fails) because of system and network delays. Additionally, the availability of these systems are a major concern. Recent developments in distributed ledger (blockchain) technologies enable this information to be reliably published on the Internet, in a distributed manner. However in a distributed system, synchronizing large amounts of data among the nodes is an expensive task. One way to combat this issue is to use cryptographic accumulators, a tool that can be used to reduce data size; when only membership test statuses are necessary from a set of data. In this study, we focus on the reliable and effective distribution of certificate revocation information. We present a design of an asymmetric cryptographic accumulator based effective certificate revocation system. To the best of our knowledge, this is the first study using asymmetric cryptographic accumulators to distribute certificate revocation data via blockchain.},
keywords={Protocols;Conferences;Public key;Blockchain;Tools;Reliability;Task analysis;Certificate Revocation;Cryptographic accumulator;PKI;Blockchain},
doi={10.1109/CCWC51732.2021.9376112},
ISSN={},
month={Jan},}
@ARTICLE{9366733,
author={Paulavičius, Remigijus and Grigaitis, Saulius and Filatovas, Ernestas},
journal={IEEE Access}, title={A Systematic Review and Empirical Analysis of Blockchain Simulators},
year={2021},
volume={9},
number={},
pages={38010-38028},
abstract={In recent years, distributed ledger technologies, and especially blockchain, have gained tremendous interest from academia, government, and industry. Although various blockchain-based solutions were created, the lack of tools to evaluate these complex distributed systems may hinder the development of the field. Many advantages of blockchain systems can be demonstrated only at large scales, e.g., using thousands of nodes. An investigation of different implementations and design choices is complicated and hardly feasible on real systems. Meanwhile, blockchain simulators give the possibility to repeat the complex real-world processes at a low cost. This work provides the first and an up-to-date systematic review and empirical analysis of blockchain simulators. Simulators are easily extensible and can test the performance of distributed ledgers using different settings and parameters on a single computer. The features and limitations of selected simulators are summarized and experimentally validated. Finally, recommendations for potential future research directions in the field are provided.},
keywords={Blockchain;Distributed ledger;Peer-to-peer computing;Mathematical model;Bitcoin;Systematics;Performance evaluation;Bitcoin;distributed ledger technology;blockchain;simulators;systematic review},
doi={10.1109/ACCESS.2021.3063324},
ISSN={2169-3536},
month={},}
@ARTICLE{9238490,
author={Li, Shupeng and Qing, Ting and Fu, Jianbin and Wang, Xiangchuan and Pan, Shilong},
journal={Journal of Lightwave Technology}, title={High-Accuracy Optical Fiber Transfer Delay Measurement Using Fiber-Optic Microwave Interferometry},
year={2021},
volume={39},
number={2},
pages={627-632},
abstract={Optical fiber transfer delay (OFTD) measurement with high accuracy and stability is an urgent demand for many applications such as fiber-optic sensors and fiber-based distributed systems. In this article, we propose a novel method using fiber-optic microwave interferometry to meet the above practice demand. Two incoherent optical carriers with different wavelengths are coupled into an intensity modulator driven by a microwave signal. The intensity-modulated signal is then divided into two portions through a dense wavelength division multiplexer. One portion directly passes through the reference path while the other undergoes the transfer delay of a fiber under test (FUT). After photo-detection, two probe signals that undergo different delays are recovered and superimposed. By sweeping the microwave frequency, periodic microwave interference fringe is generated. Then, OFTD measurement is achieved by measuring the frequency of the last valley in the interference fringe. Experimental results show that a system stability of ±0.02 ps, an accuracy of ±0.07 ps, and a measurement range of at least 500 m are achieved.},
keywords={Optical attenuators;Microwave measurement;Frequency measurement;Optical fibers;Optical fiber sensors;Microwave theory and techniques;Optical interferometry;Fiber-optic microwave interferometry;microwave measurement;optical transfer delay measurement},
doi={10.1109/JLT.2020.3033280},
ISSN={1558-2213},
month={Jan},}
@ARTICLE{9212559,
author={Cao, Hui and Yang, Xing and Deng, Raoyi},
journal={IEEE Transactions on Automation Science and Engineering}, title={Ontology-Based Holonic Event-Driven Architecture for Autonomous Networked Manufacturing Systems},
year={2021},
volume={18},
number={1},
pages={205-215},
abstract={The current trends in personalized products drive the paradigms of production systems toward autonomous networked manufacturing systems. This article proposes an ontology-based holonic event-driven architecture for implementing loosely coupled, holonic, autonomous distributed systems. The event-driven architecture (EDA) enables the services provided by different organizations and their suborganizations to be autonomously configured and integrated, while ensuring that the predefined organizational event access rules are compulsorily followed. This is realized by organizing the event services in a holonic manager and modeling the knowledge of services via the ontology developed in this study. The ontology model and the corresponding autonomous configuration mechanism are introduced. We developed a demonstration case to elaborate our approach and implemented a testbed using Java-based technology for testing the effectiveness, efficiency, scalability, and reliability of the proposed approach. Note to Practitioners-The ever-changing customer demands, the volatile market, and the trend of mass personalization require manufacturing systems to be smart in the event of uncertainties. To this end, information sharing in the supply chain is a key factor. However, there are several barriers in implementing a cross-organizational information system, including security, access privileges, and system interoperability. To lift these barriers, this article proposes an ontology model and a hierarchical EDA. Compared with conventional architectures, our approach facilitates the management of access privileges, integration of interorganizational information systems, and their interoperation. Moreover, our approach is competitive compared with the conventional service-oriented architecture in terms of both scalability and reliability of the system. Before the proposed approach can be practically applied, the ontology model needs to be further extended, and the event registration and message exchange protocol need to be standardized.},
keywords={Ontologies;Manufacturing systems;Organizations;Interoperability;Service-oriented architecture;Computer architecture;Autonomous configuration;event-driven architecture (EDA);networked manufacturing system (NMS);ontology},
doi={10.1109/TASE.2020.3025784},
ISSN={1558-3783},
month={Jan},}
@ARTICLE{9207857,
author={Liu, Xueqiao and Yang, Guomin and Susilo, Willy and Tonien, Joseph and Liu, Ximeng and Shen, Jian},
journal={IEEE Transactions on Parallel and Distributed Systems}, title={Privacy-Preserving Multi-Keyword Searchable Encryption for Distributed Systems},
year={2021},
volume={32},
number={3},
pages={561-574},
abstract={As cloud storage has been widely adopted in various applications, how to protect data privacy while allowing efficient data search and retrieval in a distributed environment remains a challenging research problem. Existing searchable encryption schemes are still inadequate on desired functionality and security/privacy perspectives. Specifically, supporting multi-keyword search under the multi-user setting, hiding search pattern and access pattern, and resisting keyword guessing attacks (KGA) are the most challenging tasks. In this article, we present a new searchable encryption scheme that addresses the above problems simultaneously, which makes it practical to be adopted in distributed systems. It not only enables multi-keyword search over encrypted data under a multi-writer/multi-reader setting but also guarantees the data and search pattern privacy. To prevent KGA, our scheme adopts a multi-server architecture, which accelerates search response, shares the workload, and lowers the key leakage risk by allowing only authorized servers to jointly test whether a search token matches a stored ciphertext. A novel subset decision mechanism is also designed as the core technique underlying our scheme and can be further used in applications other than keyword search. Finally, we prove the security and evaluate the computational and communication efficiency of our scheme to demonstrate its practicality.},
keywords={Encryption;Servers;Cloud computing;Keyword search;Public key;Data privacy;Searchable encryption;multi-keyword search;multi-user access;search pattern;access pattern},
doi={10.1109/TPDS.2020.3027003},
ISSN={1558-2183},
month={March},}
@ARTICLE{8888185,
author={Zhao, Kangfei and Su, Jiao and Yu, Jeffrey Xu and Zhang, Hao},
journal={IEEE Transactions on Knowledge and Data Engineering}, title={SQL-G: Efficient Graph Analytics by SQL},
year={2021},
volume={33},
number={5},
pages={2237-2251},
abstract={Querying graphs and conducting graph analytics become important in data processing since many real applications are dealing with massive graphs, such as online social networks, Semantic Web, knowledge graphs, etc. Over the years, many distributed graph processing systems have been developed to support graph analytics using various programming models, and many graph querying languages have been proposed. A natural question that arises is how to integrate graph data and traditional non-graph data in a distributed system for users to conduct analytics. There are two issues. One issue is related to expressiveness on how to specify graph analytics as well as data analytics by a querying language. The other issue is related to efficiency on how to process analytics in a distributed system. For the first issue, SQL is a best candidate, since SQL is a well-accepted language for data processing. We concentrate on SQL for graph analytics. Our early work shows that graph analytics can be supported by SQL in a way from “semiring + while” to “relational algebra + while” via the enhanced recursive SQL queries. In this article, we focus on the second issue on how to process such enhanced recursive SQL queries based on the GAS (Gather-Apply-Scatter) model under which efficient graph processing systems can be developed. To demonstrate the efficiency, we implemented a system by tightly coupling Spark SQL and GraphX on Spark which is one of the most popular in-memory data-flow processing platforms. First, we enhance Spark SQL by adding the capability of supporting the enhanced recursive SQL queries for graph analytics. In this regard, graph analytics can be processed using a distributed SQL engine alone. Second, we further propose new transformation rules to optimize/translate the operations for recursive SQL queries to the operations by GraphX. In this regard, graph analytics by SQL can be processed in a similar way as done by a distributed graph processing system using the APIs provided by the system. We conduct extensive performance studies to test graph analytics using large real graphs. We show that our approach can achieve similar or even higher efficiency, in comparison to the built-in graph algorithms in the existing graph processing systems.},
keywords={Structured Query Language;Sparks;Algebra;Engines;Programming;Distributed databases;Data processing;Graph analytics;distributed graph processing;SQL recursive query;spark},
doi={10.1109/TKDE.2019.2950620},
ISSN={1558-2191},
month={May},}
@INPROCEEDINGS{9376493,
author={Kundu, Barnali and Sarkar, Arghya},
booktitle={2020 IEEE 7th Uttar Pradesh Section International Conference on Electrical, Electronics and Computer Engineering (UPCON)}, title={Distributed Generators Curtailment for Congestion Mitigation},
year={2020},
volume={},
number={},
pages={1-6},
abstract={This research paper illustrates a new methodology to mitigate congestion with the influence of curtailment strategy of Distributed Generators (DG). In power system planning, Distributed System Operator (DSO) will consult with Transmission System Operator (TSO) by acquiring the knowledge of optimum DG curtailment that is accomplished using Particle Swam Optimization method in this work. The proposed methodology reduces the power injection of DGs up to a certain level. The contribution of DGs has been formulated for the critical lines as per contingency criteria. In this work, 39 Bus England Test system has been adopted for implementing the methodology. The results depicts that DG curtailment strategy mitigate the congestion as well as thermal loading of the lines. With the adaption of Particle Swam Optimization (PSO) based optimum DG curtailment strategy, DSO makes their profit in selling their power and creates a good financial impact on transmission system. Therefore, it is novel method which will be significant in the field of congestion management.},
keywords={Loading;Tariffs;Optimization methods;Power system planning;Generators;Planning;Thermal loading;Distributed Generators;curtailment;thermal loading;profit;Particle Swam Optimization},
doi={10.1109/UPCON50219.2020.9376493},
ISSN={2687-7767},
month={Nov},}
@INPROCEEDINGS{9361301,
author={Li, Haoming and Li, Yuguo},
booktitle={2020 International Conference on Artificial Intelligence and Computer Engineering (ICAICE)}, title={LogSpy: System Log Anomaly Detection for Distributed Systems},
year={2020},
volume={},
number={},
pages={347-352},
abstract={Log analysis is an important part of distributed system management. System log records the running status of the system and contains a lot of important and valuable information. This paper proposes an anomaly detection method, LogSpy, for distributed systems. It uses the combination of natural language processing technology and clustering algorithm for log template mining and feature extraction. In anomaly detection, it is found that there are a large number of remote calls in the distributed systems and traditional CNN has certain limitations on this small amount of negative sample data. LogSpy introduces the attention mechanism in detection algorithm and optimizes the detection window and computational complexity. Experiments conducted on the OpenStack test platform show that LogSpy can perform excellent anomaly detection on distributed systems compared to traditional anomaly detection methods.},
keywords={Systems operation;Clustering algorithms;Feature extraction;Natural language processing;Large-scale systems;Computational complexity;Anomaly detection;AIOps;anomaly detection;attention mechanism;CNN;distributed systems},
doi={10.1109/ICAICE51518.2020.00073},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9355357,
author={Li, Yinwei and Zhang, Dujuan},
booktitle={2020 International Conference on Advance in Ambient Computing and Intelligence (ICAACI)}, title={Hadoop-Based University Ideological and Political Big Data Platform Design and Behavior Pattern Mining},
year={2020},
volume={},
number={},
pages={47-51},
abstract={The combination of higher education and big data technology is not only the focus of the application of big data technology, but also an emerging field of self-development in the field of higher education. This article is dedicated to building a big data processing platform through Hadoop big data storage architecture, Hive, flume data collection technology, and Sqoop data synchronization technology to achieve efficient processing of big data sets. The traditional data mining algorithm is implemented using Map Reduce programming, and the implementation of the data mining algorithm of the Hadoop platform is studied, mainly to analyze its execution efficiency and scalability. We select the data clustering task in data mining as a representative, and write its Map Reduce version to test and verify its effect on the Hadoop platform. Through comparative experiments of different cluster sizes and different data sizes, it is concluded that the use of Hadoop distributed systems for data mining tasks has a good acceleration ratio and efficiency, and the extended performance analysis of computing power also shows that it has great potential.},
keywords={Education;Distributed databases;Computer architecture;Big Data;Tools;Data mining;Task analysis;Hadoop;big data platform;behavior mode;college ideology and politics},
doi={10.1109/ICAACI50733.2020.00014},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9336989,
author={Shanmugapriya, P. and Baskaran, J and Nayanatara, C. and Sharmila, P. and C, Haariharan N},
booktitle={2020 International Conference on Power, Energy, Control and Transmission Systems (ICPECTS)}, title={GA based Simultaneous Optimization of hybrid Distributed Generation in the Power System Network},
year={2020},
volume={},
number={},
pages={1-7},
abstract={The distribution system in the Power System network is to provide excellent Power supply to the consumers. Due to increase of power demand in present scenario, hybrid power generation have been increased in distributed system, so small DG units are placed to improve the voltage profile through various loads connected to feeders. Wind/solar alone does not fulfil the requirement of uniform voltage to load because it is intermittent in nature so DG along with diesel generator will be required to have maximum benefit. An intelligent technique that employs GA approach to achieve the objective function. The voltage stability index would determine the candidate bus for optimal location of these devices. This approach is tested on IEEE 57 bus system and Indian utility system NTPS 17 bus system with different types of loads through the feeders.},
keywords={System performance;Power system stability;Wind power generation;Generators;Hybrid power systems;Wind turbines;Optimization;Power system network;Distributed Generation (DG);Genetic Algorithm (GA);Bus System},
doi={10.1109/ICPECTS49113.2020.9336989},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9339279,
author={Tao, Jun and Hu, Menglu and Wang, Yaling and Zhang, Yangyang},
booktitle={2020 IEEE 5th International Conference on Signal and Image Processing (ICSIP)}, title={Distributed ADS-B system based on MPLS VPN},
year={2020},
volume={},
number={},
pages={949-954},
abstract={This paper introduces the ADS-B system. The distributed ADS-B system can increase the scope of monitoring and share data. Two distributed algorithms of ADS-B system are proposed, and their advantages and disadvantages are analyzed in detail. D-select algorithm is used to realize the online operation of the system. MPLS VPN technology is used to interconnect the ADS-B system, and the simulation experiment is used to test the results, the simulation experiment shows good results. The real ADS-B distributed system is built, all network parameters such as jitter, delay and packet loss are normal, and the functions of ADS-B distributed system are normal.},
keywords={Multiprotocol label switching;Packet loss;Lightning;Radar;Jitter;Virtual private networks;Monitoring;ADS-B;MPLS;VPN;jitter;delay},
doi={10.1109/ICSIP49896.2020.9339279},
ISSN={},
month={Oct},}
@INPROCEEDINGS{9314389,
author={Moutai, Fatima Zahra and Amine Tajioue, Mohammed and Oualla, Aicha and Azzouzi, Salma},
booktitle={2020 IEEE 2nd International Conference on Electronics, Control, Optimization and Computer Science (ICECOCS)}, title={ gt;Towards Distributed Systems Testing in Cloud Environment},
year={2020},
volume={},
number={},
pages={1-6},
abstract={There exists a close relationship between cloud computing and distributed systems. In fact, Cloud computing technology brings new organizational and technological capabilities to build an underlying infrastructure for distributed systems. Therefore, even if cloud computing offers opportunities to improve productivity and reduce costs, it also introduces a number of technical challenges, especially in testing area. In this paper, we introduce concepts such as cloud computing, cloud testing and Infrastructure as a Service (IaaS). Then, we present our distributed testing architecture for applications running in an elastic cloud computing environment using these concepts.},
keywords={Cloud computing;Testing;Computer architecture;Observability;Controllability;Elasticity;Computational modeling;Distributed testing;Cloud computing;Cloud testing;Controllability;Observability;Synchronization.},
doi={10.1109/ICECOCS50124.2020.9314389},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9291572,
author={Kang, Kai and Xu, Lijie and Wang, Wei and Wu, Guoquan and Wei, Jun and Shi, Wei and Li, Jizhong},
booktitle={2020 International Conferences on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics)}, title={A Hierarchical Automata Based Approach for Anomaly Detection in Smart Home Devices},
year={2020},
volume={},
number={},
pages={1-8},
abstract={Smart home is an important application scenario of the Internet of Things. However, smart home tends to suffer from runtime failures due to its complex software/hardware, unexpected/wrong human operations, environmental changes, etc. It is hard to detect the device anomalies, since smart devices have various types, various failure types, and they also generally do not expose the inner runtime information. Current anomaly detection techniques for general programs or distributed systems do not apply for smart home devices. To help both device manufactures and users to detect the device anomalies, we propose a general framework named SmartHome-Detector. It first builds a hierarchical automata based behavior model during the testing phase, and uses it as the baseline of IoT device. At runtime, a comprehensive anomaly detection technique is proposed to identify various device anomalies in time. We implement SmartHomeDetector based on the open source smart home system HomeAssistant. Our experiments and case studies on real-world smart home devices show that SmartHomeDetector is effective. It accurately models the running behavior of IoT devices and can detect various device anomalies at runtime.},
keywords={Smart homes;Automata;Runtime;Data models;Performance evaluation;Internet of Things;Computational modeling;Smart Home;Hierarchical Automata;Fault Detection},
doi={10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics50389.2020.00021},
ISSN={},
month={Nov},}
@ARTICLE{9300130,
author={Abdalla, Iman and Rahaim, Michael B. and Little, Thomas D. C.},
journal={IEEE Access}, title={Interference Mitigation Through User Association and Receiver Field of View Optimization in a Multi-User Indoor Hybrid RF/VLC Illuminance-Constrained Network},
year={2020},
volume={8},
number={},
pages={228779-228797},
abstract={In this paper we address interference mitigation through user association and receiver field of view (FOV) optimization in a multi-user indoor optical wireless communication (OWC) scenario. We explore several dynamic FOV receiver solutions including steerable (SDFOV) and non-steerable (DFOV) to optimize performance for multiple devices experiencing orientation dynamics. We compare their performance to a baseline fixed FOV receiver (FFOV). Through modeling and simulation we find that SDFOV receivers outperform DFOV by up to 2.6x and FFOV by up to 5.6x in terms of average minimum throughput gain using our test scenario. Similarly, DFOV receivers can achieve up to 2.2x gain over FFOV receivers. For multi-user environments, we compare the performance of coordinated versus distributed system control. Results show that in the worst case, the distributed greedy system performs on average 46%, 16%, and 57% below the coordinated system for SDFOV, DFOV, and FFOV, respectively at a reduced computational complexity compared to the centralized system. We also note that the performance gap in each system diminishes with increasing transmitter Lambertian order. This analysis is done under different room coverage achieved through optimizing the transmitted power to jointly maximize the minimum received power and the standard illuminance range probability at the working plane. Next, we show the impact of self- and random-human blockage at different Lambertian orders on the minimum and average user throughput values. Lastly, we show the gains from employing the hybrid RF/VLC network compared to a VLC-only mode for two different strategies: (1) minimum-throughput-enhancing and (2) sum-throughput-enhancing.},
keywords={Receivers;Interference;Transmitters;Optimization;Lighting;Optical transmitters;Throughput;Visible light communications (VLC);LiFi;dense networks;optical wireless communications (OWC);hybrid RF/VLC;dynamic field of view (FOV);multi-user;interference;self-blockage;user association;random orientation;FOV optimization;resource allocation;load balancing;user association;emission pattern;smart lighting},
doi={10.1109/ACCESS.2020.3045929},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9284220,
author={Shithil, Santa Maria and Adnan, Muhammad Abdullah},
booktitle={2020 IEEE 13th International Conference on Cloud Computing (CLOUD)}, title={A Prediction Based Replica Selection Strategy for Reducing Tail Latency in Distributed Systems},
year={2020},
volume={},
number={},
pages={522-526},
abstract={In the distributed system, applications often suffer from unexpectedly high response time fluctuation which is a consequence of long-tail latency. One efficient way of reducing tail latency is to use an appropriate replica selection strategy. In this paper, through extensive experiments, we analyze and evaluate two popular state-of-the-art replica selection strategies of key-value store systems: Cassandra dynamic snitch and C3. After analyzing all the experimental results we develop a prediction based replica selection strategy and implement it on Cassandra 3.0. Our proposed algorithm outperforms both C3 and Cassandra dynamic snitch. For evaluation, we test Cassandra dynamic snitch, C3, and our proposed algorithm on a 15 nodes Cassandra cluster. For generating test datasets and workloads we use industry-standard Yahoo Cloud Serving Benchmark (YCSB).},
keywords={Cloud computing;Fluctuations;Heuristic algorithms;Conferences;Clustering algorithms;Prediction algorithms;Time factors;distributed system;key-value store;replica selection strategy;tail latency},
doi={10.1109/CLOUD49709.2020.00078},
ISSN={2159-6190},
month={Oct},}
@INPROCEEDINGS{9277825,
author={Gao, Qiang and Zhu, Yizhi and Zhou, Jinhang and Chen, Xianqing and Yang, Qiang},
booktitle={2020 19th International Symposium on Distributed Computing and Applications for Business Engineering and Science (DCABES)}, title={Scenario-based Multi-Energy Power Distribution System Planning Solution for Energy Loss Minimization},
year={2020},
volume={},
number={},
pages={20-23},
abstract={This paper presented a distributed system planning (DSP) solution for identifying an optimal fuel mix in the electric power distribution networks with the minimized power line losses. The optimal fuel mix consists of both renewable energy-based distributed renewable generators, i.e. wind turbines and photovoltaic as well as the conventional fuel-based DG units. In this work, the method based on the heuristic moment matching (HMM) is firstly adopted to generate WT-PV-LD operational scenarios that combine all system uncertainties of DG generation and power demand. Such a scenario matrix is then incorporated into the robust DSP problem formulation. The proposed solution is evaluated using a 53 bus test network. Finally, the HMM-based planning approach is validated through simulation experiments and the effectiveness is confirmed.},
keywords={Planning;Reactive power;Distribution networks;Uncertainty;Substations;Wind turbines;Hidden Markov models;multi-energy;power distribution system planning;Heuristic moment matching},
doi={10.1109/DCABES50732.2020.00015},
ISSN={2473-3636},
month={Oct},}
@INPROCEEDINGS{9275877,
author={Priggemeyer, Marc and Roßmann, Jürgen},
booktitle={2020 IEEE International Systems Conference (SysCon)}, title={Meta-Information driven Modeling of Services for Experimentable Digital Twins},
year={2020},
volume={},
number={},
pages={1-8},
abstract={Experimental Digital Twins (EDT) provide a comprehensive view on the behavior of Cyber-Physical Systems (CPS). This is achieved by simulation technology and the application of virtual environments to develop and test desired functionality, including models, data structures, services and communication capabilities. This paper outlines a meta-information driven method of describing services provided by a CPS for consumption by peers via any means of communication. In addition, a set of application rules is presented, which handle cases such as service composition and the interaction of CPS components in distributed systems. We present the fundamental meta-information system that facilitates the specification process by providing the necessary type information for the dynamic object construction and automated (de-)serialization of data passed between provider and consumer.},
keywords={Digital twin;Runtime;Data models;Unified modeling language;Protocols;Industries;Context modeling},
doi={10.1109/SysCon47679.2020.9275877},
ISSN={2472-9647},
month={Aug},}
@INPROCEEDINGS{9270297,
author={Lima, Bruno and Faria, João Pascoal},
booktitle={2020 IEEE/ACM 42nd International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)}, title={DCO Analyzer: Local Controllability and Observability Analysis and Enforcement of Distributed Test Scenarios},
year={2020},
volume={},
number={},
pages={97-100},
abstract={To ensure interoperability and the correct behavior of heterogeneous distributed systems in key scenarios, it is important to conduct automated integration tests, based on distributed test components (called local testers) that are deployed close to the system components to simulate inputs from the environment and monitor the interactions with the environment and other system components. We say that a distributed test scenario is locally controllable and locally observable if test inputs can be decided locally and conformance errors can be detected locally by the local testers, without the need for exchanging coordination messages between the test components during test execution (which may reduce the responsiveness and fault detection capability of the test harness). DCO Analyzer is the first tool that checks if distributed test scenarios specified by means of UML sequence diagrams exhibit those properties, and automatically determines a minimum number of coordination messages to enforce them.The demo video for DCO Analyzer can be found at https://youtu.be/LVIusK36_bs.},
keywords={Controllability;Unified modeling language;Tools;Observability;Licenses;Monitoring;Visualization;Local Observability;Local Controllability;Distributed Systems Testing;Integration Testing},
doi={},
ISSN={2574-1926},
month={Oct},}
@INPROCEEDINGS{9263970,
author={Tajioue, Mohammed Amine and Maakoul, Oussama and Hsaini, Sara and Azzouzi, Salma and Charaf, My El Hassan},
booktitle={2020 7th International Conference on Control, Decision and Information Technologies (CoDIT)}, title={Towards Overcoming Issues of Testing Probabilistic Distributed systems},
year={2020},
volume={1},
number={},
pages={903-907},
abstract={Testing a distributed system, to ensure its conformance to the specification, involves usually placing a set of parallel testers called PTCs (Parallel Test Components) attached to each port of the implementation under test (IUT). As a result, many problems known as controllability, observability and synchronisation fault detections occur while testing such distributed systems. Hence, the design process should take into consideration the mechanisms and functions needed to support interaction, communication and coordination between the distributed components especially if the implementation exhibits a stochastic behaviour.In this paper, we use probabilistic automaton to model and analyse the stochastic behaviour of the implementation under test. Then, we propose an algorithm for generating probabilistic local test sequences (PLTS) for each tester guarantying to avoid problems of coordination, observation and synchronization during the testing process.},
keywords={Probabilistic logic;Testing;Automata;Synchronization;Stochastic processes;Fault detection;Controllability;Distributed systems;Probabilistic distributed testing;Probabilistic automata;Controllability;Synchronization},
doi={10.1109/CoDIT49905.2020.9263970},
ISSN={2576-3555},
month={June},}
@INPROCEEDINGS{9251052,
author={Kesim, Dominik and van Hoorn, André and Frank, Sebastian and Häussler, Matthias},
booktitle={2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE)}, title={Identifying and Prioritizing Chaos Experiments by Using Established Risk Analysis Techniques},
year={2020},
volume={},
number={},
pages={229-240},
abstract={The prevalence of microservice architectures and container orchestration technologies increases the complexity of assessing such systems' resilience. Chaos engineering is an emerging approach for resilience assessment by testing hypotheses after intentionally injecting faults into a distributed system and observing customer- and business-affecting metrics. As the number of potential risks within a complex system is high, the identification and prioritization of effective and efficient chaos experiments are non-trivial. In the scope of an industrial case study, this work investigates means to identify and prioritize chaos experiments by using established risk analysis techniques known from engineering safety-critical systems, namely i) Fault Tree Analysis, ii) Failure Mode and Effects Analysis, iii) and Computer Hazard and Operability Study. We conducted semi-structured interviews to elicit architectural information and resilience requirements of the case study system. The extracted knowledge was leveraged during the application of the risk analysis techniques. A subset of the identified and prioritized risks was used to create and execute chaos experiments. The risk analysis resulted in over 100 findings and revealed that the system is rather fragile as it comprises a high amount of single points of failure. The chaos experiments revealed further weaknesses for formerly unknown system behavior.},
keywords={Chaos;Hazards;Software reliability;Risk analysis;Requirements engineering;Resilience;Testing;Chaos engineering, Resilience testing, Risk analysis},
doi={10.1109/ISSRE5003.2020.00030},
ISSN={2332-6549},
month={Oct},}
@INPROCEEDINGS{9250776,
author={Kathait, Anuraj and Dhage, Sudhir N.},
booktitle={2020 IEEE 5th International Conference on Computing Communication and Automation (ICCCA)}, title={Fractal Load Balancing Method in Distributed Systems},
year={2020},
volume={},
number={},
pages={172-176},
abstract={Distributed systems are the backbone of several modern technologies. This field has been a catalyst for several other areas. Load balancing, one of the most basic applications of distributed systems, has several issues. This paper introduces a Fractal load balancing method inspired by a fractal pattern called a Sierpinski triangle. Detailed information about its design and functioning shows its potential in being an efficient load balancer. Tests carried out on its simulation gracefully highlight its performance.},
keywords={Conferences;Load management;Fractals;Pattern recognition;Resource management;Task analysis;Load modeling;Cluster Computing;Distributed System;Load Balancing;Fractal;Sierpinski triangle},
doi={10.1109/ICCCA49541.2020.9250776},
ISSN={2642-7354},
month={Oct},}
@INPROCEEDINGS{9239061,
author={Huang, Chin-Tser and Njilla, Laurent and Geng, Tieming},
booktitle={2020 IEEE International Smart Cities Conference (ISC2)}, title={Detecting Counterfeit ICs with Blockchain-based Verification Framework},
year={2020},
volume={},
number={},
pages={1-8},
abstract={Counterfeit electronic parts have posed a serious threat to consumers, industry, and government and military agencies for a long time. They not only lead to massive damage in terms of financial and reputation losses, but can also create high risk in the compromise of system safety and integrity. Our previous work shows the possibility of achieving the decentralized runtime verification by incorporating some mechanisms of the blockchain technology into a distributed system for locating the accountability when error occurs. In this paper, we propose to develop a verification framework based on blockchain technology to detect the abnormalities of counterfeit ICs at every stage of their lifecycles, including after being deployed into the system. We design an efficient and effective method that can store the inspection and testing results into a blockchain, and apply the smart marker scheme which can merge separate blockchains associated with individual ICs into one blockchain after the ICs are installed onto the same system.},
keywords={Integrated circuits;Runtime;Smart cities;Blockchain;Inspection;Safety;Testing;blockchain;counterfeit IC;validation},
doi={10.1109/ISC251055.2020.9239061},
ISSN={2687-8860},
month={Sep.},}
@INPROCEEDINGS{9210295,
author={Bird, David},
booktitle={2020 Fourth World Conference on Smart Trends in Systems, Security and Sustainability (WorldS4)}, title={Distributed Test, Track and Trace System as a Contributor to Epidemic Containment Management},
year={2020},
volume={},
number={},
pages={722-729},
abstract={The outcome of the recent novel Coronavirus pandemic is profound and particularly poignant when countries like South Korea, who have learned from previous pandemics, appear to be better able to respond to the challenge. Since lock down due to the novel Coronavirus of 2019, the Government of the United Kingdom has adopted retrospective testing and a smartphone application for contact tracing as part of its retroactive test, track and trace strategy. However, to thwart future epidemics, this paper proposes that a different approach should be adopted in preparation for the next epidemic. This consists of a distributed system solution comprising front-end testing nodes at the borders as well as test laboratories connected to a backend cloud-based data processing and data storage system. This core system can be used to test, track and trace infection hotspots at the start of an epidemic to avoid significant outbreaks in the United Kingdom. This hotspot data can be made available to local authorities so that more informed decisions can be made regarding approaches to contain any infectious viral spread. This would need to involve traditional epidemiological track and trace surveillance activities to understand the propagation effect; and therefore, understand the resultant disease coverage in the community so that mitigation measures can be successfully applied in a timelier fashion.},
keywords={Market research;Security;Sustainable development;COVID-19;Cloud;Centralized Test Data;Distributed System;Lessons Learned;Propagation Tracking and Tracing},
doi={10.1109/WorldS450073.2020.9210295},
ISSN={},
month={July},}
@INPROCEEDINGS{9191562,
author={Gemmi, Gabriele and Maccari, Leonardo},
booktitle={2020 Mediterranean Communication and Computer Networking Conference (MedComNet)}, title={NPART+: Improving Wireless Network Topology Generators with Data from the Real World},
year={2020},
volume={},
number={},
pages={1-8},
abstract={Topology generators are a key asset for researchers in computer science and telecommunications that often need to test network protocols or distributed systems in simulated environments that resemble real scenarios. Despite that, in the research area of distributed wireless networks still many works use very simplistic models that do not have the characteristics of the currently existing large-scale wireless mesh networks. The only topology generator that tries to produce synthetic graphs that look like real networks is NPART [1].In this work we test the characteristics of NPART against another, completely different approach: TrueNets [2]. TrueNets uses accurate data representing land surface of a real world location to create topologies of networks that could actually exist. The downside of TrueNets is twofold: it can be used only when data-sets are available and generating topologies is computationally intensive. We show that using aggregate data from TrueNets we are able to improve NPART. We call the new generator NPART+ and we show that compared to topologies generated with TrueNets, NPART+ (or its variants) improves NPART in several metrics, but still it can not match the accuracy of TrueNets.},
keywords={Network topology;Topology;Generators;Buildings;Mesh networks;Shadow mapping;Wireless communication},
doi={10.1109/MedComNet49392.2020.9191562},
ISSN={},
month={June},}
@ARTICLE{9187402,
author={Carvalho, Dhiego Fernandes and Ferrari, Paolo and Sisinni, Emiliano and Flammini, Alessandra},
journal={IEEE Systems Journal}, title={Improving Redundancy in LoRaWAN for Mixed-Criticality Scenarios},
year={2020},
volume={},
number={},
pages={1-10},
abstract={Most Internet of Things applications rely on wireless communication, which ensures flexibility and scalability. However, an increasing number of these applications have real-time constraints and require mixed-criticality. Indeed, the complexity of distributed systems necessarily calls for data exchanges with different priorities and different failure handling. The design of a reliable protocol supporting different traffic priorities must take into account external interferences that can inevitably affect radio links. This article focuses on LoRaWAN, the currently most famous example of low power wide area network technology. In particular, redundant transmissions offered by the proposed LoRa-REP strategy enhance the communication resilience of sporadic critical message transaction (consisting of message uplink and acknowledgment downlink). The implementation by means of “virtual nodes” assures compatibility with legacy deployments (preserving investments) and offers easy scalability. Experimental results obtained with purposely designed test bench confirm the advantages of the proposed solution in a real-world scenario. Notably, the failure probability of a critical transaction drops from 78% of single spreading factor scenario to less than 2.5% with the proposed LoRa-REP.},
keywords={Wireless communication;Redundancy;Protocols;Uplink;Downlink;Wireless sensor networks;Emergency transmission;low power wide area network (LPWAN);priority;redundancy},
doi={10.1109/JSYST.2020.3015274},
ISSN={1937-9234},
month={},}
@ARTICLE{9186641,
author={Lima, Bruno and Faria, João Pascoal and Hierons, Robert},
journal={IEEE Access}, title={Local Observability and Controllability Analysis and Enforcement in Distributed Testing With Time Constraints},
year={2020},
volume={8},
number={},
pages={167172-167191},
abstract={Evermore end-to-end digital services depend on the proper interoperation of multiple products, forming a distributed system, often subject to timing requirements. To ensure interoperability and the timely behavior of such systems, it is important to conduct integration tests that verify the interactions with the environment and between the system components in key scenarios. The automation of such integration tests requires that test components are also distributed, with local testers deployed close to the system components, coordinated by a central tester. Test coordination in such a test architecture is a big challenge. To address it, in this article we propose an approach based on the pre-processing of the test scenarios. We first analyze the test scenarios in order to check if conformance errors can be detected locally (local observability) and test inputs can be decided locally (local controllability) by the local testers for the test scenario under consideration, without the need for exchanging coordination messages between the test components during test execution. If such properties do not hold, we next try to determine a minimum set of coordination messages or time constraints to be attached to the given test scenario to enforce those properties and effectively solve the test coordination problem with minimal overhead. The analysis and enforcement procedures were implemented in the DCO Analyzer tool for test scenarios described by means of UML sequence diagrams. Since many local observability and controllability problems may be caused by design flaws or incomplete specifications, and multiple ways may exist to enforce local observability and controllability, the tool was designed as a static analysis assistant to be used before test execution. DCO Analyzer was able to correctly identify local observability and controllability problems in real-world scenarios and help the users fix the detected problems.},
keywords={Observability;Controllability;Time factors;Testing;Unified modeling language;Tools;Analytical models;Test scenarios;observability;controllability;distributed systems;time constraints},
doi={10.1109/ACCESS.2020.3021858},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9177688,
author={Fu, Lei},
booktitle={2020 12th International Conference on Advanced Computational Intelligence (ICACI)}, title={Time Series-oriented Load Prediction Using Deep Peephole LSTM},
year={2020},
volume={},
number={},
pages={86-91},
abstract={Load imbalances are inclined to cause performance losses in distributed systems. Load forecasting is a technique that has been used widely in improving load redistribution to avoid load imbalances. In order to solve the limitations of time-consuming, complex and low accuracy of the traditional prediction model, and in view of the fact that the load sequence has the characteristics of randomness, non-stationarity, nonlinearity and its continuous oscillation, a model based on long short-term memory (LSTM) is proposed to enhance load prediction precision in this paper. The proposed model in this paper is Deep Long Short-term Memory with peephole connections (DPLSTM), and it tested on a real dataset, the experiment results show that the DPLSTM is capable of predicting load information efficiently and producing very close values to the actual load values, it outperforms the other load forecasting approaches.},
keywords={Load modeling;Predictive models;Autoregressive processes;Logic gates;Time series analysis;Forecasting;Biological neural networks;load balancing;load prediction;time series forecasting;long short-term memory (LSTM);deep learning},
doi={10.1109/ICACI49185.2020.9177688},
ISSN={2573-3311},
month={Aug},}
@INPROCEEDINGS{9153715,
author={O’Halloran, Bryan and Van Bossuyt, Douglas L.},
booktitle={2020 Annual Reliability and Maintainability Symposium (RAMS)}, title={How Do Systems Fail?},
year={2020},
volume={},
number={},
pages={1-6},
abstract={Summary & Conclusions: Modern systems are changing quickly and becoming more complex through increased connectivity, smaller packaging, higher performance requirements, more components, the inclusion of complex software and Artificial Intelligence (AI), and much more. The following are high-level challenges that arise in many modern systems. The first is the distribution of the system, which are both physical (e.g., power grids) and digital (e.g., air traffic control, transportation networks). With highly distributed system, the vulnerability from the environment becomes significant. The second challenge is the implementation of new technology where examples include driverless vehicles and Boeing's 787 Dreamliner. Occasionally implementing new technology doesn't lend well to their intended purpose as observed by the Supersonic Transport (SST) aircrafts for commercial flights such as Concorde [1] and the Tupolev Tu-144 [2]. This industry suffered a major crash, Air France Flight 4590, that killed 109 passengers and crew and led to the ultimate demise of the industry [3]. The result of these design challenges is the need for improved methods to identify, assess, and mitigate off-nominal behavior. While all industries seek to create safe and reliability systems, their failures continue to splash across the news with surprising regularity. The examples are nearly endless. Across 63 years (1957-2019) there have been 402 mission failures in the spaceflight industry including satellites, manned spacecrafts, rockets, etc. As a subset of these missions, the manned spaceflight industry has seen 118 failures with a total of 262 deaths [4]; there have been 5 manned flight incidents where 19 astronauts died, 8 training or testing incidents where 11 astronauts died, 35 incidents where a total of 232 non-astronauts died (e.g., civilians, employees, etc.), and 70 incidents (35 flight and 35 training or testing) where no deaths occurred. Beyond the 402 mission failures, there have also been 118 Satellite launch failures [4]. Since the introduction of the commercial airline industry in 1918, there have been a reported 154,984 deaths [3]. Since 1970, there have been 11,634 accidents. Even more alarming is that the annual death rate hasn't decreased much with time. The death rate per year between 1970-2018 is 1722 and between 1990-2018 is 1337. While this has reduced, a large number of accidents continue to cause a large number of deaths in this industry. According to [5], there have been 25 major dam failures, 16 of which have occurred in the last 50 years. The nuclear power industry has observed over 100 failures, several of which have resulted mitigations exceeding a billion US dollars. It is important to note that systems fail with regularity regardless of the system's type, purpose, or age, the industry that the system belongs, or the era in which it was designed and built. The continued increase in what we demand from our systems has always trumped the practitioner's ability to assess and mitigate off-nominal behavior. These facts show that failure has always been imminent. Until significant improvements are made to the way that we assess and mitigate failures, it is unreasonable to consider the outcome to change. As such, one element of assessment is to understand the variety of causes that involve the failures we observe. As such, this paper seeks to characterize failures by their cause. This is done by surveying a large number of failures from several different relevant industries, then deriving categories of failure cause. Seven categories of failures are identified including: development failures, induced failures, common cause failures, propagated failures, interaction failures, malicious failures, and management, customer, and misuse failures. By understanding the different classes of failures potentially present in complex systems, engineers can better choose which failure, risk, and reliability analysis tools are most appropriate to use with specific systems. This in turn may lead to more reliable systems that are less prone to failure throughout the system lifecycle.},
keywords={Industries;Failure analysis;Taxonomy;Fires;Reliability;Autonomous vehicles},
doi={10.1109/RAMS48030.2020.9153715},
ISSN={2577-0993},
month={Jan},}
@ARTICLE{9151961,
author={Sciumè, Giuseppe and Palacios-García, Emilio Jose and Gallo, Pierluigi and Sanseverino, Eleonora Riva and Vasquez, Juan C. and Guerrero, Josep M.},
journal={IEEE Access}, title={Demand Response Service Certification and Customer Baseline Evaluation Using Blockchain Technology},
year={2020},
volume={8},
number={},
pages={139313-139331},
abstract={The use of Distributed Ledger Technologies such as Blockchain for certifying Demand Response services allows for the creation of a distributed system in which customers can communicate with the system operator to provide their flexibility, in a secure, transparent and traceable way. Blockchain technology also supports incentive mechanisms for users taking part in the service through the generation of utility tokens to recognize the user's contribution. This paper presents the experimental test of a novel methodology for Demand Response programs implementation by using the Blockchain technology. The latter is employed for defining a distributed Demand Response service and a new system for its tracing and certification. For this work, a Smart Contract has been conceived and written to execute Demand Response events, calculate users' baseline, compute the support provided by each user towards the fulfilment of the requested load curve modification and remunerate each user with utility tokens proportionally to their contribution. To test the methodology, a Hyperledger Fabric network and a Smart Contract were deployed on four nodes of the Microgrid Laboratory of the Department of Energy Technology at Aalborg University (DK). Subsequently, a realistic scenario comprising two consumer nodes was developed using power electronic converters for generating the household profiles and Smart Meters for the measurement of the consumption profiles. Theoretical and experimental results show the feasibility of Distributed Ledger Technologies in smart grids management with a minimum investment in new hardware while enabling the active participation of customers in Demand Response more transparently and fairly.},
keywords={Load management;Contracts;Microgrids;Real-time systems;Blockchain;demand response;distributed balancing;baseline;hyperledger fabric;smart contract},
doi={10.1109/ACCESS.2020.3012781},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{9145997,
author={Zhang, Shudong and Liu, Dongxue and Zhou, Lijuan and Ren, Zhongshan and Wang, Zipeng},
booktitle={2020 2nd International Conference on Computer Communication and the Internet (ICCCI)}, title={Diagnostic Framework for Distributed Application Performance Anomaly Based on Adaptive Instrumentation},
year={2020},
volume={},
number={},
pages={164-169},
abstract={Instrumentation technology can obtain the status information of the distributed system when it is running. It is the core part of software performance management tools. But the use of instrumentation technology is often accompanied by a waste of resources. In this paper, we designed an adaptive instrumentation mechanism to balance the monitoring needs and resource load. Using analytical models based on linear regression and K-Means based on density algorithm to analyze performance data, determine the actual operating conditions and monitoring requirements of the monitored system, dynamically change the insertion point, and reduce resource consumption. Experiments show that compared with traditional tools, using the method of this article for monitoring, when the user clicks a lot, the system throughput is lower, the resource load is smaller, the average response time of the tested web page is reduced by 3.37%, and the target program is Interference is smaller.},
keywords={Instruments;Time factors;Tools;Monitoring;Delays;Target tracking;Java;distributed system monitoring;instrumentation;adaptive;performance data analysis},
doi={10.1109/ICCCI49374.2020.9145997},
ISSN={},
month={June},}
@INPROCEEDINGS{9140610,
author={Qaeini, Saeid and Nazar, Mehrdad S. and Shafie-khah, Miadreza and Osório, Gerardo J. and Catalão, João P. S.},
booktitle={2020 IEEE 20th Mediterranean Electrotechnical Conference ( MELECON)}, title={Optimal Planning of CHP-based Microgrids Considering DERs and Demand Response Programs},
year={2020},
volume={},
number={},
pages={605-610},
abstract={This work addresses a stochastic framework for optimal operation and long-term expansion planning of combined heat and power based microgrid as a part of an active distributing system. The microgrid utilizes renewable energy sources, electricity and heat generation units, energy storage systems, and demand response programs. The proposed model determines the optimal location and capacity of the electrical and thermal facilities, and it considers the impact of renewable energy sources and demand response on the expansion-planning problem. A stochastic mixed-integer linear programming formulation is utilized to minimize the investment and operation costs of system for five years. To evaluate the effectiveness of the proposed model, the algorithm is assessed for the 9-bus system and the 33-bus IEEE test systems. The results demonstrate that the utilization of the proposed algorithm reduces the operational cost and increases system revenues.},
keywords={Cogeneration;Planning;Boilers;Load management;Load modeling;Renewable energy sources;Uncertainty;Microgrid;Combined heat and power;Renewable energy;Distributed generation;Expansion planning;Demand response},
doi={10.1109/MELECON48756.2020.9140610},
ISSN={2158-8481},
month={June},}
@INPROCEEDINGS{9124303,
author={Sanz, M. and Santamargarita, D. and Huerta, F. and Ochoa, D. and Lázaro, A. and Barrado, A.},
booktitle={2020 IEEE Applied Power Electronics Conference and Exposition (APEC)}, title={Reduced-Order Model of Power Converters to Optimize Power Hardware-In-the-Loop Technology in Dc-Distributed Systems},
year={2020},
volume={},
number={},
pages={2868-2874},
abstract={The Power Hardware-In-the-Loop (PHIL) technology provides a powerful tool for testing scenarios where there is a high-power interchange, in which the performance of field tests can be very complex or expensive. When performing PHIL simulations of systems with a high number of components, such as DC-distributed systems on a ship or aircraft, the use of switched or average models of the converters can require the use of expensive commercial real-time digital simulators (RTDS) reducing the advantages of these technology. This paper is focused on the proposal of a reduced order model of converters to be able to perform PHIL analysis of Dc-distributed systems using low resources of the required real-time digital simulator. The paper validates that the proposed reduced-order model is able to determine the stability on the Dc-distributed system in comparison with more complex converter models. Moreover, a comparison between both models regarding the required resources in the implementation in a commercial RTDS platform is performed to validate the benefits of the proposed model in performing PHIL analysis of large power distribution systems.},
keywords={Power Hardware-In-the-Loop;power system stability;power converter model},
doi={10.1109/APEC39645.2020.9124303},
ISSN={2470-6647},
month={March},}
@INPROCEEDINGS{9114967,
author={Yan, Lijun and Shi, Fenglei and Zhu, Hongxiang},
booktitle={2020 Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)}, title={Research on Distributed Parallel Computing Technology of Airborne Test Data},
year={2020},
volume={},
number={},
pages={16-21},
abstract={Aiming at the task requirements of a small fast-moving test vehicle with small space, short flight support interval and real-time task evaluation, at the same time, the future flight test is faced with many test parameters, wide distribution of test systems, and high real-time data requirements. To solve the rapid acquisition test evaluation data in a specific environment, we design distributed system, data analysis and fusion technology, embedded parallel computing and other technologies to realize the parallel computing and real-time processing on airborne data. The calculated data results are directly telemetry or recorded for real-time monitoring and rapid assessment of missions. The design meets the requirements of rapid real-time evaluation during the flight test and improves the flight test efficiency.},
keywords={flight test;distributed system;parallel computation},
doi={10.1109/IPEC49694.2020.9114967},
ISSN={},
month={April},}
@ARTICLE{9109421,
author={Wolf, Shaya and Cooley, Rafer and Fantl, Jason and Borowczak, Mike},
journal={IEEE Consumer Electronics Magazine}, title={Secure and Resilient Swarms: Autonomous Decentralized Lightweight UAVs to the Rescue},
year={2020},
volume={9},
number={4},
pages={34-40},
abstract={Distributed systems disseminate emergent behaviors across an interconnected group of lightweight agents. The SHARKS protocol (Secure, Heterogeneous, Autonomous, and Rotational Knowledge for Swarms) allows for a robotic swarm to encircle/ensphere a target. Motivated by security and resiliency, the SHARKS protocol addresses traditional vulnerabilities without creating a central point of vulnerability. The SHARKS protocol tolerates the loss of agents and continues to surround a target with as few as three individuals. Utilizing two movement algorithms, agents remain within a specified radial distance of the target while also evenly dispersing from their neighbors. SHARKS was tested in two-dimensional (2-D) and 3-D fields. In 2-D fields, the efficiency (how quickly the swarm stabilizes) and the capacity (the number of agents that can stabilize) were tested across different configurations. In 3-D fields, the stability was tested across many additional configurations based on when each swarm reached 68%, 95%, and 100% stability.},
keywords={Protocols;Sociology;Statistics;Drones;Dispersion;Robot sensing systems;Task analysis},
doi={10.1109/MCE.2020.2969174},
ISSN={2162-2256},
month={July},}
@ARTICLE{9106854,
author={Lu, Jie and Liu, Chen and Li, Feng and Li, Lian and Feng, Xiaobing and Xue, Jingling},
journal={IEEE Transactions on Software Engineering}, title={CloudRaid: Detecting Distributed Concurrency Bugs via Log-Mining and Enhancement},
year={2020},
volume={},
number={},
pages={1-1},
abstract={Cloud systems suffer from distributed concurrency bugs, which often lead to data loss and service outage. This paper presents CLOUDRAID, a new automatical tool for finding distributed concurrency bugs efficiently and effectively. Distributed concurrency bugs are notoriously difficult to find as they are triggered by untimely interaction among nodes, i.e., unexpected message orderings. To detect concurrency bugs in cloud systems efficiently and effectively, CLOUDRAID analyzes and tests automatically only the message orderings that are likely to expose errors. Specifically, CLOUDRAID mines the logs from previous executions to uncover the message orderings that are feasible but inadequately tested. In addition, we also propose a log enhancing technique to introduce new logs automatically in the system being tested. These extra logs added improve further the effectiveness of CLOUDRAID without introducing any noticeable performance overhead. Our log-based approach makes it well-suited for live systems. We have applied CLOUDRAID to analyze six representative distributed systems: Apache Hadoop2/Yarn, HBase, HDFS, Cassandra, Zookeeper, and Flink. CLOUDRAID has succeeded in testing 60 different versions of these six systems (10 versions per system) in 35 hours, uncovering 31 concurrency bugs, including nine new bugs that have never been reported before. For these nine new bugs detected, which have all been confirmed by their original developers, three are critical and have already been fixed.},
keywords={Computer bugs;Concurrent computing;Cloud computing;Task analysis;Runtime;Message systems;Tools;Distributed Systems;Concurrency Bugs;Bug Detection;Cloud Computing},
doi={10.1109/TSE.2020.2999364},
ISSN={1939-3520},
month={},}
@ARTICLE{9079492,
author={Dou, Hui and Chen, Pengfei and Zheng, Zibin},
journal={IEEE Access}, title={Hdconfigor: Automatically Tuning High Dimensional Configuration Parameters for Log Search Engines},
year={2020},
volume={8},
number={},
pages={80638-80653},
abstract={Search engines are nowadays widely applied to store and analyze logs generated by large-scale distributed systems. To adapt to various workload scenarios, log search engines such as Elasticsearch usually expose a large number of performance-related configuration parameters. As manual configuring is time consuming and labor intensive, automatically tuning configuration parameters to optimize performance has been an urgent need. However, it is challenging because: 1) Due to the complex implementation, the relationship between performance and configuration parameters is difficult to model and thus the objective function is actually a black box; 2) In addition to application parameters, JVM and kernel parameters are also closely related to the performance and together they construct a high dimensional configuration space; 3) To iteratively search for the best configuration, a tool is necessary to automatically deploy the newly generated configuration and launch tests to measure the corresponding performance. To address these challenges, this paper designs and implements HDConfigor, an automatic holistic configuration parameter tuning tool for log search engines. In order to solve the high dimensional optimization problem, we propose a modified Random EMbedding Bayesian Optimization algorithm (mREMBO) in HDConfigor which is a black-box approach. Instead of directly using a black-box optimization algorithm such as Bayesian optimization (BO), mREMBO first generates a lower dimensional embedded space through introducing a random embedding matrix and then performs BO in this embedded space. Therefore, HDConfigor is able to find a competitive configuration automatically and quickly. We evaluate HDConfigor in an Elasticsearch cluster with different workload scenarios. Experimental results show that compared with the default configuration, the best relative median indexing results achieved by mREMBO can reach $2.07\times $ . In addition, under the same number of trials, mREMBO is able to find a configuration with at least a further 10.31% improvement in throughput compared to Random search, Simulated Annealing and BO.},
keywords={Optimization;Search engines;Tuning;Kernel;Predictive models;Bayes methods;Clustering algorithms;Log search engine;configuration parameter tuning;black-box optimization;Bayesian optimization;random embedding},
doi={10.1109/ACCESS.2020.2990735},
ISSN={2169-3536},
month={},}
@ARTICLE{9075180,
author={Zhao, Lin and Wang, Jiqiang and Li, Zhen and Hou, Moyu and Dong, Guofeng and Liu, Tongyu and Sun, Tong and Grattan, Kenneth T. V.},
journal={IEEE Sensors Journal}, title={Quasi-Distributed Fiber Optic Temperature and Humidity Sensor System for Monitoring of Grain Storage in Granaries},
year={2020},
volume={20},
number={16},
pages={9226-9233},
abstract={Mildew is an important issue in grain storage, due to the very expensive losses that can arise due to grain spoilage by mildew. Monitoring the conditions which allow mildew to develop is important and current monitoring technology cannot readily, or safely, be used for internal humidity monitoring of the very large quantities of grain kept in storage. In this work, a quasi-distributed, tailored fiber optic temperature and humidity sensor system suitable for use in the large granaries has been developed. Dust from the grain is a major issue and the sensor system copes well with this. Various sensor designs have been evaluated and building on prior work, a design with a humidity sensitivity of 5.8pm/%RH, a measurement error of 2%RH, and response time of 4.3 minutes, which is more than adequate for the applications discussed has been developed. Field-testing of the sensor system was carried out at a major storage facility in China, monitoring over an extended period of 4 months, giving results consistent with the outputs from other point sensors currently used in the granary. The fiber optic sensor system developed shows the additional advantages that as a quasi-distributed system, it can be used to record the temperature and humidity distribution across a longitudinal section of the large size of the grain pile, in real time, reflecting the changes in the internal moisture content. The results obtained showed the sensor system has broad market application prospects in this and other important areas of agricultural and food storage.},
keywords={Temperature sensors;Humidity;Optical fiber sensors;Sensor systems;Monitoring;Fiber gratings;Grain storage safety;fiber optic sensors;temperature and humidity monitoring;distributed fiber optic sensing},
doi={10.1109/JSEN.2020.2989163},
ISSN={1558-1748},
month={Aug},}
@ARTICLE{9004477,
author={Rinaldi, Stefano and Pasetti, Marco and Bonafini, Federico and Ferrari, Paolo and Flammini, Alessandra and Sisinni, Emiliano and Artale, Giovanni and Cataliotti, Antonio and Cosentino, Valentina and Cara, Dario Di and Panzavecchia, Nicola and Tinè, Giovanni},
journal={IEEE Transactions on Instrumentation and Measurement}, title={Design of a Time Dissemination System Using Chirp Modulation for Medium Voltage Smart Grid Applications},
year={2020},
volume={69},
number={9},
pages={6686-6695},
abstract={The monitoring and the management of smart grid require an advanced communication infrastructure. The time synchronization is among the most important services such an infrastructure should offer. Several solutions are available to disseminate the time information in a distributed system; for instance, most of the times a global positioning system (GPS) receiver is used to recover accurate time information. However, GPS receivers need a clear view of the sky. Other solutions, like the use of a network-based synchronization mechanism, require a dedicated communication infrastructure, which is not always feasible for economical reason. Thus, the use of the power grid itself to disseminate a time signal is extremely interesting. The aim of this article is to investigate the possibility to distribute a time signal over a medium voltage (MV) grid using a chirp-based modulation. The timing data are coded using IRIG-B time code. Such an approach does not require the installation of a dedicated infrastructure, and it is noninvasive, because the signal is transmitted on a different band with the respect to power line communication (PLC). The effectiveness of the approach has been experimentally evaluated on a test MV line available in a laboratory. Although the tested chirp-based symbol has a limited frequency bandwidth (80 kHz), the jitter in the identification of the start of frame is on the order of 7.5 μs , fulfilling the requirements of the most smart grid applications.},
keywords={Synchronization;Global Positioning System;Couplers;Protocols;Smart grids;Monitoring;Chirp modulation;IRIG-B;medium voltage (MV) distribution grid;smart grid;time synchronization},
doi={10.1109/TIM.2020.2975372},
ISSN={1557-9662},
month={Sep.},}
@ARTICLE{8944307,
author={Li, Fangyu and Xie, Rui and Wang, Zengyan and Guo, Lulu and Ye, Jin and Ma, Ping and Song, Wenzhan},
journal={IEEE Internet of Things Journal}, title={Online Distributed IoT Security Monitoring With Multidimensional Streaming Big Data},
year={2020},
volume={7},
number={5},
pages={4387-4394},
abstract={Internet of Things (IoT) enables extensive connections between cyber and physical “things.” Nevertheless, the streaming data among IoT sensors bring “big data” issues, for example, large data volumes, data redundancy, lack of scalability, and so on. Under big data circumstances, IoT system monitoring becomes a challenge. Furthermore, cyberattacks which threaten IoT security are hard to be detected. In this article, we propose an online distributed IoT security monitoring algorithm (ODIS). An advanced influential point selection operation extracts important information from multidimensional time-series data across distributed sensor nodes based on the spatial and temporal data dependence structure. Then, an accurate data structure model is constructed to capture the IoT system behaviors. Next, hypothesis testing is carried out to quantify the uncertainty of the monitoring tasks. Besides, the distributed system architecture solves the scalability issue. Using a real sensor network testbed, we commit cyberattacks to an IoT system with different patterns and strengths. The proposed ODIS algorithm demonstrates promising detection and monitoring performances.},
keywords={Internet of Things;Monitoring;Big Data;Data models;Security;Sensors;Distributed databases;Big data;distributed;Internet of Things (IoT) security;online},
doi={10.1109/JIOT.2019.2962788},
ISSN={2327-4662},
month={May},}
@ARTICLE{8910415,
author={Šešum-Čavić, Vesna and Kühn, Eva and Fleischhacker, Lukas},
journal={IEEE Transactions on Emerging Topics in Computational Intelligence}, title={Efficient Search and Lookup in Unstructured P2P Overlay Networks Inspired by Swarm Intelligence},
year={2020},
volume={4},
number={3},
pages={351-368},
abstract={Finding an efficient way to locate incomplete data in complex distributed systems is a challenging task and, due to dynamic nature of the Internet, requires to be updated constantly. As the problem refers to selection of an efficient search algorithm, different types of algorithms are proposed up to now. A huge complexity and dynamics presented in such systems imply a necessity of usage of an intelligent, self-organized solution. However, such intelligent algorithm should not possess an additional complexity. In this paper, we propose a new, simple and effective swarm-based metaheuristic for search and lookup in an unstructured P2P system inspired by behavior of bark beetles in nature. Also, a Physarum Polycephalum mechanism is adapted for this purpose. Both algorithms are compared with Dictyostelium discoideum (Dd)-slime mold, Gnutella, AntNet and k-Walker search mechanisms and tested by using two different models, Actor and Peer. The benchmarks measured by different metrics cover a parameter sensitivity analysis, comparative analysis and scalability analysis. Both algorithms show very promising results in terms of performance and scalability.},
keywords={Peer-to-peer computing;Overlay networks;Software algorithms;Vegetation;Heuristic algorithms;Task analysis;Search problems;Swarm-inspired intelligence;bark beetles;slime molds;unstructured P2P overlay networks;intelligent lookup},
doi={10.1109/TETCI.2019.2951813},
ISSN={2471-285X},
month={June},}
@ARTICLE{8642447,
author={Qiu, Kun and Zheng, Zheng and Trivedi, Kishor S. and Yin, Beibei},
journal={IEEE Transactions on Reliability}, title={Stress Testing With Influencing Factors to Accelerate Data Race Software Failures},
year={2020},
volume={69},
number={1},
pages={3-21},
abstract={Software failures caused by data race bugs have always been major concerns in parallel and distributed systems, despite significant efforts spent in software testing. Due to their nondeterministic and hard-to-reproduce features, when evaluating systems' operational reliability, a rather long period of experimental execution time is expected to be spent on observing failures caused by data race conditions. To address this problem, in this paper, we make two contributions. First, this paper proposes stress testing with influencing factors, in which the system runs under certain workloads for a long time with controlled stress conditions to accelerate the occurrence of data race failures. Second, it explores and formulates mathematical relationship models between data races' statistical characteristics of time to failure (TTF) or mean TTF (MTTF) and the influencing factors. Such relationship models are used for TTF/MTTF extrapolation under different operational conditions and are essential to reduce systems' reliability evaluation time. The proposed method is empirically evaluated on six applications suffering from failures caused by real-world data race bugs. Through analysis of the experimental results, we obtain several important findings: First, the reduction in the manifestation time to data race failures achieved by controlling the influencing factors is statistically significant. Second, Power model is the best-fitting model of the relationship between the MTTF and the influencing factors. Third, Power Weibull distribution is the best-fitting probability distribution between the TTF and the influencing factors. Finally, the TTF/MTTF can be accurately estimated with the approach proposed in this paper.},
keywords={Stress;Testing;Data models;Computer bugs;Instruction sets;Random access memory;Data race software failure;influencing factors;relationship model;software reliability;software stress testing;time to failure/mean time to failure (TTF/MTTF) estimation},
doi={10.1109/TR.2019.2895052},
ISSN={1558-1721},
month={March},}
@ARTICLE{9514727,
author={Takrouni, Manel and Bouhouch, Rim and Hasnaoui, Salem},
journal={The Computer Journal}, title={Simulink Implementation of the Data Distribution Service for Vehicular Controllers on Top of GBE and AFDX},
year={2019},
volume={64},
number={1},
pages={860-879},
abstract={Vehicular networks have seen major changes in the past few years in order to offer reliable and real-time capable high-speed data transmission between electrical and mechatronic components to map current and future innovative functions into distributed systems within automotive applications. In the same context, the real-time middleware data distribution service (DDS) is an appropriate alternative for the standard vehicular middleware considering that it handles quality of service (QoS) parameters including real-time ones. In this paper, we are proposing a new approach for DDS implementation and integration into the vehicular system by creating a model-based design blocks. To validate this implementation, we have used the case of the Society of Automotive Engineers (SAE) vehicle benchmark as a simulation and test model. Therefore, we designed a Simulink vehicle as specified by SAE benchmark. Then, we have introduced a new methodology to link each module to a Simulink DDS blockset. The goal of this approach is to facilitate the use of DDS with vehicular controllers and to reduce the deployment and configuration complexities associated with DDS. It will also enable distributed real-time embedded systems developers to concentrate more on the business logic of the application instead of the low-level implementation details. The final developed architecture has been tested using three different types of real-time networks: FlexRay, Gigabit Ethernet and AFDX, to demonstrate that real-time application’s QoS are always met using this model.},
keywords={DDS;latency;GBE;AFDX;FlexRay;SAE benchmark},
doi={10.1093/comjnl/bxaa176},
ISSN={1460-2067},
month={Nov},}
@INPROCEEDINGS{9173481,
author={Lin, Qin and Pan, Ling and Jia, Mingquan and Liu, Hongwei and Wu, Mingqin and Zhang, Hao and Zhong, Yu},
booktitle={2019 IEEE International Conference on Signal, Information and Data Processing (ICSIDP)}, title={A lightweight edge computing platform designed on embeded system},
year={2019},
volume={},
number={},
pages={1-4},
abstract={Edge computing platform is a critical component to form next-generation intelligent equipment. In an effort to support new military and industrial applications with more flexible computing power, a new lightweight edge computing platform is introduced in this paper. This platform is fully based on embedded hardware, such as ARM, DSP and FPGA. Platform's management and scheduling architecture is developed using Go language to minimize code size and dependence on three-party libraries. In addition, the management and scheduling commands transmit across the platform over the Ethernet, while data transmit among computing nodes via the RapidIO network to isolate the commands and data channels. An experimental letter count application was performed to validate the feasibility of the platform and also supply optimization hint with a data throughput test. This work has the potential to reduce developer's experience in parallel and distributed systems, and then the cost and time in development of embedded intelligent equipment.},
keywords={lightweight edge computing platform;intelligent equipment;embedded platform;embedded system;Go programming language},
doi={10.1109/ICSIDP47821.2019.9173481},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9165448,
author={Globa, Larysa and Gvozdetska, Nataliia},
booktitle={2019 International Conference on Information and Telecommunication Technologies and Radio Electronics (UkrMiCo)}, title={Energy efficient workload processing in distributed computing environment modeling},
year={2019},
volume={},
number={},
pages={1-6},
abstract={Global digital transformation requires more productive large scale distributed systems. Such systems should meet lots of requirements, such as high availability, low latency, reliability. However, new challenges become more and more important nowadays. One of them is energy efficiency of large scale computing systems. Many service providers prefer to use cheap commodity servers in their distributed infrastructure, what makes the problem of energy efficiency even harder because of hardware inhomogeneity. In this chapter an approach to finding balance between performance and energy efficiency requirements within inhomogeneous distributed computing environment is proposed. The main idea of the proposed approach is to use each node's individual energy consumption models in order to generate distributed system scaling patterns based on the statistical daily workload, and then adjust these patterns to match the current workload, while using PCPB scheduling strategy to optimize hardware utilization. An approach is tested using Matlab modelling. As a result of applying the proposed approach, large-scale distributed computing systems save energy while maintaining a fairly high level of performance and meeting the requirements of the service level agreement (SLA).},
keywords={Communications technology;Energy efficiency;Distributed computing;Computational modeling;energy efficiency;performance;SLA;distributed computing system;scheduling;horizontal scaling},
doi={10.1109/UkrMiCo47782.2019.9165448},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{9124816,
author={Fu, Yanfang and Hao, Lingling and Guo, DengDeng},
booktitle={2019 IEEE International Conference on Unmanned Systems and Artificial Intelligence (ICUSAI)}, title={Application Research of Distributed Simulation System Based on Data Distribution},
year={2019},
volume={},
number={},
pages={268-273},
abstract={DDS (Data Distribution Service) is a set of API and interoperability protocol specifications developed by OMG. It adopts a datacentric publish/subscribe architecture to meet the demand of high efficiency and real-time communication, This paper designed a kind of DDS based on data distribution system and used in distributed simulation system at first through the analysis of DDS communication principle. Then, it achieved the system from the design of communication interface to the construction of distributed simulation system. And by means of Quality of Service (QoS) Strategy in DDS, the particularly modified QoS Strategy designed for distributed simulation system is proposed. Finally, through the analysis on effect of disturbance (system average transmission delay, jittering and the amount of packet loss) on real-time and reliability requirements of system, the test results indicates that The distributed simulation system based on DDS can meet the real-time and reliability of general distributed systems.},
keywords={Data distribution of service;Publish/Subscribe;Quality of Service;Topic},
doi={10.1109/ICUSAI47366.2019.9124816},
ISSN={},
month={Nov},}
@INPROCEEDINGS{9033010,
author={Khan, Rabia and Schulz, Noel N. and Nasir, Mashood},
booktitle={2019 IEEE Global Humanitarian Technology Conference (GHTC)}, title={Distribution Loss Analysis of DC Microgrids for Rural Electrification},
year={2019},
volume={},
number={},
pages={1-8},
abstract={Rural electrification is important to mitigate energy poverty and improve human development in remote unelectrified regions. Optimal planning and designing of the microgrid architectures are required for providing electricity access. DC microgrid architectures with static generation and load demand have some limitations with respect to the planning side. Time-based dynamic load demand and generation gives an insight into a more practical and real-time approach. In this paper, a detailed distribution loss analysis of both centralized and distributed microgrid architectures with dynamic load and generation profiles is presented. The distributed architecture consists of individual household consumers that form independent nanogrids, which can operate in both standalone and integrated manner to make the microgrid scalable. The centralized architecture comprises of distributed load with centralized generation and storage. Both centralized and distributed systems with ring and radial orientations are considered and their performance is evaluated using a modified Newton Raphson method for DC systems. A comparative distribution loss analysis with various conductor sizes and voltage levels shows that the distributed ring architecture is significantly advantageous based on low distribution losses, high efficiency, and low voltage drop. It offers an additional feature of scalability and low capital cost. The microgrid architectures can be tested for any region by using the real-time solar irradiation data, weather conditions, and the dynamic load demand of that community.},
keywords={Microgrids;Load modeling;Conductors;Resistance;Computer architecture;Newton method;Conferences;Rural Electrification;DC microgrids;Central Generation;Distributed Generation},
doi={10.1109/GHTC46095.2019.9033010},
ISSN={2377-6919},
month={Oct},}
@INPROCEEDINGS{9077390,
author={Smith, Phillip and Luong, Anh and Sarkar, Shamik and Singh, Harsimran and Patwari, Neal and Kasera, Sneha and Derr, Kurt and Ramirez, Samuel},
booktitle={2019 IEEE 16th International Conference on Mobile Ad Hoc and Sensor Systems (MASS)}, title={Sitara: Spectrum Measurement Goes Mobile Through Crowd-Sourcing},
year={2019},
volume={},
number={},
pages={46-54},
abstract={Software-defined radios (SDRs) are often used in the experimental evaluation of next-generation wireless technologies. While crowd-sourced spectrum monitoring is an important component of future spectrum-agile technologies, there is no clear way to test it in the real world, i.e., with hundreds of users each carrying an SDR while uploading data to a cloud-based controller. Current fully functional SDRs are bulky, with components connected via wires, and last at most hours on a single battery charge. To address the needs of such experiments, we design and develop a compact, portable, untethered, and inexpensive SDR we call Sitara. Our SDR interfaces with a mobile device over Bluetooth 5 and can function standalone or as a client to a central command and control server. The Sitara offers true portability: it operates up to one week on battery power, requires no external wired connections and occupies a footprint smaller than a credit card. It transmits and receives common waveforms, uploads IQ samples or processed receiver data through a mobile device to a server for remote processing and performs spectrum sensing functions. Multiple Sitaras form a distributed system capable of conducting experiments in wireless networking and communication in addition to RF monitoring and sensing activities. In this paper, we describe our design, evaluate our solution, present experimental results from multi-sensor deployments and discuss the value of this system in future experimentation.},
keywords={Mobile Systems, Spectrum Monitoring, Wireless Networks, Crowd-sourcing, Software-Defined Radio},
doi={10.1109/MASS.2019.00015},
ISSN={2155-6814},
month={Nov},}
@INPROCEEDINGS{9058238,
author={Stewart, Christopher},
booktitle={2019 IEEE National Aerospace and Electronics Conference (NAECON)}, title={Continuous Adaptive Runtime Integration Testbed for Complex and Autonomous Systems},
year={2019},
volume={},
number={},
pages={332-334},
abstract={The integration of complex, distributed systems typically takes thousands of man-hours and years of detailed design and testing. Despite all this effort, the effectiveness of classical system integration becomes a major issue when one adds the runtime adaptive behavior that is increasingly vital for dynamic, autonomous systems influenced by their surroundings. Self-integrating systems proposed in recent research strive to autonomously integrate new components, reducing design and testing costs. However, these systems are challenging to validate, especially at scale. This paper describes CHARIOT, Continuous High-level Adaptive Runtime IntegratiOn Testbed, which allows for different approaches and systems to be dynamically deployed, assessed and compared on a shared common platform. CHARIOT uses self-flying drones and self-driving cars to validate autonomous signal integration. This paper will discuss our early work in designing the CHARIOT architecture and integration protocol.},
keywords={Runtime;Computational modeling;Conferences;Adaptive systems;Autonomous systems;System integration;Software},
doi={10.1109/NAECON46414.2019.9058238},
ISSN={2379-2027},
month={July},}
@INPROCEEDINGS{9029122,
author={Jahn, Uwe and Poliakov, Vladimir and Gardi, Meghadoot and Schulz, Peter and Wolff, Carsten},
booktitle={2019 20th International Conference on Parallel and Distributed Computing, Applications and Technologies (PDCAT)}, title={Introducing PulseAT: A Tool for Analyzing System Utilization in Distributed Systems},
year={2019},
volume={},
number={},
pages={271-276},
abstract={For the development and maintenance of distributed systems, it is useful to analyze the system condition and utilization for each hardware component. With pulseAT, a tool has been developed which collects that system utilization systematically with lightweight pulseAT Agents. The hierarchical structure of pulseAT allows having all system utilization data at one place on a pulseAT Manager to show an overall current health condition of the system. A cloud-based pulseAT Analyzer stores the data into a time-based database to support long-term analyses and to process analyzing algorithms, e.g., to forecast future health conditions. This paper describes the structure of pulseAT, main concepts, e.g., how the response time for each component is calculated. Some technical details of the implementation are shown. Finally, it describes how pulseAT has been tested on a mobile robot, the DAEbot.},
keywords={Hardware;Real-time systems;Monitoring;Task analysis;Tools;Software;Data visualization;Analysis;Condition Monitoring;Real-time;Distributed Systems;Embedded Systems;Modular Systems;Cloud Computing;IoT;Robotics},
doi={10.1109/PDCAT46702.2019.00057},
ISSN={2640-6721},
month={Dec},}
@INPROCEEDINGS{9005504,
author={Geldenhuys, Morgan K. and Thamsen, Lauritz and Gontarskay, Kain Kordian and Lorenz, Felix and Kao, Odej},
booktitle={2019 IEEE International Conference on Big Data (Big Data)}, title={Effectively Testing System Configurations of Critical IoT Analytics Pipelines},
year={2019},
volume={},
number={},
pages={4157-4162},
abstract={The emergence of the Internet of Things has seen the introduction of numerous connected devices used for the monitoring and control of even Critical Infrastructures. Distributed stream processing has become key to analyzing data generated by these connected devices and improving our ability to make decisions. However, optimizing these systems towards specific Quality of Service targets is a difficult and time-consuming task, due to the large-scale distributed systems involved, the existence of so many configuration parameters, and the inability to easily determine the impact of tuning these parameters.In this paper we present an approach for the effective testing of system configurations for critical IoT analytics pipelines. We demonstrate our approach with a prototype that we called Timon which is integrated with Kubernetes. This tool allows pipelines to be easily replicated in parallel and evaluated to determine the optimal configuration for specific applications. We demonstrate the usefulness of our approach by investigating different configurations of an exemplary geographically-based traffic monitoring application implemented in Apache Flink.},
keywords={Pipelines;Testing;Production;Measurement;Distributed databases;Tuning;Quality of service;Distributed Stream Processing;Internet of Things;Configuration Testing;Quality of Service},
doi={10.1109/BigData47090.2019.9005504},
ISSN={},
month={Dec},}
@INPROCEEDINGS{9002401,
author={Kumar, Mukesh and Singh, Awadhesh Kumar},
booktitle={2019 International Conference on Communication and Electronics Systems (ICCES)}, title={FDDS: An Integrated Conceptual FDDS Framework for DDS Based Middleware},
year={2019},
volume={},
number={},
pages={1952-1956},
abstract={Data Distribution Services (DDS) has changed the way of the development process for complex distributed systems by tightly coupling QoS policies for achieving desired functionality at execution time. DDS middleware can be configured to handle edge-to-edge quality of service (QoS) policies. However, unpredictable environment imposes a greater challenge in the design and execution of these systems. The publish/subscribe middleware has improved the scalability and interoperability of the distributed systems. The stringent Quality of Services (QoS) standards imposes a greater challenge to developers. We propose a framework based approach for designing, development, and testing of distributed systems on top of Real-Time Innovations (RTI) and Open-Splice (OSPL) DDS middleware. The framework consists of ACGF Module which generates the code for application. DCF Module provides the abstraction from core functionality of DDS by using templates. Finally, ATF Module will help developer to test their application. This work will be beneficial for the development of different types of DDS applications rapidly.},
keywords={Quality of service;Middleware;Real-time systems;Conferences;Standards;Testing;Data Distribution Services (DDS);Object Management Group(OMG);Publisher/Subscriber Architecture;QoS;RTI;OSPL},
doi={10.1109/ICCES45898.2019.9002401},
ISSN={},
month={July},}
@INPROCEEDINGS{8978295,
author={Pandey, Archit and Bhasi, Mohit and Chandrasekaran, K.},
booktitle={2019 Global Conference for Advancement in Technology (GCAT)}, title={VoteChain: A Blockchain Based E-Voting System},
year={2019},
volume={},
number={},
pages={1-4},
abstract={In the past, electronic voting systems have not seen widespread adoption due to data privacy concerns. Previously proposed e-voting systems make use of a central database to store data, resulting in the servers used to store these databases being a single point of failure. These systems have also been found to be vulnerable to DoS attacks, leading to concerns over their reliability. Blockchains have been used to build secure and scalable distributed systems which have shown several benefits over centralized systems. They have seen uses in sectors ranging from finance and healthcare to food and energy. In this paper, we present VoteChain, a blockchain based voting system to help bring transparency and security to polls. We report on our implementation of VoteChain, as well as the results obtained in testing the system in a real-world poll which prove that such a system can be used in practice for large-scale elections.},
keywords={E-Voting;Blockchain},
doi={10.1109/GCAT47503.2019.8978295},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8971962,
author={Wan, Guangxi and Nie, Zhenbang and Wang, Peng and Zeng, Peng},
booktitle={2019 IEEE 17th International Conference on Industrial Informatics (INDIN)}, title={The Component-based Design Method for Agent-based Multi-AGV System},
year={2019},
volume={1},
number={},
pages={647-654},
abstract={The agent-based multi-AGV system is envisioned to be highly flexible and adaptable for the evolving requirement of industrial automation. However, the current design method of the automation software remains monolithic and is still based on the centralized control mechanism, which makes the system engineers struggle with the complexity of the development process under the distributed system requirements. This paper applies component-based design method and the embedded agent architecture for multi-AGV system to reduce the agent design effort and improve the flexibility of the design process, and proposes a task-oriented component granularity partitioning method. A multi-AGV system in laboratory level test scenario verifies the easy-and-flexible-using of this investigation.},
keywords={component-based design;agent;AGV;IEC 61499;function block},
doi={10.1109/INDIN41052.2019.8971962},
ISSN={2378-363X},
month={July},}
@INPROCEEDINGS{8974670,
author={Sridevi, J and Rani, V. Usha and Rao, B. Loveswara},
booktitle={2019 1st International Conference on Electrical, Control and Instrumentation Engineering (ICECIE)}, title={Integration of Renewable DGs to Radial Distribution System for Loss Reduction and Voltage Profile Improvement},
year={2019},
volume={},
number={},
pages={1-6},
abstract={The necessity for smart electrical systems having minimum power loss and environmental impact is providing impetus to go for Distributed Generations (DGs) which may offer several other advantages such as reduced transmission and distribution system resources, increased reliability, better power quality. Proper placement of DGs, such as wind turbines and photovoltaic units, in the distribution system is still a very challenging issue for obtaining their maximum potential benefits. Optimal location and sizing of distributed generation (DG) is a key for loss minimization and improvement of voltage profile in radial distribution systems. The present paper proposes a loss sensitivity factor for DG placement in distribution network for power loss minimization while maintaining the voltage profile in the system within the specified limits. An Analytical approach is proposed for sizing of renewable DGs such as solar, wind and Hydel Units for minimizing active power loss, annual operation costs (installation, maintenance, and active power loss costs). This is tested in different cases. All cases are compared to identify the superiority of the proposed method. The proposed method is tested on IEEE 33 Bus Radial Distributed System to demonstrate the performance and effectiveness in ETAP software.},
keywords={Distributed Generation;Loss Sensitivity;optimal location;Wind turbine modeling;Photo Voltaic module;Sizing of DG;Loss Minimization;Voltage profile},
doi={10.1109/ICECIE47765.2019.8974670},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8960794,
author={Baena, Jorge Andrés and Duitama, John Freddy},
booktitle={2019 Congreso Internacional de Innovación y Tendencias en Ingenieria (CONIITI )}, title={Machine Learning-based Approach to Detect Online Performance Problems in Distributed Systems},
year={2019},
volume={},
number={},
pages={1-6},
abstract={Detecting the root cause of a performance problem in a distributed system is a complex and costly task. Identifying the fault, which can be internal or external, requires a deep knowledge of the system, and tools that allow processing and filtering large amounts of information. This paper describes a methodology to identify performance problems on distributed systems. The operation flows are inferred from log files and are used to measure a performance indicator. This information is complemented by system data, such as metrics of each node (CPU, memory, disk), that is analyzed with two machine learning techniques, multivariate regression and one class support vector machine (OCSVM), with the purpose of predicting the expected performance and the presence of unusual events. The mentioned model is implemented in a tool called Logmapper, that is used to validate the approach in a controlled test environment, where several types of failures that affect performance were applied. The validation results are composed of the validation metrics of the learning methods and the response given by the tool when failures were induced in the distributed system.},
keywords={performance;machine learning;OCSVM;distributed systems;log files},
doi={10.1109/CONIITI48476.2019.8960794},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8952191,
author={},
booktitle={2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, title={Keynotes},
year={2019},
volume={},
number={},
pages={36-38},
abstract={With the development of big data, machine learning, and AI, existing software engineering techniques must be reimagined to provide the productivity gains that developers desire. This talk will review emerging roles of data scientists and the tools they need to build scalable, correct, and efficient software for a data centric world. Kim will present a large-scale study of about 800 data scientists in collaboration with Microsoft Research, which looked at data scientists' educational background, problem topics that they work on, tools they use, and activities. From the gathered data, she has identified nine distinct clusters of data scientists and best practices and challenges faced by each cluster. In the second half of this talk, she will discuss the needs of retargeting SE research community's directions to address new challenges in the era of data-centric software development. In particular, she will detail some examples of her group's work that re-invents debugging and testing for big data distributed systems such as Apache Spark. She will conclude with open SE problems in ML and heterogeneous computing that support data-centric software development.},
keywords={},
doi={10.1109/ASE.2019.00010},
ISSN={2643-1572},
month={Nov},}
@INPROCEEDINGS{8924314,
author={Martynyuk, Oleksandr and Drozd, Oleksandr and Tamim, Ahmesh and Van Thuong, Bui and Sachenko, Anatoliy and Mykhailova, Halyna and Dombrovskyi, Mykhaylo},
booktitle={2019 10th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)}, title={Hierachical Model of Behavior On-line Testing for Distributed Information Systems},
year={2019},
volume={2},
number={},
pages={724-729},
abstract={The paper presents a three-level decomposition model of behavioral on-line testing for distributed information systems of the application level. The model is based on the representation of distributed information systems by a three-tier composition of Petri nets, the identification of reference positions/transitions, recognition of behavioral reference fragments, hierarchical inheritance of the recognized behavior. In Petri nets space-time check models are distinguished, which allow the decomposition of the behavior of a distributed system. The following tasks are solved - definition of distributed information systems analytical models - hierarchical extended Petri nets with structural spatial and temporal decomposition of processes and construction of multi-level analytical models of behavioral online testing of distributed information systems - a multi-level fixed extended behavior of the hierarchical extended Petri nets with additional recognition and encapsulation operations, relations of preordering and inheritance, that are defined on it. Behavioral online testing assumes the preceding definition of recognition of reference positions/transitions, reference fragments and their structures and is applicable for model of project verification and verification of implementations for real distributed information systems.},
keywords={C# languages;distributed information system;behavioral on-line testing;Petri net;identifier;behavior fragment},
doi={10.1109/IDAACS.2019.8924314},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8912390,
author={Kalakova, Aidana and Nunna, H. S.V. S. Kumar and Jamwal, Prashant K. and Doolla, Suryanarayana},
booktitle={2019 IEEE Industry Applications Society Annual Meeting}, title={Genetic Algorithm for Dynamic Economic Dispatch with Short-Term Load Forecasting},
year={2019},
volume={},
number={},
pages={1-6},
abstract={The shift from conventional energy sources to renewable sources leads to high penetration of Distributed Generators (DGs) in Active Distribution Networks (ADNs). This trend along with other factors causes major problems in ADNs such as energy management, voltage imbalances, poor efficiency, and reliability. To overcome these challenges the article suggests a novel approach by optimally scheduling energy using advanced methods for Dynamic Economic Dispatch (DED) algorithms with machine learning based load forecasts. Initially, the proposed system deploys Short-Term Load Forecasting (STLF) on the load buses based on the previously observed patterns using Multilayer Artificial Neural Networks (MANN). Then, the system implements a Genetic Algorithm based DED with major operational and network constraints. The proposed methods are integrated and tested on the IEEE 30-bus network using Kazakhstan's power plant and electricity demand parameters. The proposed system has shown improvement in the reliability and efficiency of the distributed system. Since the proposed methodology is generic and was verified on the standard radial network it can be further implemented in other systems.},
keywords={Load forecasting;Generators;Economics;Cost function;Genetic algorithms;Heuristic algorithms;Wind power generation;Distributed Generator (DG);Distributed Network (DN);Active Distribution Network (ADN);Dynamic Economic Dispatch (DED);Genetic Algorithm (GA);Multilayer Neural Network (MNN);Short-Term Load Forecasting (STLF)},
doi={10.1109/IAS.2019.8912390},
ISSN={2576-702X},
month={Sep.},}
@INPROCEEDINGS{8881803,
author={Aqib, Muhammad and Ukil, Abhisek},
booktitle={2019 IEEE Innovative Smart Grid Technologies - Asia (ISGT Asia)}, title={Voltage Sensitivity Analysis and Demand Dispatch Option of Electric Vehicle in Smart Grid},
year={2019},
volume={},
number={},
pages={218-223},
abstract={The increasing integration of Electric Vehicle (EV) poses unique challenges for voltage and frequency regulation in electric distribution grids. EVs are at present the most reliable substitutes for standard fossil fuel-based cars. The mobility and variability due to EV loads exacerbates the problem of voltage regulation in distribution grids that are characterized by X/R ratios. This paper examines the effect of different operations and impact of EV on the system having voltage sensitivity analysis and choosing an appropriate time of charging and discharging cycles. Coding was done on the distributed system simulator for obtaining the results. To inspect its effectiveness and the performance of EV, IEEE-34 Nodes Test Feeder network is used. The adopted platform to perform the designed task is designed on Open Distribution System Simulator (OpenDSS).},
keywords={Shape;Sensitivity analysis;Power system reliability;Reliability;Batteries;Electric vehicle charging;Sequences},
doi={10.1109/ISGT-Asia.2019.8881803},
ISSN={2378-8542},
month={May},}
@INPROCEEDINGS{8869467,
author={Lesi, Vuk and Jakovljevic, Zivana and Pajic, Miroslav},
booktitle={2019 24th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)}, title={Synchronization of Distributed Controllers in Cyber-Physical Systems},
year={2019},
volume={},
number={},
pages={710-717},
abstract={Due to misaligned clock sources, distributed control in Cyber-Physical Systems (CPS) requires not only synchronous execution of control algorithms on distributed system components, which we refer to as cyber-synchronization, but also appropriate generation of actuation signals-we refer to this as physical-synchronization. In this paper, we define general requirements for cyber-physical synchronization, as well as show their use on a specific real-world application-distributed motion control for reconfigurable manufacturing systems. We present synchronization challenges in such systems and investigate effects of synchronization errors on the overall system functionality (i.e., machining accuracy). Furthermore, we introduce a low-cost synchronization scheme that can be implemented with of-the-shelf components and validate it on standardized accuracy tests with 2D configurations of industry-grade single-axis robots. We show that our cyber-physical synchronization techniques ensure minimal accuracy impairment of distributed motion control without introducing significant cost/overhead to system design.},
keywords={Synchronization;Clocks;Motion control;Hardware;Frequency control;Protocols;Robots;cyber-physical synchronization;distributed position control;reconfigurable manufacturing systems},
doi={10.1109/ETFA.2019.8869467},
ISSN={1946-0759},
month={Sep.},}
@INPROCEEDINGS{8867600,
author={Styugin, Mikhail A. and Zolotarev, Vyacheslav V. and Parotkin, Nikolay Y.},
booktitle={2019 International Russian Automation Conference (RusAutoCon)}, title={Open Information Systems Protection by Its Continuous Change},
year={2019},
volume={},
number={},
pages={1-7},
abstract={The number of distributed systems with open code increased rapidly in recent years worldwide. Most common are distributed registry systems (blockchains). It contains cryptographic algorithms in the base and logics, which can be used to discredit them in the long-term perspective. Method of open distributed systems continuous change with the aim of its protection against discredit was suggested in frames of this paper. Unpredictable system's algorithms change does not allow to use knowledge, gained by an attacker during system's examination, or to perform long-term preparation or resources redistribution with the aim of its discredit. Methodology that allows proving an unpredictable character of information system modification, despite accessibility of its program code for all participants and procedure of new algorithms generation in untrusted program environment, was suggested in this observation. The process can be built on the base of recursive program modification by itself. Another method is to use an open procedure of modification by system's participants with confirmation of changes correctness according to Proof of Necessity of Change - PNC model. A prototype of distributed registry with the protection against examination (BlurChain) was suggested as an example of methodology implementation. A system to confirm transaction history integrity that is similar to classic block chain functions can be built based on the prototype considered. Prototype tests showed the ability of its practical use in tasks of distributed registry construction.},
keywords={Resistance;Cryptography;Blockchain;Hash functions;Prediction algorithms;Information systems;Software algorithms;Information protection;information security;block chain;distributed registry;protection against research;invisibility;diversification;protection based on moving target;automatic modification},
doi={10.1109/RUSAUTOCON.2019.8867600},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8858458,
author={Sengupta, Souvik and Kahvazadeh, Sarang and Masip-Bruin, Xavi and Garcia, Jordi},
booktitle={2019 IEEE 24th International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD)}, title={SFDDM: A Secure Distributed Database Management in Combined Fog-to-Cloud Systems},
year={2019},
volume={},
number={},
pages={1-7},
abstract={Technological revolutions have greatly increased the use of IoT devices for our daily life. Driving the fact that everything surrounding us is getting connected what turns into an unstoppable increase in the amount of data produced. This data represents the state of diverse environmental events and helps to control a large set of distinct activities. So, accurate and secure management of this data is essential for any computing platform. Moreover, in order to provide real-time services in a distributed system (i.e., smart city), the data should be properly and securely managed. It is well known that shifting these tasks to the edge (i.e., near to the end users), highly facilitates these two objectives. The recently proposed Fog-to-Cloud (F2C) model is intended to enable data processing near to the edge, which helps to get better latency-sensitive services. However, some challenges remain to accurately and securely manage this data over the system, mainly due to the distributed F2C nature. Thus, considering these facts and challenges, in this paper we propose an architectural solution aimed at building a secure distributed database for F2C systems. Then, considering a real-case scenario, we perform some tests to measure the performance of our proposing schema. Finally, by comparing the performance between traditional cloud, fog/edge based execution model and our proposing SFDDM, we validate the effectiveness of our proposing schema.},
keywords={Security;Distributed databases;Smart cities;Monitoring;Cloud computing;Air quality;Fog-to-Cloud (F2C);Distributed Database;Internet-of-Thing (IoT);Security and Privacy},
doi={10.1109/CAMAD.2019.8858458},
ISSN={2378-4873},
month={Sep.},}
@INPROCEEDINGS{8855636,
author={Zhang, Zhongyi and Zheng, Chao and Yang, Wei and Liu, Yang and Yang, Rong and Liu, Qingyun},
booktitle={2019 IEEE 21st International Conference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)}, title={Tensor: A Transaction-Oriented Low-Latency and Reliable Data Distribution Scheme for Multi-IDCs Based on Redis},
year={2019},
volume={},
number={},
pages={2351-2359},
abstract={In order to ensure the data security, high availability of services and good access performance, more and more large-scale distributed systems are deployed across Internet Data Centers(IDCs). When different parts of distributed systems work cooperatively, critical data such as configuration and control information will be frequently exchanged across IDCs. Faced with the latency and reliability challenges introduced by cross-IDC data distribution, we propose a Redis-based low-latency data publish/subscribe framework: Tensor. In order to deal with the problems of data loss and duplication caused by network anomalies or cluster node failures, we design a transaction-oriented information transfer mechanism in Tensor to guarantee the eventual consistency in cross-IDC data distribution. To improve the data synchronization performance, we optimized Redis's replication mechanism to make it better suit the unstable network links between cross-area IDCs. What's more, we design an intelligent log analysis based system bottleneck prediction method and a service discovery oriented system failover strategy to ensure the high availability of data distribution service. An extensive set of tests on Tensor in the production environment prove the low-latency and high-reliability of its data distribution service.},
keywords={Conferences;High performance computing;Smart cities;Data science;cross-IDC data distribution, transaction oriented, Replication Backlog, intelligent log analysis},
doi={10.1109/HPCC/SmartCity/DSS.2019.00327},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8845043,
author={Talluri, Sacheendra and Iosup, Alexandru},
booktitle={IEEE INFOCOM 2019 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, title={Efficient Estimation of Read Density when Caching for Big Data Processing},
year={2019},
volume={},
number={},
pages={502-507},
abstract={Big data processing systems are becoming increasingly more present in cloud workloads. Consequently, they are starting to incorporate more sophisticated mechanisms from traditional database and distributed systems. We focus in this work on the use of caching policies, which for big data raise important new challenges. Not only they must respond to new variants of the trade-off between hit rate, response time, and the space consumed by the cache, but they must do so at possibly higher volume and velocity than web and database workloads. Previous caching policies have not been tested experimentally with big data workloads. We address these challenges in this work. We propose the Read Density family of policies, which is a principled approach to quantify the utility of cached objects through a family of utility functions that depend on the frequency of reads of an object. We further design the Approximate Histogram, which is a policy-based technique based on an array of counters. This technique promises to achieve runtime-space efficient computation of the metric required by the cache policy. We evaluate through trace-based simulation the caching policies from the Read Density family, and compare them with over ten state-of-the-art alternatives. We use two workload traces representative for big data processing, collected from commercial Spark and MapReduce deployments. While we achieve comparable performance to the state-of-art with less parameters, meaningful performance improvement for big data workloads remain elusive.},
keywords={Big Data;Histograms;Cloud computing;Mathematical model;Arrays;Data models;Conferences},
doi={10.1109/INFCOMW.2019.8845043},
ISSN={},
month={April},}
@INPROCEEDINGS{8796872,
author={Cao, Wu and Kang, Haotian and Liu, Kangli and Wang, Shunyu and Zhao, Jianfeng and Ji, Xiaochun and Zhao, Jun},
booktitle={2019 10th International Conference on Power Electronics and ECCE Asia (ICPE 2019 - ECCE Asia)}, title={Improved Switching Ripple Suppression Strategy for Multi-Paralleled Grid-Connected Inverters System},
year={2019},
volume={},
number={},
pages={2336-2343},
abstract={The switching operations of inverters in the distributed system inevitably introduce high-frequency harmonics into grid, which may make influences on electrical devices. To address it, this paper proposes a ripple suppression strategy based on active cancellation combined with passive filtering. The active cancellation is realized by carrier-phase-shift (CPS) technique and the passive filtering is achieved by filters. Furthermore, two component-shared LCL-type filters improved by traditional LCL filters are studied from the points of resonance and accuracy of grid-injected current. To valid the effectiveness of the proposed strategy, simulations and experimental test are carried out. The results show that the proposed strategy can effectively suppress high-frequency harmonics and two component-shared LCL-type filters can become promising filters for the multi-paralleled grid-connected inverters.},
keywords={Inverters;Power harmonic filters;Passive filters;Switches;Harmonic analysis;Power system stability;Multi-parelleled grid-connected;switching ripples;component-shared;LCL filters},
doi={10.23919/ICPE2019-ECCEAsia42246.2019.8796872},
ISSN={2150-6086},
month={May},}
@ARTICLE{8792156,
author={Wang, Shangping and Tang, Xixi and Zhang, Yaling and Chen, Juanjuan},
journal={IEEE Access}, title={Auditable Protocols for Fair Payment and Physical Asset Delivery Based on Smart Contracts},
year={2019},
volume={7},
number={},
pages={109439-109453},
abstract={With the rapid development of electronic information technology, online transaction will gradually surpass traditional market transaction, among which online payment and asset delivery become the focus of attention. But in fact, due to the incomplete third-party payment mechanism and the intrusion risk of various charging Trojan, it is easy to cause a trust crisis. The existing centralized framework often leads to information asymmetry between the two parties. Therefore, how to realize the fairness of payment and the auditability of assets in the distributed system is a challenging problem. The emerging blockchain technology provides a new method with its openness, transparency and verifiability. Existing researches do not provide a complete shopping model for consumers, most of which focuses on payments or only on asset delivery. In this paper, we propose an auditable fair payment and physical asset delivery protocol based on smart contracts. Three types of smart contracts are designed to achieve reliable and fair payment among merchants, consumers and logistics companies. The traceability and auditability of blockchain provide an effective method to audit assets and data sharing in the whole transportation. In view of the phenomenon of goods being switched, the way of ”pre-verification” is added. In order to prevent the illegal elements to fake pickup code, induce consumers to conduct illegal operations, cause property loss, in our system the pickup codes are generated by consumers to reduce the risk of fraud. In addition, our plan designs a complete return process for the first time, providing better service experience and higher efficiency for consumers. Finally, all the contracts involved in the scheme are implemented and deployed on the ethereum test network. The results of security analysis and evaluation showed that our scheme was improved in cost, with high security and availability.},
keywords={Blockchain;Smart contracts;Logistics;Transportation;Companies;Bitcoin;Online transactions;blockchain;assets audit;smart contract},
doi={10.1109/ACCESS.2019.2933860},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8778220,
author={Sanders, William S. and Srivastava, Srishti and Banicescu, Ioana},
booktitle={2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)}, title={A Container-Based Framework to Facilitate Reproducibility in Employing Stochastic Process Algebra for Modeling Parallel Computing Systems},
year={2019},
volume={},
number={},
pages={373-381},
abstract={Scientific applications are increasingly complex and domain specific, and the underlying architectures of the parallel and distributed systems on which they are executed also continue to grow in complexity. As these high performance parallel and distributed computing applications and environments continue to grow both in complexity and computing power, there is an increasing financial cost associated with both the acquisition and maintenance of those systems. Therefore, the ability to model the performance of these applications and systems before and during their development and deployment to guide cost-effective decisions about their resources and configurations is highly important to the designers of those applications and systems. Performance Evaluation Process Algebra (PEPA) is a modeling language and framework for modeling parallel and distributed computing and communication applications and systems, and numerous examples are present in the literature where PEPA has been utilized to model these systems for evaluating or predicting their performance using various metrics, including throughput, utilization, and robustness. Since its development, the PEPA modeling framework has been expanded to model biological systems and networks (Bio-PEPA), and massive (on the order of ~10^129 components) homogeneous systems with Grouped PEPA (GPEPA). PEPA and its derivatives are implemented in a variety of ways, ranging from plug-ins integrated with the Eclipse integrated development environment to standalone command-line based interpreters, each with their own unique and often challenging installation and configuration requirements. To help enable other researchers to more easily utilize these frameworks and facilitate increased and robust reproducibility across end-user platforms, we present and make available containerized versions of a number of these PEPA frameworks. We have validated the functionality of these containers by testing them with models available from the research community that utilizes PEPA. These containers serve as a readily available resource for the community and can be executed on any environment capable of executing the underlying containerization framework.},
keywords={Computational modeling;Biological system modeling;Data models;Algebra;Containers;Markov processes;Application virtualization, Reproducibility of results, Performance modeling, Performance evaluation, Robustness analysis, Parallel computing, Process algebra, Scalability},
doi={10.1109/IPDPSW.2019.00070},
ISSN={},
month={May},}
@ARTICLE{8768380,
author={Viveros Martínez, Yair and López Domínguez, Eduardo and Hernández Velázquez, Yesenia and Domínguez Isidro, Saúl and Medina Nieto, María Auxilio and De La Calleja, Jorge},
journal={IEEE Access}, title={Layered Software Architecture for the Development of Third-Generation Video Surveillance Systems},
year={2019},
volume={7},
number={},
pages={98507-98521},
abstract={Mobile distributed systems of third-generation video surveillance (MDSV) have become a useful tool to provide multiple security services to people. For this type of systems, three key aspects must be carried out: 1) protection, which consists of preventing undesirable events; 2) detection, which refers to determining the exact moment in which the event occurred; and 3) reply, in this regard, actions such as activating alarms and generating warnings are executed. Previous works have proposed software architectures to development video surveillance systems on mobile distributed systems (MDS). However, these architectures focus mainly on providing services/aspects of protection and detection; without considering in its design the requirements that arise from the characteristics of the MDS, such as limited processing and storage capacities of devices, frequent disconnections, among others. In this paper, we introduce a layered software architecture to build MDSV. The proposed architecture considers and satisfies the requirements that arise from the critical aspects of protection, detection, and reply, including the characteristics of the MDS. Based on our architecture, an MDSV prototype was implemented. The tests carried out on the prototype show that the proposed architecture correctly provides users with various essential services in terms of protection, detection, and reply. From our point of view, the most important advantages of our proposed software architecture are the following: define the basic technical guidelines that an MDSV must have and accomplish; streamline overall development, providing a solid framework for developers; and contribute to satisfying the requirements that arise from quality attributes that the MDSV must possess.},
keywords={Computer architecture;Video surveillance;Software architecture;Mobile handsets;Temperature sensors;Mobile distributed systems;third generation video surveillance;layered software architecture},
doi={10.1109/ACCESS.2019.2930401},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8765999,
author={Hollós, Ádám and Kovácsházy, Tamás},
booktitle={2019 20th International Carpathian Control Conference (ICCC)}, title={Measurement System for the Performance Assessment of GNSS Receivers},
year={2019},
volume={},
number={},
pages={1-5},
abstract={With the continuing spread of distributed measurement and control systems (e.g. Cyber-Physical Systems, Industry 4.0) there is an increased need to implement synchronized sampling and actuation over the components of a distributed system. To achieve this goal, the clocks of the system must be synchronized, including the ones doing sampling and actuation. In most cases local synchronization alone is not enough; It is also needed to keep in sync with a global reference time in order, for example, to be able to implement timestamps based on global time and simultaneous sampling across system components that are spatially distributed (operate on different sites). This global reference time mostly means one which is provided by a GNSS (Global Navigation Satellite System) service, such as GPS. In these systems, it is the master clock which is responsible for attaining (e.g. via a GNSS receiver) and keeping the global reference time. It can then pass it on in the local communication network, usually using IEEE 1588 (Precision Time Protocol). It follows, that thus, the characteristics of such systems are heavily limited by the precision, accuracy and reliability of the used GNSS receiver. Our paper thus is about the examination of potentially promising GNSS receivers for which we developed a custom measurement system. This includes the design of a test PCB, the writing of a PC data collecting program, the formulation of MATLAB scripts which process the collected data, and the validation of the data collecting system via GNSS receivers simulated by TI evaluation boards. We also conducted measurements with the system on the Quectel L86 GNSS receiver, the results of which are presented in the paper.},
keywords={Receivers;Global navigation satellite system;Synchronization;Satellites;Clocks;Reliability;Cyber-Physical System;Industry 4.0;Global Navigation Satellite System;Clock Synchronization;Measurement System;IEEE 1588},
doi={10.1109/CarpathianCC.2019.8765999},
ISSN={},
month={May},}
@INPROCEEDINGS{8738487,
author={Thottappillil, Rajeev and Bäckström, Mats},
booktitle={2019 URSI Asia-Pacific Radio Science Conference (AP-RASC)}, title={Intentional Electromagnetic Interference and Critical Infrastructure},
year={2019},
volume={},
number={},
pages={1-1},
abstract={Intentional Electromagnetic Interference or IEMI is the result of maliciously created electromagnetic disturbances in sensitive electronic systems. Modern infrastructure such as power supply, wireless communication networks, banking system, and transportation networks are dependent on civilian-off-the-shelf (COTS) equipment for its uninterrupted and reliable operations. Even though COTS equipment are tested for electromagnetic compatibility as per pre-defined un-intentional electromagnetic environment and test methods, intentionally created electromagnetic disturbances creates a lot of uncertainty in the proper functioning of critical COTS elements of the infrastructure. Infrastructure is interconnected widely distributed systems and there are several ports where substantial electromagnetic energy can be coupled intentionally by saboteurs and once coupled these disturbances travel through the interconnected system and cause breakdown at one or more weak links. It is probable that some of these breakdowns may lead to widespread disruption of critical infrastructure, such as power supply blackouts, financial service shutdown or disruption in railway network due to signal failures. Due to its inherent nature, EM attacks to civilian infrastructure may happen anonymously and repeatedly without detection. Protection of civilian critical infrastructure against the effects of IEMI has received a lot of attention recently from the EMC community. This article reviews the work done around the world with special emphasis on the work done in Sweden [e.g; 1-5].},
keywords={Electromagnetic interference;Critical infrastructure;Electromagnetics;Power supplies;Electromagnetic compatibility;Electric breakdown;Rail transportation},
doi={10.23919/URSIAP-RASC.2019.8738487},
ISSN={},
month={March},}
@INPROCEEDINGS{8732884,
author={Banihashemi, Sepideh and Li, Jason and Abhari, Abdolreza},
booktitle={2019 Spring Simulation Conference (SpringSim)}, title={Scalable Machine Learning Algorithms for a Twitter Followee Recommender System},
year={2019},
volume={},
number={},
pages={1-8},
abstract={Recently, machine learning (ML) algorithms have been employed in social networking recommender systems. In this paper, a Twitter recommender system is simulated by a multi-agent system that can be used to provide the users with a list of useful recommendations, specifically a list of users (i.e., followees) that a user is interested in following. The simulator is used to test the scalability of a machine learning algorithm (i.e., Neural Network, Multilayer Perceptron) for data analysis with parallel implementation on multi-node distributed systems. The distributed environment is simulated by a multi-agent modeling. The initial parameters that should be set up on the simulator include the number of nodes, the algorithm employed in the simulated recommender system, and the actual followees and followers information. The experimental results were obtained on three distinct datasets for evaluating the accuracy and the execution time of a simulated recommender system when testing the ML algorithm in different scenarios.},
keywords={Machine learning algorithms;Recommender systems;Twitter;Neurons;Multilayer perceptrons;Training;Timing;Simulating Recommender Systems;Scalable Machine Learning;Multilayer Perceptron.},
doi={10.23919/SpringSim.2019.8732884},
ISSN={},
month={April},}
@INPROCEEDINGS{8729050,
author={Yongliang, Wu and Peng, Luo and Xiaoping, Chen and Lichuan, Xiong},
booktitle={2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)}, title={Research on Clock Synchronization Mechanism of Distributed Simulation System based on DDS},
year={2019},
volume={},
number={},
pages={2224-2229},
abstract={Flight control system simulation test is a key process in aircraft design stage. Distributed system architecture is often used in flight control simulation test. Clock synchronization has always been a research hotspot of distributed simulation system. Data distribution service (DDS) is widely used in distributed simulation system for its good real-time performance, high efficiency and low coupling. However DDS does not provide a excellent clock synchronization mechanism. Aiming at the demand of clock synchronization of distributed simulation test system, an improved PTP clock synchronization algorithm is proposed in this paper, and a virtual clock of distributed simulation system is constructed on the basis of the propulsion timing model. Experimental data show that the model can be applied to simulation nodes stably, and the improved algorithm can also achieve clock synchronization between nodes of distributed system.},
keywords={Clocks;Synchronization;Atmospheric modeling;Aerospace control;Calibration;Protocols;Data distribution services;Clock synchronization;Virtual clock},
doi={10.1109/ITNEC.2019.8729050},
ISSN={},
month={March},}
@INPROCEEDINGS{8730179,
author={Lima, Bruno},
booktitle={2019 12th IEEE Conference on Software Testing, Validation and Verification (ICST)}, title={Automated Scenario-Based Integration Testing of Time-Constrained Distributed Systems},
year={2019},
volume={},
number={},
pages={486-488},
abstract={In a growing number of domains, such as IoT for e-health and smart cities, the provisioning of end-to-end services to the users depends on the proper interoperation of multiple systems, forming a new distributed system, often subject to timing constraints. To ensure interoperability and integrity, it is important to conduct integration tests that verify the interactions with the environment and between the system components in key scenarios. To solve the test automation challenges, we propose algorithms for decentralized conformance checking and test input generation, and for checking and enforcing the conditions (local observability and controllability) that allow decentralized test execution. With this, we expect to improve the fault detection and localization capabilities and reduce the communication overhead comparatively to other model-based testing approaches. Our approach will be validated using real case studies from industrial partners.},
keywords={Testing;Unified modeling language;Observability;Controllability;Tools;Conferences;Computer architecture;Model-Based Testing, Observability, Control lability, Integration Testing, Distributed Systems, UML, Time-constraints},
doi={10.1109/ICST.2019.00060},
ISSN={2159-4848},
month={April},}
@INPROCEEDINGS{8718213,
author={Itkin, Iosif and Gromova, Anna and Sitnikov, Anton and Legchikov, Dmitry and Tsymbalov, Evgenii and Yavorskiy, Rostislav and Novikov, Andrey and Rudakov, Kirill},
booktitle={2019 IEEE International Conference On Artificial Intelligence Testing (AITest)}, title={User-Assisted Log Analysis for Quality Control of Distributed Fintech Applications},
year={2019},
volume={},
number={},
pages={45-51},
abstract={Testing of distributed systems is a complex task, which is hampered by the impossibility of guaranteed reproduction of errors associated with race conditions. Even minor instrumentation of the system significantly changes its characteristics, which becomes critical, especially for load testing. All of that increases the importance of quality control methods based on the system log analysis. In this paper, we present our experience of semi-automated analysis of the behavior of clearing and settlement system by utilizing its logs for the purpose of identifying and classifying errors.},
keywords={Data mining;Testing;Quality control;Computer science;Economics;Instruments;Software;Log Analysis;Clustering;Distributed Applications;Fintech},
doi={10.1109/AITest.2019.000-9},
ISSN={},
month={April},}
@ARTICLE{8710371,
author={Binquan, Li and Xiaohui, Hu},
journal={Journal of Systems Engineering and Electronics}, title={Effective distributed convolutional neural network architecture for remote sensing images target classification with a pre-training approach},
year={2019},
volume={30},
number={2},
pages={238-244},
abstract={How to recognize targets with similar appearances from remote sensing images (RSIs) effectively and efficiently has become a big challenge. Recently, convolutional neural network (CNN) is preferred in the target classification due to the powerful feature representation ability and better performance. However, the training and testing of CNN mainly rely on single machine. Single machine has its natural limitation and bottleneck in processing RSIs due to limited hardware resources and huge time consuming. Besides, overfitting is a challenge for the CNN model due to the unbalance between RSIs data and the model structure. When a model is complex or the training data is relatively small, overfitting occurs and leads to a poor predictive performance. To address these problems, a distributed CNN architecture for RSIs target classification is proposed, which dramatically increases the training speed of CNN and system scalability. It improves the storage ability and processing efficiency of RSIs. Furthermore, Bayesian regularization approach is utilized in order to initialize the weights of the CNN extractor, which increases the robustness and flexibility of the CNN model. It helps prevent the overfitting and avoid the local optima caused by limited RSI training images or the inappropriate CNN structure. In addition, considering the efficiency of the Näıve Bayes classifier, a distributed Näıve Bayes classifier is designed to reduce the training cost. Compared with other algorithms, the proposed system and method perform the best and increase the recognition accuracy. The results show that the distributed system framework and the proposed algorithms are suitable for RSIs target classification tasks.},
keywords={Training;Feature extraction;Computer architecture;Bayes methods;Task analysis;Convolutional neural networks;Remote sensing;convolutional neural network (CNN);distributed architecture;remote sensing images (RSIs);target classification;pre-training.},
doi={10.21629/JSEE.2019.02.02},
ISSN={1004-4132},
month={April},}
@INPROCEEDINGS{8671550,
author={Geier, Maximiliano and Tessone, Claudio J. and Vanotti, Marco and Vileriño, Silvio and Márquez, David González and Mocskos, Esteban},
booktitle={2019 27th Euromicro International Conference on Parallel, Distributed and Network-Based Processing (PDP)}, title={Using Network Emulation to Study Blockchain Distributed Systems: The Ethereum Case},
year={2019},
volume={},
number={},
pages={51-58},
abstract={Large-scale distributed systems are becoming more widespread and, at the same time, their sizes grow day by day. In this type of systems, the adoption of blockchains is gaining particular traction for data storage in a secure and distributed manner. Nevertheless, design and testing of new protocols and features face the challenge of determining whether the proposed modifications would actually improve the system as expected. In the case of existing cryptocurrency systems, building an evaluation platform poses additional difficulties due to the resource-consuming nature of the associated processes. In this paper, we propose a novel methodology that relies on container-based network emulation to create scalable local testbeds in which Proof-of-Work-based blockchain systems can be evaluated. Using one of the mainstream Ethereum clients, we replaced the mining algorithm with a simulation model built upon the statistical characteristics of the mining process and instrumented the client to capture relevant network events. These events are used to create an offline reconstruction of the global view of the blockchain and all forking events, and to completely characterize the working conditions under arbitrary setups. Based on the versatility and scalability of our platform, we are able to test several network scenarios of increasing size in which we analyze the incidence of the target time in the generation of contradictory views of the blockchain (i.e. forks). We show that even using a limited testbed constituted by just commodity hardware, it is possible to use our platform to study the dynamics of blockchain-based systems up to hundreds of nodes.},
keywords={Global Positioning System;Optical fibers;Hafnium compounds},
doi={10.1109/EMPDP.2019.8671550},
ISSN={2377-5750},
month={Feb},}
@ARTICLE{8664090,
author={Mohamed, Mohamed A. and Al-Sumaiti, Ameena Saad and Krid, Mohamed and Awwad, Emad Mahrous and Kavousi-Fard, Abdollah},
journal={IEEE Access}, title={A Reliability-Oriented Fuzzy Stochastic Framework in Automated Distribution Grids to Allocate $\mu$ -PMUs},
year={2019},
volume={7},
number={},
pages={33393-33404},
abstract={This paper proposes a reliability-oriented stochastic aggregated integer linear framework for full observability of the automated distributed systems based on the μ-synchrophasor units. The μ-synchrophasor unit as a newly introduced high-tech device makes it possible for an accurate and highspeed measurement of the voltage and current waveforms in the distribution systems. This paper proposes a multi-stage strategy for the μ-synchrophasor unit placement together with the communication system requirements in the reconfigurable distribution systems, considering the zero-injection constraints in the model. To determine the optimal topology at the end of each phase, a reliability-based cost function is developed to optimize the customer interruption costs and power losses simultaneously. In order to model the uncertainties of forecast error in the active and reactive load demands as well as the failure rate and repair rate parameters, a stochastic framework based on the fuzzy cloud theory is employed. The proposed bi-level mixed integer linear programing approach is used to co-optimize the network switching scheme as well as the optimal μ-synchrophasor positions and communication infrastructure costs in the same framework. The simulation results on a practical test system verify the observability of the automated reconfigurable distribution system during the reconfiguration process.},
keywords={Phasor measurement units;Voltage measurement;Stochastic processes;Load modeling;Observability;Reliability;Topology;μ-synchrophasor measurement unit;uncertainty;reconfigurable automated grids;visibility;optimization},
doi={10.1109/ACCESS.2019.2902465},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8644495,
author={Hossain, Nafize Ishtiaque and Reza, Sakib and Ali, Muhammad},
booktitle={2019 International Conference on Robotics,Electrical and Signal Processing Techniques (ICREST)}, title={VibNet: Application of Wireless Sensor Network for Vibration Monitoring Using ARM},
year={2019},
volume={},
number={},
pages={248-252},
abstract={Vibration in electrical and mechanical systems is an important parameter which is related to the systems health, stability and efficiency. In practical condition, the system is distributed and most of the time unreachable during their operating time. In this paper, triple axis wireless vibration monitoring system for distributed system is described. The system also includes development of real time graphical user interface with MATLAB. Here triple axis accelerometer ADXL335 is used for acceleration measurement , cost effective ARM M4 processor TM4C123GXL is used as processing unit and low power Zigbee transceiver is used to transfer the vibration data. To prevent data tapping, the data is encrypted by Advanced Encryption Standard (AES) algorithm in sending side and then decrypted in receiver end. The overall latency of the system is also measured. The received signal strength based performance analysis is also done where the RSSI value is approximately -43dBm at 587.94m distance. The system is tested for vibration measurement of a single phase induction motor.},
keywords={Vibrations;Zigbee;Acceleration;Monitoring;Accelerometers;Vibration measurement;Wireless sensor networks;vibration;zigbee;ARM M4;MATLAB;AES algorithm;RSSI},
doi={10.1109/ICREST.2019.8644495},
ISSN={},
month={Jan},}
@ARTICLE{8558099,
author={Razik, Lukas and Berr, Nicolas and Khayyam, Sara and Ponci, Ferdinanda and Monti, Antonello},
journal={IEEE Transactions on Vehicular Technology}, title={REM-S–Railway Energy Management in Real Rail Operation},
year={2019},
volume={68},
number={2},
pages={1266-1277},
abstract={This paper presents the prototype implementation of an advanced automation architecture for electrical railway systems, designed to operate them as cyber-physical systems, such as smart grids. This REM-S architecture, from the literature, is a distributed system coordinating rolling stock, electrical substations, sideway resources within the constraints, and objectives (overall energy demand, power consumption, and cost optimization) dictated by the control center. The prototype software suites presented here are the REM-S Offline Suite and the REM-S Online Suite. Among others, they consist of an application for day-ahead optimization, which is performed by the control center for the next day, and another application for minute-ahead optimization, which is continuously performed during rail operation on so-called intelligent substations at time intervals of, e.g. 15 min. The REM-S Offline Suite can be used for dry runs of test scenarios before running the aforementioned applications in combination with others on the participating trains, substations, and so forth, as part of the REM-S Online Suite within a railway system in real time. The REM-S Online Suite, which is an implementation of a distributed optimization, was validated in a field test, performed in a suburban 3-kV dc railway line in Málaga, Spain during real rail operation where the objectives within the constraints given by the control center have been successfully reached. Besides, the software architecture of both software suites also selected algorithms that are presented for a better understanding of the overall communication and distributed real-time optimization processes.},
keywords={Optimization;Energy management;Rail transportation;Substations;Computer architecture;Software;Real-time systems;Energy management;railway system;distributed optimization;real-time;field test},
doi={10.1109/TVT.2018.2885007},
ISSN={1939-9359},
month={Feb},}
@ARTICLE{8482295,
author={Zaitsev, Dmitry and Tomov, Stanimire and Dongarra, Jack},
journal={IEEE Transactions on Parallel and Distributed Systems}, title={Solving Linear Diophantine Systems on Parallel Architectures},
year={2019},
volume={30},
number={5},
pages={1158-1169},
abstract={Solving linear Diophantine systems of equations is applied in discrete-event systems, model checking, formal languages and automata, logic programming, cryptography, networking, signal processing, and chemistry. For modeling discrete systems with Petri nets, a solution in non-negative integer numbers is required, which represents an intractable problem. For this reason, solving such kinds of tasks with significant speedup is highly appreciated. In this paper we design a new solver of linear Diophantine systems based on the parallel-sequential composition of the system clans. The solver is studied and implemented to run on parallel architectures using a two-level parallelization concept based on MPI and OpenMP. A decomposable system is usually represented by a sparse matrix; a minimal clan size of the decomposition restricts the granulation of the technique. MPI is applied for solving systems for clans using a parallel-sequential composition on distributed-memory computing nodes, while OpenMP is applied in solving a single indecomposable system on a single node using multiple cores. A dynamic task-dispatching subsystem is developed for distributing systems on nodes in the process of compositional solution. Computational speedups are obtained on a series of test examples, e.g., illustrating that the best value constitutes up to 45 times speedup obtained on 5 nodes with 20 cores each.},
keywords={Petri nets;Matrix decomposition;Mathematical model;Task analysis;Sparse matrices;Software algorithms;Parallel architectures;MPI;OpenMP;linear diophantine system;Petri net;clan;speed-up},
doi={10.1109/TPDS.2018.2873354},
ISSN={1558-2183},
month={May},}
@ARTICLE{8440732,
author={Bako, Zeïnabou Nouhou and Tankari, Mahamadou Abdou and Lefebvre, Gilles and Maiga, Amadou Seidou},
journal={IEEE Transactions on Industry Applications}, title={Experiment-Based Methodology of Kinetic Battery Modeling for Energy Storage},
year={2019},
volume={55},
number={1},
pages={593-599},
abstract={This paper presents a methodology of the battery modeling based on experimental tests results. As a main contribution, authors conducted an analysis of the main constraints and effects of each parameter of the batteries model on its behavior and lifetime. This can help to improve the accuracy of the model. Due to its capability to model the recovery effect and the rate-capacity one, the kinetic battery model serves as the basis of the study. Results of experimental tests are used to realize the analysis and to define the suitable methodology of batteries parameters identification. These last can serve for prediction and estimation of the battery lifetime according to the actual operating conditions, particularly in microgrid and distributed systems.},
keywords={Batteries;Discharges (electric);Kinetic theory;US Department of Defense;Analytical models;Switches;Characterization;energy storage;experimental tests;kinetic battery model (KiBaM);lead-acid battery;modeling},
doi={10.1109/TIA.2018.2866148},
ISSN={1939-9367},
month={Jan},}
@ARTICLE{8399533,
author={He, Kun and Meng, Xiaozhu and Pan, Zhizhou and Yuan, Ling and Zhou, Pan},
journal={IEEE Transactions on Parallel and Distributed Systems}, title={A Novel Task-Duplication Based Clustering Algorithm for Heterogeneous Computing Environments},
year={2019},
volume={30},
number={1},
pages={2-14},
abstract={As a crucial task in heterogeneous distributed systems, DAG-scheduling models a scheduling application with a set of distributed tasks by a Direct Acyclic Graph (DAG). The goal is to assign tasks to different processors so that the whole application can finish as soon as possible. Task Duplication-Based (TDB) scheme is an important technique addressing this problem. The main idea is to duplicate tasks on multiple machines so that the results of the duplicated tasks are available on multiple machines to trade computation time for communication time. Existing TDB algorithms enumerate and test all possible duplication candidates, and only keep the candidates that can improve the overall scheduling. We observe that while a duplication candidate is ineffective at the moment, after other duplications have been applied, this ineffective duplication candidate can become effective, which in turn can cause other ineffective duplications to become effective. We call this phenomenon the chain reaction of task duplication. We propose a novel Task Duplication based Clustering Algorithm (TDCA) to improve the schedule performance by utilizing duplication task more thoroughly. TDCA improves parameter calculation, task duplication, and task merging. The analysis and experiments are based on randomly generated graphs with various characteristics, including DAG depth and width, communication-computing cost ration, and variant computation power of processors. Our results demonstrate that the TDCA algorithm is very competitive. It improves the schedule makespan of task duplication-based algorithms for heterogeneous systems for various communication-computing cost ratios.},
keywords={Task analysis;Program processors;Scheduling;Clustering algorithms;Scheduling algorithms;Optimal scheduling;Task duplication;clustering and merging;optimal scheduling;heterogeneous environment},
doi={10.1109/TPDS.2018.2851221},
ISSN={1558-2183},
month={Jan},}
@ARTICLE{8334263,
author={Akbarimajd, Adel and Olyaee, Mohsen and Sobhani, Behrooz and Shayeghi, Hossein},
journal={IEEE Transactions on Sustainable Energy}, title={Nonlinear Multi-Agent Optimal Load Frequency Control Based on Feedback Linearization of Wind Turbines},
year={2019},
volume={10},
number={1},
pages={66-74},
abstract={Large scale power systems have distributed structure since power generation units are distributed in the network. In such systems, where the communications are sometimes imperfect, centralized control structures have concerns to be applied. For such inherently distributed systems, multi-agent control structures are more appropriate. On the other hand, power systems have nonlinear dynamics. When all generation units are synchronous generators, linear approximation of swing equation of generators is a common choice. However, by penetration of renewable energy generation units, those have highly nonlinear dynamics, nonlinear control approaches should be applied. In this paper, a nonlinear multi-agent feedback linearization approach is adopted for optimal load frequency control of a power system with wind generation units. Multiagent controllers with both distributed and decentralized structures are proposed. Centralized feedback controller is also applied for comparison purposes. The parameters of the controllers are optimized using gray wolf optimization algorithm to improve time response of the system and reduce generation costs. IEEE 30-bus system with six synchronous generators and four wind turbines is considered as a test system. Simulation results verify the inefficiency of centralized controller and show that distributed control structure can produce a balanced performance in terms of time response and is better from the cost reduction view point than the decentralized controller.},
keywords={Frequency control;Power system stability;Wind turbines;Feedback linearization;Wind power generation;Decentralized control;Distributed control;decentralized control;multi-agent systems;optimal load frequency control;feedback linearization;economic load dispatch},
doi={10.1109/TSTE.2018.2823062},
ISSN={1949-3037},
month={Jan},}
@ARTICLE{8301552,
author={Moret, Fabio and Pinson, Pierre},
journal={IEEE Transactions on Power Systems}, title={Energy Collectives: A Community and Fairness Based Approach to Future Electricity Markets},
year={2019},
volume={34},
number={5},
pages={3994-4004},
abstract={While power system organization has evolved from a hierarchical structure to a more decentralized model, electricity markets are still not up to date with the ongoing transformation toward more consumer-centric economies. As information and communication technologies are broadly adopted, they allow prosumers to have a more proactive role in power system operation. This work introduces the concept of energy collectives, as a community-based electricity market structure. We find that when prosumers are allowed to share energy at community level, the overall electricity procurement for the community reflects prosumers' preferences. We show that community members can be influenced by a supervisory third-party in charge of interfacing with the market and system operator and of guaranteeing the collective common agreements. We simulate a number of test cases and apply typical principles from analysis of communication networks and distributed systems to assess community fairness.},
keywords={Organizations;Peer-to-peer computing;Power systems;Information and communication technology;Electricity supply industry;Cost function;Electricity markets;prosumers community;renewable energy integration;distributed optimization;fairness},
doi={10.1109/TPWRS.2018.2808961},
ISSN={1558-0679},
month={Sep.},}
@INPROCEEDINGS{9045047,
author={Zheng, Kexin and Yang, Jingli and Zheng, Shuai},
booktitle={2018 Eighth International Conference on Instrumentation Measurement, Computer, Communication and Control (IMCCC)}, title={Prediction Model for Information Transmission Performance of Publish/Subscribe Systems with Heterogeneous Servers},
year={2018},
volume={},
number={},
pages={426-430},
abstract={A novel prediction model for information trans-mission performance is presented in this paper, which is suitable for publish/subscribe systems with heterogeneous servers. Firstly, a uniform business logic model is developed by analyzing the principle of several typical types of publish/subscribe distributed systems. In addition, a mathematical model named MMPP(2)/G/1 cascaded queuing model is proposed by introducing heterogeneous server weighting factors into classical queuing model and Markov modulated Poisson process, which can eliminate iteration errors of the queuing model. The proposed prediction model is fully verified by carrying out a series of experiments in an Internet test platform. The results of the experiments illustrate that the proposed prediction model can achieve a higher prediction accuracy than the traditional model.},
keywords={Publish/Subscribe;Performance Prediction;Distributed System Information Transmission;Queuing Model},
doi={10.1109/IMCCC.2018.00096},
ISSN={2373-6844},
month={July},}
@INPROCEEDINGS{8724228,
author={Bairagi, Kishalay and Dey, Ratnadeep and Das, Pradip K.},
booktitle={2018 International Conference on Information Technology (ICIT)}, title={Design of A Pedagogical Framework for Configuration, Execution and Analysis of Distributed Programs},
year={2018},
volume={},
number={},
pages={85-90},
abstract={In distributed system it may pose some problems to visualize the exact nature of functioning and the sequence of events in various nodes of the said system and such insights are essential to comprehend and analyze the behavior of different algorithms executing in different nodes of the system mentioned. This paper describes a framework that has been implemented to help visualize and analyze the working behavior of various algorithms on distributed system. Using this platform it is possible for students and researchers to gain insight into the working of various nodes of a distributed system, check their interactions at run time and thus to test different algorithms in a simulated as well as actual environment. The framework lets the student to check the behavior of certain standard built-in distributed algorithms, viz., Leader Election using Bully and Ring Algorithms, Ricart-Agrawala's Mutual Exclusion Algorithm and Chandy-Mishra-Hash deadlock detection algorithm. The users can also write their own algorithms and use a program generator module to generate programs for different nodes using this framework. Subsequent to the execution, a graphical analyzer module aids the user by showing the execution behavior of the algorithm using a space-time graphical diagram that makes use of Lamport's Time Stamping algorithm. The framework provides built-in system-level supports to facilitate exclusive access to shared resources and detects distributed deadlock also.},
keywords={Skeleton;Distributed algorithms;System recovery;Automatic programming;Voting;System kernels;Protocols;Distributed System, Pedagogy, Input File, Run Time Input File, Program Executor, Control Node, Result Log File, Graphical Result Analyzer},
doi={10.1109/ICIT.2018.00028},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8704447,
author={Saxena, Nitin Kumar and Kumar, Ashwani and Gebreyohans, Gebrehiwot and Mena, Degu and Dawit, Wondimu},
booktitle={2018 IEEE 8th Power India International Conference (PIICON)}, title={Role of STATCOM in Mitigating Probabilistic Load Disturbances in Isolated Hybrid Electrical System},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Role of load composition is an important factor for choosing appropriate compensating device from the family of FACTS devices. Devices providing reactive power compensation are important tool for adjusting voltage level in any distributed system. Especially for this paper study, FACTS devices may provide sincere attempts during the abrupt changes in either load or input system conditions such as change in wind speed for wind power generation. From shunt connected FACTS family, STATCOM is a popular voltage source converter (VSC) type compensator that has several merits over reactance based SVC especially for the purpose of voltage control and therefore STATCOM is preferred in many studies in spite of being high compensation tariff compare to that of SVC. In available voltage control studies of Isolated Hybrid System (IHS), STATCOM is applied with deterministic type static load. The accessibility of STATCOM with composite load for mitigating the disturbance, both deterministic and probabilistic, is elaborated in this paper. The effect of static and/ or composite load and deterministic and/or probabilistic load patterns on voltage and reactive power responses in 2.5 MW rating test system of wind diesel based isolated hybrid electrical system is analyzed.},
keywords={Reactive power;Load modeling;Mathematical model;Automatic voltage control;Probabilistic logic;Induction motors;static load;composite load;deterministic load patterns;probabilistic load patterns;STATCOM;isolated hybrid electrical system},
doi={10.1109/POWERI.2018.8704447},
ISSN={2642-5289},
month={Dec},}
@INPROCEEDINGS{8674942,
author={Alam, Afroz and Zaid, Mohammad and Gupta, Abhishek and Bindal, Parth and Siddiqui, Aiman},
booktitle={2018 International Conference on Computing, Power and Communication Technologies (GUCON)}, title={Power Loss Reduction in a Radial Distribution Network Using Distributed Generation},
year={2018},
volume={},
number={},
pages={1142-1145},
abstract={Reduction in real power losses and improvement in voltage profile are two major objectives of Distributed system utilities. For this purpose, placement of step-size distributed generations (DG) in radial distribution system is used. In this paper, IEEE 33-bus and 69-bus Radial Distribution System (RDS) are tested for same purpose, using load flow solution in MATLAB environment which is based on BIBC matrix formulation. After performing the load flow, a DG is placed on the bus having worst voltage profile, repeatedly. After the analysis of test results it is concluded that reasonably lower losses and improved voltage profile of a distribution system can be obtained by placing DGs of suitable sizes and at proper locations in the system.},
keywords={Minimization;Distributed power generation;Conferences;Electrical engineering;Capacitors;Resource management;Insertion loss;Distribution system;Distributed generation;Voltage profile},
doi={10.1109/GUCON.2018.8674942},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8647493,
author={Noor, Jannatun and Hossain, Md. Golam and Alam, Muhammad Ahad and Uddin, Ashraf and Chellappan, Sriram and Al Islam, A. B. M. Alim},
booktitle={2018 IEEE Global Communications Conference (GLOBECOM)}, title={svLoad: An Automated Test-Driven Architecture for Load Testing in Cloud Systems},
year={2018},
volume={},
number={},
pages={1-7},
abstract={Nowadays, Internet-based technologies possess immense processing power, capacity, flexibility, and are gradually moving towards a service- oriented functionality in order to build new distributed storage systems in the cloud. Several distributed systems are currently running in different geographically located data centers for successful deployment of modern web and social services such as Facebook, Twitter, ringID, etc. Both cache and backend servers in such distributed systems must be functional and reliable for incoming workloads by means of efficient allocation of capacity along with proper configuration and tuning of multiple system resources. To address these challenges, in this paper, we propose a test-driven automated architecture for load testing, named as 'svLoad' to compare the performance of cache and backend servers. Here, we designed test cases considering diversified real scenarios such as different protocol types, same or different URLs, with or without load, cache hit or miss, etc. using tools namely JMeter, Ansible, and some custom utility bash scripts. To validate the efficacy of our proposed methodology, we conduct a set of experiments by running these test cases over a real private cloud development setup using two open source projects - Varnish as the cache server and OpenStack Swift as the backend server. Our focus is also to find out bottlenecks of Varnish and Swift by executing load requests, and then tune the system based on our load test analysis. After successfully tuning the Swift, Varnish, and network system, based on our test analysis, we were able to improve the response time by up to 80%.},
keywords={Servers;Testing;Cloud computing;Measurement;Computer architecture;Uniform resource locators;Tuning},
doi={10.1109/GLOCOM.2018.8647493},
ISSN={2576-6813},
month={Dec},}
@INPROCEEDINGS{8628507,
author={Setiawan, Ivan Pandu and Sukaridhoto, Sritrusta and Pramadihanto, Dadet},
booktitle={2018 International Electronics Symposium on Knowledge Creation and Intelligent Computing (IES-KCIC)}, title={Message Passing Support for FLoW Microkernel},
year={2018},
volume={},
number={},
pages={17-22},
abstract={The demand for faster computation speed in modern digital signal processing is huge. However, the computation speed that single processor can provide is limited. To address this demand, both distributed system and parallel processing are becoming a requirement in an embedded system. Therefore, research about algorithm and application of parallel processing is very important to be conducted. Implementing MPI's standard to an embedded system will increase the application portability, therefore parallel programming will be easier to be implemented. This paper presents a novel design and implementation of MPI on top of our microkernel named FLoW which are built and run on an embedded system. To decrease communication latency, we propose a communication layer design based on MPI. On this layer, a process manager is made to handle multi-processes and routing services mechanism. In addition, a mailbox system is created to temporarily keep the message which is sent when the collective operation occurs. From our experiments, the time required to complete the data transmission process ranges from 400 to 500 microseconds for each process, and in parallel task testing using MPI, the speedup can achieve up to 40-50%.},
keywords={Message systems;Portals;Message passing;Embedded systems;Standards;Parallel processing;Aerospace electronics;MPI;parallel processing;process manager;distributed system},
doi={10.1109/KCIC.2018.8628507},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8622866,
author={Ding, Hong and Zhang, Chuang and Chen, Xiaojun and Shi, Jinqiao and Wang, Wenan},
booktitle={2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)}, title={Cloud-MOM: A Content-Based Real-Time Message-Oriented Middleware for Cloud},
year={2018},
volume={},
number={},
pages={750-757},
abstract={In order to improve the efficiency of real-time data transmission between different Internet Data Centers (IDCs) by content, this paper proposes a content-based real-time message-oriented middleware for cloud environment (Cloud-MOM). Nodes of Cloud-MOM can be deployed in IDCs in different regions. The messages from each IDC can be routed and transmitted through nodes efficiently. In another way, for real-time processing of real-time data streams, content-based triggers for data streams are supported in Cloud-MOM. In this paper, functional tests and performance tests on Cloud-MOM are performed. The experimental results show that Cloud-MOM can efficiently and stably carry out data transmission across IDCs, and the content-based triggers effectively improve the quality of network transmission and reduce the network load as well. This paper provides a novel solution for efficient and stable data transmission for the distributed systems.},
keywords={Routing;Cloud computing;Method of moments;Real-time systems;Data models;Message-oriented middleware;Data communication;cloud, content-based, cross-IDCs, triggers, message-oriented middleware},
doi={10.1109/HPCC/SmartCity/DSS.2018.00128},
ISSN={},
month={June},}
@INPROCEEDINGS{8611812,
author={Skondric, Goran and Mudnic, Eugen},
booktitle={2018 26th Telecommunications Forum (TELFOR)}, title={Node Grouping Based on Pattern of Behavior in P2P Distributed System},
year={2018},
volume={},
number={},
pages={1-4},
abstract={For Peer to Peer (P2P) distributed systems the most critical feature is availability concerning behavior nature of nodes that comprise this type of distributed systems. In this paper we recognized several node patterns of presence that can be used in P2P storage distributed systems. We analyzed computers from real life environment trying to recognize certain groups of node pattern behavior, and tried to determine if we can use this grouping as basis for using resources in P2P storage systems. Using gathered information we tested nodes availability using different grouping approaches like: random approach, approach based on group of nodes that are recognized using tool for social group networks analysis, approach based on hierarchical cluster method and the use of coincidence matrix. Research filtered two approaches that showed best results and their main characteristics.},
keywords={Workstations;Peer-to-peer computing;Monitoring;Tools;Visualization;Pattern recognition;Switches;P2P;storage distributed systems;availability},
doi={10.1109/TELFOR.2018.8611812},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8605765,
author={Ismayilov, Goshgar and Topcuoglu, Haluk Rahmi},
booktitle={2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)}, title={Dynamic Multi-Objective Workflow Scheduling for Cloud Computing Based on Evolutionary Algorithms},
year={2018},
volume={},
number={},
pages={103-108},
abstract={Cloud computing is a dominant heterogeneous and distributed system, that offers on-demand resource capacity for different requirements of customers. Cloud workflow scheduling is a largely studied research area that targets efficient utilization of cloud resources. In this paper, we model a dynamic workflow scheduling problem, which is among the first attempts that incorporate dynamism on resource failures and changing number of objectives. A set of dynamic multi-objective evolutionary algorithms from the literature are utilized for dynamic workflow scheduling problem, where four of them are variants of the commonly used NSGA-II algorithm (DNSGA-II-HM, DNSGAII-A, DNSGA-II-B, DNSGA-II-RI) and the remaining one is dynamic extension of the multi-objective particle swarm optimization algorithm (DMOPSO). In our experimental study, five different objectives are considered, which are minimization of the makespan, the cost and the energy, and maximization of the reliability and the resource utilization. The empirical study of the given five algorithms is conducted with real-world applications from Pegasus workflow management systems, where the DNSGAII-B procedure outperforms the other alternatives for most of the test instances, based on the number of non-dominated solutions, the Schott's spacing and the hyper-volume metrics.},
keywords={Task analysis;Heuristic algorithms;Dynamic scheduling;Cloud computing;Reliability;Processor scheduling;workflow scheduling;resource failures;dynamic multi-objective evolutionary algoirthms},
doi={10.1109/UCC-Companion.2018.00042},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8605752,
author={Gortázar, Francisco and Gallego, Micael},
booktitle={2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)}, title={A Simple Path Towards Testing Cloud Applications},
year={2018},
volume={},
number={},
pages={28-29},
abstract={Integration and end-to-end (e2e) testing of distributed systems is a much more complex task than that of a monolithic application. Distributed systems require several services and tools to be managed, even for the simplest integration tests. The tutorial will show how the efforts required for: 1) testing such systems, and 2) doing root cause analysis in the presence of failures, can be diminished by using ElasTest. We will start explaining how to perform e2e testing over a docker-based application using Jenkins, a popular CI server. Then, we will resort to ElasTest, and we will show how these tasks can be easily performed on this new testing platform specifically tailored for distributed systems testing. Two different approaches with ElasTest will be showcased: a first one tightly integrated with Jenkins that requires minimal efforts and changes to the existing Jenkins configuration. A second one where the full application lifecycle is left to the ElasTest platform, and the team can focus exclusively on testing.},
keywords={Cloud computing;cloud applications;testing;end-to-end testing;testing observability},
doi={10.1109/UCC-Companion.2018.00029},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8605786,
author={Bertolino, Antonia and Calabrò, Antonello and Marchetti, Eda and Cervantes Sala, Anton and Tuñón de Hita, Guiomar and Gheorghe Pop, Ilie Daniel and Gowtham, Varun},
booktitle={2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)}, title={Perceived Needs and Gains from an Industrial Study in Cloud Testing Automation},
year={2018},
volume={},
number={},
pages={238-244},
abstract={Challenges, methods and tools for testing in the Cloud have been actively researched, however there is lack of evidence about the actual motivations, issues and gains for adoption of automated cloud testing technology in real world industrial contexts. In this paper we report our findings from an empirical study involving four quasi-experiments within different application domains, namely e-commerce, 5G networking, WebRTC and IoT. The study is part of the ElasTest validation strategy, aiming at assessing the impact of the ElasTest open source platform for end-to-end testing of large distributed systems.},
keywords={Testing;Cloud computing;Measurement;Computer bugs;Browsers;Automation;Tools;Cloud testing;Industrial demonstrator;Quasi experiment},
doi={10.1109/UCC-Companion.2018.00062},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8606632,
author={Burdick, Sam and Risner, Jahrme and Chiu, David and Sawin, Jason},
booktitle={2018 IEEE/ACM 5th International Conference on Big Data Computing Applications and Technologies (BDCAT)}, title={Fault-Tolerant Query Execution over Distributed Bitmap Indices},
year={2018},
volume={},
number={},
pages={21-30},
abstract={Advances in storage software and filesystems have proliferated a vast array of easy-to-use distributed storage services, removing the barrier for a growing number of organizations to geo-distribute large data sets. While leaving data in their distributed environments is convenient for data collection, various types of processing (that might use multiple data sources) are precluded due to the prohibitive costs of data movement. Users are therefore burdened with finding creative ways of performing data analysis, often requiring expert knowledge in multiple domains. This paper reports on the design and implementation of a query engine that enables high-level queries over distributed data sets. Our system generates bitmap indices at multiple geo-distributed data sources in order to approximate large amounts of raw data values. The bitmaps are replicated for fault-tolerance and performance. Upon accepting a high-level (SQL-like) query, our system generates a query plan, resolves dependencies, and schedules for its execution over the distributed system. The system has been tested rigorously, and experimental results show that most overheads (i.e., query planning, node spawning, etc.) are negligible. Our testing also shows that our system is capable of delivering query results in the face of node failures, with no observable impact on query execution for up to 20% of the system failing. The system also provides a framework that is easily extendible for future research on the interplay between distributed systems and bitmap indices.},
keywords={Indexes;Distributed databases;Fault tolerance;Fault tolerant systems;Engines;Organizations;Remuneration;fault tolerance;bitmap indexing;distributed query processing},
doi={10.1109/BDCAT.2018.00012},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8601012,
author={Çilden, Erkin and Gültekin, Emre and Poyraz, Doğan and Canberi, M. Haluk},
booktitle={2018 IEEE/ACM 22nd International Symposium on Distributed Simulation and Real Time Applications (DS-RT)}, title={A Generic Distributed Architecture to Integrate Simulated Participants with Modular Avionics},
year={2018},
volume={},
number={},
pages={1-2},
abstract={This study proposes a generic distributed system integration laboratory architecture for avionics, which brings together three major architectural components to maintain flexibility: the actual avionic hardware to be tested, the simulated participants of the intended target system, and a configurable data gateway which acts as a hub to integrate modular avionics with the simulated space. As a proof of concept, an implementation of the architecture is presented and discussed.},
keywords={Aerospace electronics;Logic gates;Computer architecture;Software;Hardware;Data models;System integration;system integration laboratory;DDS;tactical environment simulation;avionics;data gateway},
doi={10.1109/DISTRA.2018.8601012},
ISSN={1550-6525},
month={Oct},}
@INPROCEEDINGS{8593117,
author={Abbache, Farid and Kalla, Hamoudi},
booktitle={2018 21st Saudi Computer Society National Computer Conference (NCC)}, title={Maximizing Reliability of Heterogeneous Distributed System Using an Adapted Discrete Flower Pollination Algorithm for Task Allocation Problem},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Finding task allocation that maximizes reliability of a heterogeneous distributed system is an NP-hard problem. For that, meta-heuristic is used to get a sub optimal solution in reasonable time. The Flower Pollination algorithm is new meta-heuristic used successfully for solving different problems in different fields. The original Flower Pollination algorithm is designed to deal with continuous problem. Thus, applying this algorithm to discrete problems in its original form seems to be very hard or useless. In this paper, we propose an adapted discrete version of the Flower Pollination algorithm to deal with the problem of maximizing reliability of a heterogeneous distributed system under task allocation problem. This algorithm is called Adapted Discrete Flower Pollination algorithm (ADFP). To confirm the effectiveness of our algorithm, we have tested and compared its results with that of Hybrid Particle Swarm optimization (HPSO). The Experiments results show the effectiveness and superiority of ADFP over HPSO in all tested cases.},
keywords={Task analysis;Program processors;Resource management;Reliability;Heuristic algorithms;Switches;Computational modeling;Discrete Flower Pollination algorithm;Heterogeneous distributed system;Reliability;Task allocation},
doi={10.1109/NCG.2018.8593117},
ISSN={},
month={April},}
@INPROCEEDINGS{8588539,
author={Bombardelli, Felipe Gustavo and Vidal, Leonardo de Amaral and Todt, Eduardo},
booktitle={2018 Latin American Robotic Symposium, 2018 Brazilian Symposium on Robotics (SBR) and 2018 Workshop on Robotics in Education (WRE)}, title={Unified Robotic System: Exploring Limitation and Opportunities},
year={2018},
volume={},
number={},
pages={129-134},
abstract={On robotics, distributed systems have been used to organize the software projects in modules, facilitating the development, sharing, maintenance and testing. However, this modularization brings an increased complexity of communication between the modules, which must be synchronized and standardized, motivating the development of various efforts to overcome these difficulties. In addition, the module code has several overhead lines over the framework used, which limits sharing of its modules between different frameworks. This work proposes a model based in the Von Neumann architecture to model different systems and to perform a study on the data types to improve the transparency, code orthogonality and platform independence.},
keywords={Robots;Task analysis;Computer architecture;Service-oriented architecture;Servers;Data models;Robotic Software Frameworks, Modularization},
doi={10.1109/LARS/SBR/WRE.2018.00032},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8590404,
author={Yu, Yue and Wada, Keiji},
booktitle={2018 IEEE International Power Electronics and Application Conference and Exposition (PEAC)}, title={Higher-reliable DC Distribution Systems using the Triple Active Bridge Converter without Batteries},
year={2018},
volume={},
number={},
pages={1-7},
abstract={Due to significant issue of energy consumption and sharp power demand of information and computing systems in worldwide, the applications of DC micro-grid that can utilize renewable energy sources such as photovoltaic power and wind energy have been advanced. Consequently, a DC micro-grid should be upgraded to achieve high reliability in power distribution systems. For DC power distribution in a data center, power managements including power flow control and demand analysis are highly recommended. In this paper, a prototype of DC power distribution system is proposed using the triple active bridge (TAB) converter for data centers. Furthermore, the power management for distributed system uses the TAB converter for power flow control is introduced. Eventually, performance test of power management in high reliable system is verified by simulation, and the reliability analysis proofed the reliability of proposed system superior than the conventional power distribution systems.},
keywords={Power distribution;Reliability;Data centers;Power system reliability;Load flow control;Load flow;DC micro-grid;distributed system;triple active bridge (TAB) converter;reliability analysis},
doi={10.1109/PEAC.2018.8590404},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8554682,
author={Raza, Ali and Navaie, Keivan and Nicholson, Richard},
booktitle={2018 Fifth International Conference on Internet of Things: Systems, Management and Security}, title={An Architecture for Dependable Connectivity in OSGi-Enabled Dynamic Distributed Systems},
year={2018},
volume={},
number={},
pages={99-106},
abstract={From air pollution monitoring to debatable surveillance for better security, dynamic distributed systems led to the birth of versatile smart environments. Dependability of such systems is challenged by the reliability of the communication links between various sub-systems. In this paper, we address this issue by designing a TCP/I P based Client-Server architecture using diverse channels, to ensure zero tolerance with regards to outage of service. The prototype system uses Raspberry Pi 3, as a remote client, to intelligently communicate with the server node by choosing one or a set of available communication links, e.g., Ethernet (LAN), Wireless-LAN (Wi-Fi), and Bluetooth channels. We further implement the system using Open Services Gateway Initiative (OSGi), a modular and interpolate-able code foundation, to withstand the challenges faced by the modern software industry e.g. complexity and scalability. Prototype system was successfully tested for hardware and software fault tolerance using different test scenarios to ensure uninterrupted service delivery. In the end, we also present a machine learning technique to mitigate the effects of severe channel hostilities for diverse channels system. The results show improvement in the quality of data transmission by exploiting the flexibility of alternate channels. We demonstrate this intelligent and seamless communication link switching technique using Support Vector Machine (SVM) in MATLAB.},
keywords={Dynamic distributed systems;dependability;Support Vector Machine (SVM);reliability;security;software complexity;modular programing},
doi={10.1109/IoTSMS.2018.8554682},
ISSN={},
month={Oct},}
@ARTICLE{8548553,
author={Gavriluţ, Voica and Zhao, Luxi and Raagaard, Michael L. and Pop, Paul},
journal={IEEE Access}, title={AVB-Aware Routing and Scheduling of Time-Triggered Traffic for TSN},
year={2018},
volume={6},
number={},
pages={75229-75243},
abstract={IEEE 802.1 time-sensitive networking (TSN) is a set of amendments to the IEEE 802.1 standard that enable safety-critical and real-time behavior over Ethernet for the industrial automation and automotive domains. Selected TSN mechanisms offer the possibility to emulate the well-known traffic classes found in mixed-criticality distributed systems: Time-triggered (TT) communication with low jitter and bounded endto-end latency, audio-video-bridging (AVB) streams with bounded end-to-end latency, and general besteffort messages, which have no timing guarantees. Critical traffic is guaranteed via the global network schedule which is stored in so-called gate control lists (GCLs) and controls the timely behavior of frames for each queue of an egress port. Although researchers have started to propose approaches for the routing and scheduling (i.e., GCL synthesis) of TT traffic, all previous research has ignored lower priority realtime traffic, such as AVB, resulting in TT configurations that may increase the worst-case delays of AVB traffic, rendering it unschedulable. In this paper, we propose a joint routing and scheduling approach for TT traffic, which takes into account the AVB traffic, such that both TT and the AVB traffic are schedulable. We extensively evaluate our approach on a number of synthetic and realistic test cases.},
keywords={Routing;Real-time systems;Standards;Schedules;Ethernet;Timing;Logic gates;IEEE 802.1 Time-sensitive networking;deterministic ethernet;real-time networks;routing;scheduling;meta-heuristic optimization},
doi={10.1109/ACCESS.2018.2883644},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8539369,
author={Kurtz, Fabian and Laukhin, Igor and Bektas, Caner and Wietfeld, Christian},
booktitle={2018 International Conference on Information and Communication Technology Convergence (ICTC)}, title={Evaluating Software-Defined Networking-Driven Edge Clouds for 5G Critical Communications},
year={2018},
volume={},
number={},
pages={405-410},
abstract={Modern societies are increasingly dependent on Critical Infrastructures (CIs) such as Smart Grids (SGs) and Intelligent Transportation Systems (ITS). Due to continuously rising demands in terms of efficiency and capabilities, complex control and monitoring strategies are employed. These in turn rely on robust, high performance communication networks. As CIs are distributed systems with diverging demands, the deployment of individual networks is often too costly and time consuming. Furthermore, the vast physical scope traditionally results in unacceptably high end-to-end delays. Therefore, a convergence of public and dedicated networks, as well as traditional Information and Communication Technology (ICT) with Information Technology (IT), are considered key aspects of 5G for addressing these challenges. Hence, this paper provides an empirical evaluation of CI communication services on basis of a Software-Defined Networking (SDN) and Network Function Virtualization (NFV) driven Edge Clouds (ECs) within a sliced 5G network. By shifting computing resources from the backbone or back office towards the network's access level, ECs allow for drastically reduced delays. Also, the traffic load on several layers of the communication infrastructure is reduced, as data can be kept locally, i.e. close to the source. This is demonstrated by shifting an ITS application from a central cloud to the EC. Services are transferred step-wise and transparently, minimizing interruptions while dynamically adapting to the backhaul's available data rate. The developed system is evaluated under realistic traffic conditions within a physical testing environment.},
keywords={Cloud computing;Delays;5G mobile communication;Testing;Network slicing;Quality of service;Communication networks},
doi={10.1109/ICTC.2018.8539369},
ISSN={2162-1233},
month={Oct},}
@INPROCEEDINGS{8535938,
author={Yang, Dongsheng and Lian, Mengjia and Zhang, Zhan and Li, Mingshi},
booktitle={2018 IEEE International Conference on Intelligence and Safety for Robotics (ISR)}, title={The research and design of Pub/Sub Communication Based on Subscription Aging},
year={2018},
volume={},
number={},
pages={475-479},
abstract={The large-scale distributed systems usually use publish/subscribe to implement asynchronous and loosely-coupled communication, which well satisfies the requirements for resource sharing and work coordination between distributed applications. When a large number of messages are transmitted in traditional pub/sub communication system, the message server often suffers from performance bottlenecks and single point of failure. Hence this article proposes a pub/sub communication model based on subscription aging, which is an improvement over the traditional pub/sub model. It sets the priority of subscribers according to the timeliness of subscription conditions, introduces subscriber groups to classify the subscribers, and then adopts multicast transmission method to send data to the sub_group. The way improves the efficiency and performance of the pub/sub system. Multiple priority cache queues were designed based on the mapping relationship between priority and hot topics. According to experimental tests, this method has good communication performance when the number of subscribers gradually increase, and the caching mechanism ensures the reliability of the system.},
keywords={Servers;Aging;Reliability;Safety;Load modeling;Computer network reliability},
doi={10.1109/IISR.2018.8535938},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8529940,
author={Zhang, Li and Li, Qinwei},
booktitle={2018 10th International Conference on Modelling, Identification and Control (ICMIC)}, title={Research on Consensus Efficiency Based on Practical Byzantine Fault Tolerance},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Byzantine Fault Tolerance (BFT) will be increasingly important to provide reliability in distributed systems and various Byzantine fault-tolerant algorithms have been proposed to improve efficiency. However, the practicality of BFT is still limited by its cost of consensus. Practical Byzantine Fault Tolerance (PBFT) is one of the best algorithms used in reality. Confronted with a distributed system with numerous nodes, PBFT is not satisfying. In order to improve the consensus efficiency, a new algorithm named Group-Hierarchy (GH) based on PBFT is proposed in this article. GH divides all replicas in a distributed system into several groups and every group has a primary. Replicas in a group reach an agreement respectively, which is called Local Consensus. And primaries of all the groups also reach a consensus based on the Local Consensus, which is called Global Consensus. After the Local Consensus and Global Consensus, replicas could reply to client. GH correlates the message complexity to the number of replicas and the number of groups in a distributed system in theory. The message complexity of GH is lower than PBFT in theory and the performed tests show that GH can execute a request even several times faster than PBFT.},
keywords={Fault tolerance;Fault tolerant systems;Complexity theory;Big Data;Computer science;Practical Byzantine Fault Tolerance;Distributed systems;Parallel processing;Consensus},
doi={10.1109/ICMIC.2018.8529940},
ISSN={},
month={July},}
@INPROCEEDINGS{8516724,
author={Mukhin, Vadim and Kornaga, Yaroslav and Bazaka, Yuriy and Bazaliy, Maxim and Yakovleva, Alla},
booktitle={2018 IEEE First International Conference on System Analysis Intelligent Computing (SAIC)}, title={Modified Method of Software Testing for Distributed Computer System},
year={2018},
volume={},
number={},
pages={1-4},
abstract={In this work are described the issues on the software testing methods organization in the distributed computer systems. There are presented the features of classical testing methods implementation for the distributed systems, and is suggested a modified testing method based on Kohn pyramid which allow empower the testing process and take into account the features of distributed system. There is developed an environment for modelling of the testing procedures in distributed system to maintain the experimental comparative researches.},
keywords={testing;software;distributed system;Kohn testing pyramid},
doi={10.1109/SAIC.2018.8516724},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8500651,
author={Xiong, Guangming and Kang, Ziyi and Li, Hao and Song, Weilong and Jin, Yaying and Gong, Jianwei},
booktitle={2018 IEEE Intelligent Vehicles Symposium (IV)}, title={Decision - making of Lane Change Behavior Based on RCS for Automated Vehicles in the Real Environment},
year={2018},
volume={},
number={},
pages={1400-1405},
abstract={This paper proposes the decision-making framework of lane change behavior based on Hierarchical State Machine (HSM) and we build distributed control system architecture based on RCS (Real-Time Control System) to test the model. Environment perception module, decision planning module and execution control module are put into the distributed system architecture based on RCS to improve real-time and ensure that several modules run simultaneously. Besides, the decision-making framework of lane change behavior consists of two parts: miniature scene information model and decision-making model of lane change behavior based on multi-attribute decision-making. The decision-making model of lane change behavior is based on HSM and it sets two top-level state machines: free lane change model and mandatory lane change model. Free lane change model changes the state by using lane reward model to judge and assess driving condition of each lane, while mandatory lane change model uses strategy of multi-source information fusion tojudge whether to lane change. In the end, the unmanned platform BYD Tang using vehicle embedded platform is used to verify the reliability and effectiveness of the lane change decision-making algorithm proposed in this paper in the real road environment.},
keywords={Decision making;Mathematical model;Real-time systems;Roads;Automobiles;Task analysis;Planning;automated vehicle;lane change behavior decision-making model;hierarchical state machine;RCS;real-vehicle tests},
doi={10.1109/IVS.2018.8500651},
ISSN={1931-0587},
month={June},}
@INPROCEEDINGS{8486176,
author={Al-Oqaily, Ali. T. and Shakah, Ghazi},
booktitle={2018 8th International Conference on Computer Science and Information Technology (CSIT)}, title={SOLVING NON-LINEAR OPTIMIZATION PROBLEMS USING PARALLEL GENETIC ALGORITHM},
year={2018},
volume={},
number={},
pages={103-106},
abstract={In last few decades, a growing interest in the domain of evolutionary algorithms has been observed due to its performance in discover the optimal solutions for the complex problems. The Genetic Algorithm (GA) is one of the most used evolutionary algorithms that attract the researchers' interests in many fields such as the physics and mathematics. GA can provide optimal solution for the problems of complex environments i.e. polarize environments. Like other evolutionary algorithms, the execution time of GA is relatively long, whereby the optimization processes could consume many hours. This study aims to improve the optimization accuracy and reduce the execution time of traditional GA. The parallel GA is proposed to conduct the optimization processes through distributed machines or processors (network of processes). The original complex problem is divided into sub-small areas, and each sub area is optimized by sequential GA that applied in each processor in the network. The final solution is collected from all processors in the network in order to decide the best final solution. To evaluate the proposed parallel genetic algorithm, two complex problems are optimized; De Jong's and Ackeleys path functions. For testing purpose, a network of four processors is constructed using MATLAB toolbox distributed system toolbox. The significances results show that proposed parallel GA give better results with less execution time than the traditional or sequential GA. The contribution of this study is the segmentation of large and complex area into small area and optimizes the solutions of the small areas using network of processors. This approach simplifies the optimized problems, reduce the execution time, and give better chances to discover the optimal solutions.},
keywords={Electronics packaging;Genetic algorithms;Optimization;Program processors;Sociology;Statistics;Sensors;GA;Parallel;Non-linear problems;optimization;distribution},
doi={10.1109/CSIT.2018.8486176},
ISSN={},
month={July},}
@INPROCEEDINGS{8484855,
author={Giraldo, Mario A. and Duitama, John F. and Arias-Londoño, Julián D.},
booktitle={2018 IEEE 1st Colombian Conference on Applications in Computational Intelligence (ColCACI)}, title={MapReduce and Spark-based architecture for bi-class classification using SVM},
year={2018},
volume={},
number={},
pages={1-6},
abstract={Support Vector Machine (SVM) is a classifier widely used in machine learning because of its high generalization capacity. The sequential minimal optimization (SMO) its most popular implementation, scales somewhere between linear and quadratic in the training set size for various test problems. This fact makes using SVM to train large data sets have a high computational cost. SVM implementations on distributed systems such as MapReduce and Spark have shown efficiency to improve computational cost; this paper analyzes how data subset size and number of mapping tasks affects SVM performance on MapReduce and Spark. Also, a cost model as a useful tool for setting data subset size according to available hardware and data to be processed is proposed.},
keywords={Task analysis;Support vector machines;Training;Sparks;Computational efficiency;Computational modeling;Machine learning;Support Vector Machine;MapReduce;Spark;Classification},
doi={10.1109/ColCACI.2018.8484855},
ISSN={},
month={May},}
@INPROCEEDINGS{8478849,
author={Parker, Glenn A.},
booktitle={SoutheastCon 2018}, title={Automatic Generation of Software Tools Using a Language Grammar},
year={2018},
volume={},
number={},
pages={1-4},
abstract={This paper discusses early results for automatic generation of software tools from interface definitions described by Interface Definition Language (IDL). Modern distributed systems and System-of-Systems designs often employ IDL to define messages that may be exchanged during operation, and these definitions are then used in the design and testing of individual subsystems. A method is presented here for using a popular domain-specific language compiler in conjunction with IDL definitions to generate test tools for a distributed system. This approach can dramatically speed development and decrease cost in systems where the interface definition is subject to change.},
keywords={Grammar;Tools;Software;Syntactics;C++ languages;Testing;Java;Software Testing;Lexer;Parser;Language Tools},
doi={10.1109/SECON.2018.8478849},
ISSN={1558-058X},
month={April},}
@INPROCEEDINGS{8469412,
author={Chen, Jinchao and Du, Chenglie and Jiang, Zejun},
booktitle={2018 2nd IEEE Advanced Information Management,Communicates,Electronic and Automation Control Conference (IMCEC)}, title={A Universal and Configurable Simulator for Distributed Systems},
year={2018},
volume={},
number={},
pages={900-904},
abstract={S271 has been widely adopted as a support tool for the validation and experimentation of distributed systems. It allows different devices and applications to be evaluated and analysed without requiring the actual presence of those machines. Although the simulation plays an important role in investigating the behaviours of distributed systems, it results in a serious simulator building problem as the systems become more complicated and dynamically data-driven. Most simulators are designed and developed to target a specific application, lacking the capabilities to become a general and standardized tool for researchers. In order to solve this problem, a new approach to implement a universal and configurable simulator is proposed in this paper. The simulator presented uses a logic automaton to simulate the activities of a real device, and generates the incentive data for tested equipment according to the predefined XML-based files. The proposed approach is a trustworthy means of improving the flexibility and robustness of a simulator design subject to future changes. The experimental results show that this approach is efficient and can meet the increasing requirement of modern simulations of real-time distributed systems.},
keywords={Automata;Performance evaluation;Real-time systems;Communication networks;Computational modeling;Tools;Software;simulator;distributed system;incentive data;logical automaton;configurability},
doi={10.1109/IMCEC.2018.8469412},
ISSN={},
month={May},}
@ARTICLE{8454427,
author={Lee, Seil and Kim, Hanjoo and Park, Seongsik and Kim, Seijoon and Choe, Hyeokjun and Yoon, Sungroh},
journal={IEEE Access}, title={CloudSocket: Fine-Grained Power Sensing System for Datacenters},
year={2018},
volume={6},
number={},
pages={49601-49610},
abstract={Today's data centers have various computing and storage devices for processing a myriad of data, and they generally consume a considerable amount of electrical energy. This paper proposes a smart grid-inspired methodology to observe and profile the power consumption of a data center. Based on this technique, our paper provides information that is useful for moderating the peak power consumption of the data centers. Our power measurement platform consists of several devices named CloudSockets, and each CloudSocket unit can measure the power consumption of multiple computing nodes and periodically transmit measurement data wirelessly to the coordinator unit. This data can be used to analyze the relationship between the workload and the power consumption of the data center. We tested our methodology through the application of various algorithms with a 32-node distributed system that runs Apache Spark for large-scale data analytics. An analysis of our experimental results reveals how and where the peak power of each node in the grid overlaps, providing opportunities for informed coordination of the computing components for peak power reduction.},
keywords={Power demand;Power measurement;Monitoring;Servers;Machine learning;Smart grids;Sensors;Datacenter;smart grid;cloud computing},
doi={10.1109/ACCESS.2018.2868469},
ISSN={2169-3536},
month={},}
@ARTICLE{8451869,
author={Ficco, Massimo and Pietrantuono, Roberto and Russo, Stefano},
journal={IEEE Access}, title={Hybrid Simulation and Test of Vessel Traffic Systems on the Cloud},
year={2018},
volume={6},
number={},
pages={47273-47287},
abstract={This paper presents a cloud-based hybrid simulation platform to test large-scale distributed system-of-systems for the management and control of maritime traffic, the so-called vessel traffic systems (VTS). A VTS consists of multiple, heterogeneous, distributed, and interoperating systems, including radar, automatic identification systems, direction finders, electro-optical sensors, and gateways to external VTSs, information systems; identifying, representing, and analyzing interactions is a challenge to the evaluation of the real risks for safety and security of the marine environment. The need for reproducing in fabric the system behaviors that could occur in situ demands for the ability of integrating emulated and simulated environments to cope with the different testability requirements of involved systems and to keep testing cost sustainable. The platform exploits hybrid simulation and virtualization technologies, and it is deployable on a private cloud, reducing the cost of setting up realistic and effective testing scenarios.},
keywords={Testing;Complexity theory;Cloud computing;Radar;Computational modeling;Marine vehicles;Cloud computing;emulation;HLA;simulation;system-of-systems;testing;vessel traffic systems},
doi={10.1109/ACCESS.2018.2865683},
ISSN={2169-3536},
month={},}
@INPROCEEDINGS{8450397,
author={Gurabi, Mehdi Akbari and Alfandi, Omar and Bochem, Arne and Hogrefe, Dieter},
booktitle={2018 14th International Wireless Communications Mobile Computing Conference (IWCMC)}, title={Hardware based Two-Factor User Authentication for the Internet of Things},
year={2018},
volume={},
number={},
pages={1081-1086},
abstract={In the distributed Internet of Things (IoT) architecture, sensors collect data from vehicles, home appliances and office equipment and other environments. Various objects contain the sensor which process data, cooperate and exchange information with other embedded devices and end users in a distributed network. It is important to provide end-to-end communication security and an authentication system to guarantee the security and reliability of the data in such a distributed system. Two-factor authentication is a solution to improve the security level of password-based authentication processes and immunized the system against many attacks. At the same time, the computational and storage overhead of an authentication method also needs to be considered in IoT scenarios. For this reason, many cryptographic schemes are designed especially for the IoT; however, we observe a lack of laboratory hardware test beds and modules, and universal authentication hardware modules. This paper proposes a design and analysis for a hardware module in the IoT which allows the use of two-factor authentication based on smart cards, while taking into consideration the limited processing power and energy reserves of nodes, as well as designing the system with scalability in mind.},
keywords={Authentication;Smart cards;Wireless sensor networks;Sensors;Hardware;Protocols;Internet of Things;two-factor authentication;embedded security;public key infrastructure;smart card;wireless sensor networks},
doi={10.1109/IWCMC.2018.8450397},
ISSN={2376-6506},
month={June},}
@INPROCEEDINGS{8442524,
author={Ray, Biplob R. and Chowdhury, Sujan},
booktitle={2018 8th International Conference on Cloud Computing, Data Science Engineering (Confluence)}, title={Reverse Engineering Technique (RET) to Predict Resource Allocation in a Google Cloud System},
year={2018},
volume={},
number={},
pages={688-693},
abstract={This paper presents a reverse engineering machine learning technique for resource allocation in cloud computing system. Efficient and timely resource allocation is a crucial task for complex operations in a large scale distributed system like cloud computing. Furthermore, to support Service Level Agreement (SLA) like priority, latency, and efficiency, the resource provisioning should be highly influenced by SLA requirements of the system. Therefore in this paper, we propose the Reverse Engineering Technique (RET) which highly influence by priority to improve resource allocation accuracy. The paper used neural network based deep learning and Levenberg-Marquardt training algorithm for resource allocation prediction. The dataset of Google cloud computing system, which is publicly available dataset for research, is used to test the proposed RET. Our experiment shows that the proposed technique improves resource provisioning accuracy for cloud based systems.},
keywords={Handheld computers;Cloud computing;Data science;Google;Machine learning;Resource management;Reverse engineering;cloud computing;SLA;machine learning;resource predicting;HPC;deep learning},
doi={10.1109/CONFLUENCE.2018.8442524},
ISSN={},
month={Jan},}
@INPROCEEDINGS{8436929,
author={Plewinski, Pawel and Makowski, Dariusz and Perek, Piotr and Napieralski, Andrzej},
booktitle={2018 25th International Conference "Mixed Design of Integrated Circuits and System" (MIXDES)}, title={Integration and Testing of Large Scale Diagnostic Systems},
year={2018},
volume={},
number={},
pages={85-88},
abstract={The design of the modern Instrumentation and Control (I&C) systems in large-scale projects such as ITER is technically challenging. They often consist of dozens of various complex and distributed systems, which are developed by numerous independent organizations and companies. The integration of the sub-systems into a coherent system is critical for such projects. System testing at all stages of development and integration is a common method of ensuring the proper quality and adherence to the accepted standards. It is necessary that the tests cover all the requirements and functionality of the system and that they can be executed in a repeatable and error-proof way in order to maximize the gains obtained from the use of testing. This paper presents the strategies of testing and system integration illustrated with an example of ITER project. An approach to automated system testing is also proposed.},
keywords={Integrated circuits;Instrumentation and Control;Integration;Testing;Large Scale Diagnostic Systems;System Testing Strategies;Divertor Neutron Flux Monitor;Automated Testing;Unit Testing;Experimental Physics and Industrial Control System},
doi={10.23919/MIXDES.2018.8436929},
ISSN={},
month={June},}
@INPROCEEDINGS{8433708,
author={Ileri, Can Umut and Dagdeviren, Orhan},
booktitle={2018 IEEE International Black Sea Conference on Communications and Networking (BlackSeaCom)}, title={Evaluating Fault Tolerance Properties of Self-Stabilizing Matching Algorithms in Wireless Sensor Networks},
year={2018},
volume={},
number={},
pages={1-5},
abstract={Self stabilization is an important paradigm for the autonomous recovery of a distributed system from transient failures such as energy depletion of nodes and disrupted connections. It has been used in wireless sensor networks (WSN) as these networks are expected to automatically recover from a transient fault without human intervention. Graph matching is fundamental a graph theory problem which has a broad application range in WSNs and it has been studied extensively in self-stabilizing settings. In this work, we build a simulation model and perform tests to evaluate the fault tolerance properties of self-stabilizing matching algorithms. To the best of our knowledge, this is the first practical evaluation of these algorithms. Considering WSNs, we assume distributed fair and synchronous schedulers. Simulation results have shown that there is a tradeoff between stabilization time of algorithms and the quality of their results. The improvement algorithms which has better lower bounds give better matchings at the cost of longer durations of instability.},
keywords={Wireless sensor networks;Approximation algorithms;Program processors;Conferences;Transient analysis;Complexity theory;Computational modeling;Self-stabilization;Graph Matching;Wireless Sensor Networks;Performance Evaluation},
doi={10.1109/BlackSeaCom.2018.8433708},
ISSN={},
month={June},}
@INPROCEEDINGS{8431197,
author={Brzeziński, Krzysztof M.},
booktitle={2018 11th International Conference on Human System Interaction (HSI)}, title={Tiny TTCN-Inspired Testing Tools for Experimenting with Hybrid IoT Systems},
year={2018},
volume={},
number={},
pages={261-267},
abstract={This work presents the design of a lightweight, low-end, virtually zero-cost on-line passive testing technology (Tiny TTCN-inspired Testing Tool) that is complementary to other, well established testing frameworks. A tester is built around an Arduino-class microcontroller and is programmed in a test language that re-creates the basic semantics and look-and-feel of the standardized test language TTCN-3 (Testing and Test Control Notation). It is intended for rapid prototyping and as a one-off development platform for the validation of IoT-class distributed systems involving humans-in-the-loop, especially for long-term unobtrusive supervision of in-the-wild behaviour change experiments in an instrumented home/work facility. To justify feasibility claims, selected implementation details are also reported.},
keywords={Testing;Tools;Telecommunications;Computer languages;Protocols;Random access memory;Microcontrollers;testing;runtime verification;programmable controllers;validation;socio- technical systems;behaviour change experiments},
doi={10.1109/HSI.2018.8431197},
ISSN={},
month={July},}
@INPROCEEDINGS{8433134,
author={Ozmen, Muslum Ozgur and Behnia, Rouzbeh and Yavuz, Attila A.},
booktitle={2018 IEEE Conference on Communications and Network Security (CNS)}, title={Compact Energy and Delay-Aware Authentication},
year={2018},
volume={},
number={},
pages={1-9},
abstract={Authentication and integrity are fundamental security services that are critical for any viable system. However, some of the emerging systems (e.g., smart grids, aerial drones) are delay-sensitive, and therefore their safe and reliable operation requires delay-aware authentication mechanisms. Unfortunately, the current state-of-the-art authentication mechanisms either incur heavy computations or lack scalability for such large and distributed systems. Hence, there is a crucial need for digital signature schemes that can satisfy the requirements of delay-aware applications. In this paper, we propose a new digital signature scheme that we refer to as Compact Energy and Delay-aware Authentication (CEDA). In CEDA, signature generation and verification only require a small-constant number of multiplications and Pseudo Random Function (PRF) calls. Therefore, it achieves the lowest end-to-end delay among its counterparts. Our implementation results on an ARM processor and commodity hardware show that CEDA has the most efficient signature generation on both platforms, while offering a fast signature verification. Among its delay-aware counter-parts, CEDA has a smaller private key with a constant-size signature. All these advantages are achieved with the cost of a larger public key. This is a highly favorable trade-0ff for applications wherein the verffier is not memory-limited. We open-sourced our implementation of CEDA to enable its broad testing and adaptation.},
keywords={Authentication;Public key;Digital signatures;Real-time systems;Smart grids;Time factors;Applied cryptography;delay-aware authentication;real-time networks;digital signatures},
doi={10.1109/CNS.2018.8433134},
ISSN={},
month={May},}
@INPROCEEDINGS{8429306,
author={Hildenbrandt, Everett and Saxena, Manasvi and Rodrigues, Nishant and Zhu, Xiaoran and Daian, Philip and Guth, Dwight and Moore, Brandon and Park, Daejun and Zhang, Yi and Stefanescu, Andrei and Rosu, Grigore},
booktitle={2018 IEEE 31st Computer Security Foundations Symposium (CSF)}, title={KEVM: A Complete Formal Semantics of the Ethereum Virtual Machine},
year={2018},
volume={},
number={},
pages={204-217},
abstract={A developing field of interest for the distributed systems and applied cryptography communities is that of smart contracts: self-executing financial instruments that synchronize their state, often through a blockchain. One such smart contract system that has seen widespread practical adoption is Ethereum, which has grown to a market capacity of 100 billion USD and clears an excess of 500,000 daily transactions. Unfortunately, the rise of these technologies has been marred by a series of costly bugs and exploits. Increasingly, the Ethereum community has turned to formal methods and rigorous program analysis tools. This trend holds great promise due to the relative simplicity of smart contracts and bounded-time deterministic execution inherent to the Ethereum Virtual Machine (EVM). Here we present KEVM, an executable formal specification of the EVM's bytecode stack-based language built with the K Framework, designed to serve as a solid foundation for further formal analyses. We empirically evaluate the correctness and performance of KEVM using the official Ethereum test suite. To demonstrate the usability, several extensions of the semantics are presented. and two different-language implementations of the ERC20 Standard Token are verified against the ERC20 specification. These results are encouraging for the executable semantics approach to language prototyping and specification.},
keywords={Contracts;Semantics;Tools;Virtual machining;Bitcoin;kevm;evm;ethereum;blockchain;k-framework;verification;smart-contract;semantics;ethereum-virtual-machine;formal-methods},
doi={10.1109/CSF.2018.00022},
ISSN={2374-8303},
month={July},}
@INPROCEEDINGS{8408010,
author={Kupper, Martin and Brenneisen, Jochen and Stark, Oliver and Krebs, Stefan and Hohmann, Sören},
booktitle={2018 Chinese Control And Decision Conference (CCDC)}, title={Cascaded Fractional Kalman Filtering for State and Current Estimation of Large-Scale Lithium-Ion Battery Packs},
year={2018},
volume={},
number={},
pages={5071-5078},
abstract={In this paper, a cascaded fractional Kalman filter for state of charge and branch current estimation of large- scale battery systems is proposed. As a centralized approach for the estimation of a large-scale system is costly in terms of effort and time, a partition into smaller and, therefore, simpler subsystems is applied. Since the overall system is divided into smaller units, a local computation is allowed and complexity reduced. In these distributed systems, usually, the subsystems communicate with each other to exchange relevant data. Using a model based on mesh currents, we receive a cascaded system structure which results in a hierarchical arrangement of all subsystems. This concludes in a one-directional information flow and, therefore, reduces the overall communication effort. Using this proposed approach, it is not only possible to estimate the states of each branch locally but also to calculate the branch currents when the total current is known. Finally, a practical test with real measurement data is presented.},
keywords={Batteries;State of charge;Estimation;Integrated circuit modeling;Kalman filters;Current measurement;Battery charge measurement;State of Charge estimation;Current estimation;Kalman filter;Cascaded systems;Fractional order;Battery pack;Large-scale;Lithium-ion},
doi={10.1109/CCDC.2018.8408010},
ISSN={1948-9447},
month={June},}
@INPROCEEDINGS{8377710,
author={Wu, Hsiang-Yi and Lee, Che-Rung},
booktitle={2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)}, title={Energy Efficient Scheduling for Heterogeneous Fog Computing Architectures},
year={2018},
volume={01},
number={},
pages={555-560},
abstract={Heterogeneous fog computing architectures that hybridize different types of edge nodes can achieve better scalability and lower cost to serve massive number of Internet Of Things (IOT) devices than the centralized architecture of cloud computing. In this paper, we propose an efficient scheduling algorithm to minimize the energy consumption for IOT workflows on heterogeneous fog computing architectures. We first build an integer linear programming model that minimizes the total energy. The purpose of the ILP model is not to be used directly for the computation, but to reveal key factors to minimize the energy consumption in a distributed system. Based on the observations from the model, we derive an energy minimization scheduling (EMS) algorithm that combines different policies to achieve the near optimal scheduling. We implemented and tested our model and algorithm via simulations. The experimental results show that EMS can achieve near optimal energy consumption with much faster execution time.},
keywords={Energy consumption;Computer architecture;Computational modeling;Edge computing;Scheduling;Energy management;Minimization;IOT, Heterogeneous, Fog Computing, Scheduling},
doi={10.1109/COMPSAC.2018.00085},
ISSN={0730-3157},
month={July},}
@ARTICLE{8362148,
author={Oliveira Rocha, Helder Roberto and Silvestre, Leonardo Jose and Cardoso Celeste, Wanderley and Custodio Coura, Daniel Jose and Rigo Junior, Luis Otavio},
journal={IEEE Latin America Transactions}, title={Forecast of Distributed Electrical Generation System Capacity Based on Seasonal Micro Generators using ELM and PSO},
year={2018},
volume={16},
number={4},
pages={1136-1141},
abstract={This work proposes the development of a computational tool that goal at forecasting the daily generation capacity of electric power in a distributed system based on micro generators that use renewable and seasonal sources. In the specific case, wind and photovoltaic microgenerators are used, which can be found in smart homes. The forecasting tool is based in Extreme Learning Machine (ELM) which is an artificial neural network model. The parameter selection implemented for ELM is based on the Particle Swarm Optimization (PSO). The forecasting system used the mathematical models of the seasonal micro-generators and a meteorological database of the geographic region where the distributed system is located. The tests performed indicate that the Mean Square Error Root (REQM) of the forecast is 7.3 percent.},
keywords={Smart grids;Mathematical model;RNA;Forecasting;Learning (artificial intelligence);IEEE transactions;Generators;Artificial Neural Networks;Demand Forecasting;Distributed Generation;Electric Power Systems;Smart Home},
doi={10.1109/TLA.2018.8362148},
ISSN={1548-0992},
month={April},}
@INPROCEEDINGS{8361300,
author={Liu, Yang and Zhou, Yang},
booktitle={2018 IEEE 15th International Conference on Networking, Sensing and Control (ICNSC)}, title={Development of distributed cache strategy for analytic cluster in an Internet of Things system},
year={2018},
volume={},
number={},
pages={1-6},
abstract={This paper discusses the development of a distributed cache strategy for an analytic cluster in an Internet of Things (IoT) system. In this paper, Least Recently Used (LRU), Proactive Cache and distributed system related concepts are discussed. The study about the approaches for performance optimization, nodes and data distributing in the IoT system is also performed. The novelty about this research is the concept of a placeholder and its use together with Proactive Cache to optimize the traditional LRU algorithm in the use case of a data analytic cluster in an IoT system, which can reduce the response delay of the query up to 90% in the test case.},
keywords={Data models;Data structures;Distributed databases;Cache storage;Optimization;Approximation algorithms;caching;distributed system;IoT;data partitioning;cluster topology},
doi={10.1109/ICNSC.2018.8361300},
ISSN={},
month={March},}
@INPROCEEDINGS{8341605,
author={Aldhaheri, Ahmed and Etemadi, Amir},
booktitle={2018 IEEE Applied Power Electronics Conference and Exposition (APEC)}, title={DC distributed systems stabilization and performance improvement using small-signal voltage injection},
year={2018},
volume={},
number={},
pages={3481-3485},
abstract={In this paper, we introduce a method to eliminate the impact of constant power loads (CPLs) on dc distributed systems. A CPL, in small-signal sense, resembles a negative resistance that is connected to its source. The negative resistance interacts with the source output impedance, leading to instability or degrading the dynamic performance at the dc-bus. The proposed method injects the bus-voltage oscillations into the control loop of the source-converter using a high-pass filter. The filter suppresses the dc quantity of the bus-voltage, while passing the high-frequency oscillations. Thus, the operating point of the source-converter is not modified, while the dynamic performance is improved. A prototype dc distributed system was analytically analyzed, simulated, and experimentally tested in order to validate the effectiveness of the proposed controller.},
keywords={Power system dynamics;Impedance;Oscillators;Stability criteria;Voltage control;Vehicle dynamics},
doi={10.1109/APEC.2018.8341605},
ISSN={2470-6647},
month={March},}
@INPROCEEDINGS{8337250,
author={Ivashko, A. Alexander G. and Vorobeva, B. Marina S. and Oshepkov, C. Anatoliy Yu. and Vorobev, D. Artem M.},
booktitle={2018 Moscow Workshop on Electronic and Networking Technologies (MWENT)}, title={Mathematical model for administration of web-conference distributed systems},
year={2018},
volume={},
number={},
pages={1-5},
abstract={The development of Web-technologies has led to the development of tools for distance technologies in education. Every year the market of web conferencing systems is replenished with both proprietary and freely distributed systems. The main "bottleneck" in conducting web conferencing is the server load, which can occur both in the insufficient server performance, and in the insufficient bandwidth of the Internet channel. In the article the Mathematical model of the distributed system with the addition of a distributor, the advantages of which are that the possible options are concealed from the user, and the system chooses the server itself, and the algorithm for choosing the "optimal" node in a distributed web conferencing system. The mathematical model of distributed web conferencing system allows describing the state of the system at any time in the form of a set of server characteristics, created webinars and client applications connected to them. A series of full-scale tests of the developed software complex, which manages web conferencing deployed in a cluster of three servers powered by IBM Lotus Sametime, was conducted. Model experiments of managing a distributed web conference system were conducted.},
keywords={Servers;Mathematical model;Internet;Conferences;Bandwidth;Probability;Education;distance learning;distributed information systems;information technology;monitoring;videoconferences;web-conference},
doi={10.1109/MWENT.2018.8337250},
ISSN={},
month={March},}
@INPROCEEDINGS{8330613,
author={Emeka, Busalire Onesmus and Liu, Shaoying},
booktitle={2018 International Conference on Electronics, Information, and Communication (ICEIC)}, title={Assessing and extracting software security vulnerabilities in SOFL formal specifications},
year={2018},
volume={},
number={},
pages={1-4},
abstract={The growth of the internet has brought along positive gains such as the emergence of a highly interconnected world. However, on the flip side, there has been a growing concern on how secure distributed systems can be built effectively and tested for security vulnerabilities prior to deployment. Developing a secure software product calls for a deep technical understanding of some complex issues with regards to the software and its operating environment, as well as embracing a systematic approach of analyzing the software. This paper proposes a method for identifying software security vulnerabilities from software requirement specifications written in Structured Object-oriented Formal Language (SOFL). Our proposed methodology leverages on the concept of providing an early focus on security by identifying potential security vulnerabilities at the requirement analysis and verification phase of the software development life cycle.},
keywords={Software;Formal specifications;Natural languages;Input variables;Password;Computer bugs;Software Security Vulnerabilities;SOFL;Security Vulnerability Extraction;Requirement Specifications},
doi={10.23919/ELINFOCOM.2018.8330613},
ISSN={},
month={Jan},}
@INPROCEEDINGS{8330260,
author={Liang, Jie and Wang, Mingzhe and Chen, Yuanliang and Jiang, Yu and Zhang, Renwei},
booktitle={2018 IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER)}, title={Fuzz testing in practice: Obstacles and solutions},
year={2018},
volume={},
number={},
pages={562-566},
abstract={Fuzz testing has helped security researchers and organizations discover a large number of vulnerabilities. Although it is efficient and widely used in industry, hardly any empirical studies and experience exist on the customization of fuzzers to real industrial projects. In this paper, collaborating with the engineers from Huawei, we present the practice of adapting fuzz testing to a proprietary message middleware named libmsg, which is responsible for the message transfer of the entire distributed system department. We present the main obstacles coming across in applying an efficient fuzzer to libmsg, including system configuration inconsistency, system build complexity, fuzzing driver absence. The solutions for those typical obstacles are also provided. For example, for the most difficult and expensive obstacle of writing fuzzing drivers, we present a low-cost approach by converting existing sample code snippets into fuzzing drivers. After overcoming those obstacles, we can effectively identify software bugs, and report 9 previously unknown vulnerabilities, including flaws that lead to denial of service or system crash.},
keywords={Fuzzing;Tools;Libraries;Software;Computer bugs;Complexity theory},
doi={10.1109/SANER.2018.8330260},
ISSN={},
month={March},}
@ARTICLE{8318901,
author={Morán, Jesús and Bertolino, Antonia and de la Riva, Claudio and Tuya, Javier},
journal={IEEE Transactions on Reliability}, title={Automatic Testing of Design Faults in MapReduce Applications},
year={2018},
volume={67},
number={3},
pages={717-732},
abstract={New processing models are being adopted in Big Data engineering to overcome the limitations of traditional technology. Among them, MapReduce stands out by allowing for the processing of large volumes of data over a distributed infrastructure that can change during runtime. The developer only designs the functionality of the program and its execution is managed by a distributed system. As a consequence, a program can behave differently at each execution because it is automatically adapted to the resources available at each moment. Therefore, when the program has a design fault, this could be revealed in some executions and masked in others. However, during testing, these faults are usually masked because the test infrastructure is stable, and they are only revealed in production because the environment is more aggressive with infrastructure failures, among other reasons. This paper proposes new testing techniques that aimed to detect these design faults by simulating different infrastructure configurations. The testing techniques generate a representative set of infrastructure configurations that as whole are more likely to reveal failures using random testing, and partition testing together with combinatorial testing. The techniques are automated by using a test execution engine called MRTest that is able to detect these faults using only the test input data, regardless of the expected output. Our empirical evaluation shows that MRTest can automatically detect these design faults within a reasonable time.},
keywords={Combinatorial testing;Software testing ;Optimization;Big Data;Big Data;combinatorial testing;MapReduce;metamorphic testing;partition testing;random testing;software testing},
doi={10.1109/TR.2018.2802047},
ISSN={1558-1721},
month={Sep.},}
@ARTICLE{8307419,
author={Hasan, Khondokar Fida and Feng, Yanming and Tian, Yu-Chu},
journal={IEEE Transactions on Intelligent Transportation Systems}, title={GNSS Time Synchronization in Vehicular Ad-Hoc Networks: Benefits and Feasibility},
year={2018},
volume={19},
number={12},
pages={3915-3924},
abstract={Time synchronization is critical for the operation of distributed systems in networked environments. It is also demanded in vehicular ad-hoc networks (VANETs), which, as a special type of wireless networks, are becoming increasingly important for emerging cooperative intelligent transport systems. Global navigation satellite system (GNSS) is a proven technology to provide precise timing information in many distributed systems. It is well recognized to be the primary means for vehicle positioning and velocity determination in VANETs. However, GNSS-based time synchronization is not well understood for its role in the coordination of various tasks in VANETs. To address this issue, this paper examines the requirements, potential benefits, and feasibility of GNSS time synchronization in VANETs. The availability of GNSS time synchronization is characterized by almost 100% in our experiments in high-rise urban streets, where the availability of GNSS positioning solutions is only 80%. Experiments are also conducted to test the accuracy of time synchronization with 1-PPS signals output from consumer-grade GNSS receivers. They have shown 30-ns synchronization accuracy between two receivers of different models. All these experimental results demonstrate the feasibility of GNSS time synchronization for stringent VANET applications.},
keywords={Synchronization;Vehicular ad hoc networks;Global navigation satellite system;Mobile computing;Vehicular ad-hoc network;time synchronization;GNSS},
doi={10.1109/TITS.2017.2789291},
ISSN={1558-0016},
month={Dec},}
@INPROCEEDINGS{8301632,
author={Dominka, Sven and Mandl, Michael and Dübner, Michael and Ertl, Dominik},
booktitle={2018 IEEE 8th Annual Computing and Communication Workshop and Conference (CCWC)}, title={Using combinatorial testing for distributed automotive features: Applying combinatorial testing for automated feature-interaction-testing},
year={2018},
volume={},
number={},
pages={490-495},
abstract={Modern passenger cars have a comprehensive embedded distributed system with a huge number of bus devices interlinked in several communication networks. The number of (distributed) features and hence the risk of undesired feature-interaction within this distributed system rises significantly. Such distributed automotive features pose a huge challenge in terms of efficient testing. Bringing together Combinatorial Testing with Automated Feature-Interaction Testing reduces the testing effort for such features significantly.},
keywords={Testing;Automotive engineering;Software product lines;Automation;Sensors;Embedded software;distributed system;embedded system;distributed feature;feature interaction;automotive;software testing;combinatorial testing;test automation},
doi={10.1109/CCWC.2018.8301632},
ISSN={},
month={Jan},}
@ARTICLE{8302601,
author={Jia, Qi and Guo, Linke and Jin, Zhanpeng and Fang, Yuguang},
journal={IEEE Transactions on Parallel and Distributed Systems}, title={Preserving Model Privacy for Machine Learning in Distributed Systems},
year={2018},
volume={29},
number={8},
pages={1808-1822},
abstract={Machine Learning based data classification is a widely used data mining technique. By learning massive data collected from the real world, data classification helps learners discover hidden data patterns. These hidden data patterns are represented by the learned model in different machine learning schemes. Based on such models, a user can classify whether the new incoming data belongs to an existing class; or, multiple entities may test the similarity of their datasets. However, due to data locality and privacy concerns, it is infeasible for large-scale distributed systems to share each individual's datasets for classifying or testing. On the one hand, the learned model is an entity's private asset and may leak private information, which should be well protected from all other non-collaborative entities. On the other hand, the new incoming data may contain sensitive information which cannot be disclosed directly for classification. To address the above privacy issues, we propose an approach to preserve the model privacy of the data classification and similarity evaluation for distributed systems. With our scheme, neither new data nor learned models are directly revealed during the classification and similarity evaluation procedures. Based on extensive real-world experiments, we have evaluated the privacy preservation, feasibility, and efficiency of the proposed scheme.},
keywords={Data models;Data privacy;Predictive models;Distributed databases;Support vector machines;Cryptography;Privacy;Machine learning;privacy preservation;data classification;model evaluation},
doi={10.1109/TPDS.2018.2809624},
ISSN={1558-2183},
month={Aug},}
@ARTICLE{7932530,
author={Bianchi, Francesco Adalberto and Margara, Alessandro and Pezzè, Mauro},
journal={IEEE Transactions on Software Engineering}, title={A Survey of Recent Trends in Testing Concurrent Software Systems},
year={2018},
volume={44},
number={8},
pages={747-783},
abstract={Many modern software systems are composed of multiple execution flows that run simultaneously, spanning from applications designed to exploit the power of modern multi-core architectures to distributed systems consisting of multiple components deployed on different physical nodes. We collectively refer to such systems as concurrent systems. Concurrent systems are difficult to test, since the faults that derive from their concurrent nature depend on the interleavings of the actions performed by the individual execution flows. Testing techniques that target these faults must take into account the concurrency aspects of the systems. The increasingly rapid spread of parallel and distributed architectures led to a deluge of concurrent software systems, and the explosion of testing techniques for such systems in the last decade. The current lack of a comprehensive classification, analysis and comparison of the many testing techniques for concurrent systems limits the understanding of the strengths and weaknesses of each approach and hampers the future advancements in the field. This survey provides a framework to capture the key features of the available techniques to test concurrent software systems, identifies a set of classification criteria to review and compare the available techniques, and discusses in details their strengths and weaknesses, leading to a thorough assessment of the field and paving the road for future progresses.},
keywords={Testing;Software systems;Message passing;History;Concurrent computing;Computer architecture;Synchronization;Survey;classification;testing;concurrent systems;parallel systems;distributed systems},
doi={10.1109/TSE.2017.2707089},
ISSN={1939-3520},
month={Aug},}
@ARTICLE{7485817,
author={Roth, Thomas and McMillin, Bruce},
journal={IEEE Transactions on Dependable and Secure Computing}, title={Physical Attestation in the Smart Grid for Distributed State Verification},
year={2018},
volume={15},
number={2},
pages={275-288},
abstract={A malicious process in a distributed system can fabricate its internal state in its communications with its peers. These state fabrications can cause other processes in the distributed system to make incorrect control decisions. Smart grid systems have a unique advantage in the detection of falsified state attacks because process control decisions have an observable effect on a shared physical infrastructure. The physical infrastructure acts as a high-integrity message channel that broadcasts changes in individual process states. This work proposes a new distributed security mechanism called physical attestation that combines physical feedback with methods from computer security to detect state fabrications in the smart grid. The theory of physical attestation is proven using an information flow security property called nondeducibility, and supported with experimental results from a simulation test bed.},
keywords={Smart grids;State estimation;Phasor measurement units;Security;Topology;Information security;distributed computing;smart grids;cyber security},
doi={10.1109/TDSC.2016.2577021},
ISSN={1941-0018},
month={March},}
@INPROCEEDINGS{8487560,
author={Murthy, PVR and Ulrich, Andreas},
booktitle={2017 14th IEEE India Council International Conference (INDICON)}, title={Distributed GUI Test Automation},
year={2017},
volume={},
number={},
pages={1-6},
abstract={The paper provides the design of a distributed test system for testing distributed GUI (Graphical User Interface) applications. The distributed GUI application that is considered allows multiple operators or users to interact with a distributed system in parallel. The distributed GUI application allows the use of existing GUI test frameworks for local test execution and offers additional services of coordination and synchronization among the deployed GUI test frameworks, test verdict arbitration, and debugging support, all of which are needed to deal with the concurrent nature of the system under test. In addition, the paper offers a test specification method for test cases that is based on the semantic model of a concurrent event flow graph. This method allows the automatic test case execution on top of the suggested distributed test system. The overall approach allows the extension of testing of single GUI application systems towards testing of distributed GUI application systems by preserving investments in test automation for single GUI testing.},
keywords={Graphical user interfaces;Testing;Servers;Automation;Flow graphs;Concurrent computing;Unified modeling language;Test Automation;Distributed Graphical User Inteifaces;multiple client – single server system;Synchronization},
doi={10.1109/INDICON.2017.8487560},
ISSN={2325-9418},
month={Dec},}
@INPROCEEDINGS{8368364,
author={Xie, Zhuohan and Ding, Wencheng and Wang, Hongya and Xiao, Yingyuan and Liu, Zhenyu},
booktitle={2017 IEEE 23rd International Conference on Parallel and Distributed Systems (ICPADS)}, title={D-Ary Cuckoo Filter: A Space Efficient Data Structure for Set Membership Lookup},
year={2017},
volume={},
number={},
pages={190-197},
abstract={Many networking and distributed systems use Bloom filters and their variants for high speed set membership tests. Such probabilistic techniques provide very good space efficiency at the cost of a small fraction of false positive answers. The original Bloom filters do not permit deletion of items from the set and most attempts to extend Bloom filters to support deletions suffer from either space or time performance degradation. Recently, inspired by Cuckoo Hashing, Fan et. al. proposed a data structure called Cuckoo filter that achieves even better space performance than Bloom filters while supporting dynamic insertion and deletion of items. By allowing that each element has more than two candidate buckets, d-ary Cuckoo Hashing is capable of providing much higher space utilization. Motivated by this study, we generalize Cuckoo filter to d-ary Cuckoo filter for further reduction in space cost. The main difficulty here is that increasing the number of candidate buckets is not as easy as it appears because only fingerprints are available for the calculation of candidate locations. To solve this problem, we introduce the base-d digitwise xor operations as the foundation for computing the d candidate buckets of each element in a cyclic fashion. Theoretical analysis and experiment study show that d-ary Cuckoo filters can save up to one bit for each element at the cost of increased lookup and insertion performance.},
keywords={Standards;Data structures;Testing;Probability;Computer science;Probabilistic logic;Degradation;Data Structures Bloom Filters Cuckoo Filters Membership Tests},
doi={10.1109/ICPADS.2017.00035},
ISSN={1521-9097},
month={Dec},}
@INPROCEEDINGS{8361506,
author={Giliberti, G. and De Grassi, A. and Marchei, D. and Lai, C. and Ragnoni, A.},
booktitle={EVI-GTI Conference on Gas Turbine Instrumentation (GTI 2017)}, title={Applying hardware in the loop to designing, integrating, verifying and validating the control system of new aircraft engines},
year={2017},
volume={},
number={},
pages={1-6},
abstract={A new experimental rig is presented, developed to design, integrate, verify and validate the control system of the all-new advanced turboprop engine, produced by GE Aviation and chosen by Textron Aviation to power its new Cessna Denali airplane. The ATP Wet Rig has been conceived to demonstrate the successful integration of engine components, to perform a functional verification at software and system level and to allow an early validation of the final engine system in a controlled and safe environment, preliminary to flight test activities. A dedicated Engine Simulator has been developed to run the ATP Engine model within a Hard Real Time framework. It provides aircraft's interfaces emulation, controls the load actuators and the electrical motors' speeds, replicates consistent loads on the components in compliance with the engine simulated status, intercepts all the EECU electrical I/O lines through appropriate “break out boxes” and physical test points, required for integration purposes. Finally, a dedicated Rig Control System has been developed to communicate and to control all the equipment present in the Wet Rig. It is a distributed system responsible for supervision, test point management, data acquisition, subsystems control, safety and equipment management, communication with the Engine Simulator.},
keywords={Hardware in the Loop;ATP Program;Wet Rig;Engine Simulation;Validation},
doi={10.1049/cp.2017.0317},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8327126,
author={Feinbube, Lena and Pirl, Lukas and Troger, Peter and Polze, Andreas},
booktitle={2017 18th International Conference on Parallel and Distributed Computing, Applications and Technologies (PDCAT)}, title={Dependability Stress Testing of Cloud Infrastructures},
year={2017},
volume={},
number={},
pages={453-460},
abstract={Modern distributed systems have reached a level of complexity where software bugs and hardware failures are no longer exceptional, but a permanent operational threat. This holds especially for cloud infrastructures, which need to deliver resources to their customers under well-defined service-level agreements. Dependability need to be assessed carefully. This article presents a structured approach for dependability stress testing in a cloud infrastructure. We automatically determine and inject the maximum amount of simultaneous non-fatal errors in different variations. This puts the existing resiliency mechanisms under heavy load, so that they are tested for their effectiveness in corner cases. The starting point is a failure space dependability model of the system. It includes the notion of fault tolerance dependencies, which encode fault-triggering relations between different software layers. From the model, our deterministic algorithm automatically derives fault injection campaigns that maximize dependability stress. The article demonstrates the feasibility of the approach with an assessment of a fault tolerant OpenStack cloud infrastructure deployment.},
keywords={Stress;Software;Testing;Fault tolerance;Fault tolerant systems;Fault trees;Load modeling;fault injection;dependability modeling;testing;dependability stress;fault tolerance;OpenStack;fault tolerance dependency},
doi={10.1109/PDCAT.2017.00078},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8322759,
author={Chen, Xiao and Gao, Wei and Zhou, Lingyu and Chen, Yiou and Ling, Xiang},
booktitle={2017 3rd IEEE International Conference on Computer and Communications (ICCC)}, title={A new schedule algorithm for LTE signal processing},
year={2017},
volume={},
number={},
pages={1333-1337},
abstract={Scheduling applications on multi-processor is an effective core technology in using distributed systems. Task scheduling is to map applications to parallel system in order to minimize the makespan. Task scheduling is classified into static scheduling and dynamic scheduling. Most task scheduling is NP-complete problem. The authors come up with a new scheduling algorithm: Chunking Splicing Algorithm for a special type of DAG like the processing of PDSCH (Physical Downlink Shared Channel) in LTE. When mapping that special type of DAG (like signal processing of PDSCH), its performance is quite good comparing with traditional list scheduling algorithms: HLFET, ISH, ETF, DLS algorithm. Also the authors use some random DAGs to test the performance of our new algorithm.},
keywords={Task analysis;Program processors;Scheduling algorithms;Delays;Heuristic algorithms;Clustering algorithms;scheduling algorithm;multi-processors;chunking splicing algorithm;makespan;PDSCH},
doi={10.1109/CompComm.2017.8322759},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8308381,
author={Long, Anh-Toan Bui and Ouhammou, Yassine and Grolleau, Emmanuel},
booktitle={2017 IEEE/ACS 14th International Conference on Computer Systems and Applications (AICCSA)}, title={Leveraging Real-Time Network Analyses by Extending a Model-Based Framework},
year={2017},
volume={},
number={},
pages={871-878},
abstract={Real-Time Systems are subject to temporal requirements. To check if these latter are met, performance analysis tests are required. However, the performance analysis through model-based process is still difficult due to (i) the complexity of real-time systems (ii) and the lack of methodologies enabling to leverage performance tests to be applied easily to design models.In this paper, we focus on the temporal scheduling analysis of distributed systems with real-time networks. We present an extension of a pivot scheduling analysis aware methodology in order to explicit the analysis models. That is, we propose a modeling viewpoint dedicated to distributed real-time systems in order to bridge the semantic gap between system design and analysis techniques of real-time networks.Our proposed extension has been implemented and integrated in a modeling framework dedicated to the schedulability analysis. The paper also contains a case study that stresses the contribution and shows its usefulness.},
keywords={Analytical models;Real-time systems;Unified modeling language;Tools;Task analysis;Job shop scheduling;Protocols;Real-Time;Real-time Analysis;Network;Model Driven Engineering;Design;Modeling},
doi={10.1109/AICCSA.2017.157},
ISSN={2161-5330},
month={Oct},}
@INPROCEEDINGS{8284295,
author={Gandhi, Tanvi and Nitin and Alam, Taj},
booktitle={2017 Tenth International Conference on Contemporary Computing (IC3)}, title={Quantum genetic algorithm with rotation angle refinement for dependent task scheduling on distributed systems},
year={2017},
volume={},
number={},
pages={1-5},
abstract={Distributed systems are efficient means of realizing High-Performance Computing (HPC). They are used in meeting the demand of executing large-scale high-performance computational jobs. Scheduling the tasks on such computational resources is one of the prime concerns in the heterogeneous distributed systems. Scheduling jobs on such systems are NP-complete in nature. Scheduling requires either heuristic or metaheuristic approach for sub-optimal but acceptable solutions. An application can be divided into a number of tasks which can be represented as Direct Acyclic Graph (DAG). To accomplish high performance, it is important to efficiently schedule these dependent tasks on resources with the satisfaction of the constraints defined for schedule generation. Inspired by Quantum computing, this work proposes a Quantum Genetic Algorithm with Rotation Angle Refinement (QGARAR) for optimum schedule generation. In this paper, the proposed QGARAR is compared with its peers under various test conditions to account for minimization of the makespan value of dependent jobs submitted for execution on heterogeneous distributed systems.},
keywords={Task analysis;Genetic algorithms;Quantum computing;Schedules;Sociology;Statistics;Biological cells;Dependent task scheduling;Genetic Algorithm;Quantum Genetic Algorithm;Topological sort;Rotation angle refinement},
doi={10.1109/IC3.2017.8284295},
ISSN={2572-6129},
month={Aug},}
@INPROCEEDINGS{8273834,
author={Hambridge, Sarah and Lu, Ning and Huang, Alex Q. and Yu, Ruiyang},
booktitle={2017 IEEE Power Energy Society General Meeting}, title={A frequency based real-time electricity rate for residential prosumers},
year={2017},
volume={},
number={},
pages={1-5},
abstract={The emergence of distributed generation has made a case for a deregulated, competitive, transactive energy market, operating at the level of the traditional residential consumer. These new energy players, prosumers, will interact as the larger energy generators do under the supervision of Independent System Operators (ISOs), but with their own Distributed System Operators (DSOs). This work proposes a prosumer energy management scheme, broken into a day-ahead schedule and a realtime adjustment, mirroring the ISO market structure. Within this framework, a dynamic rate can be designed and tested for the prosumer. A time-of-use (TOU) rate was combined with a frequency based, real-time dynamic rate to produce a hybrid rate that the prosumer can optimize for during its day-ahead and real-time dispatch. This hybrid rate can be calculated every one minute and applied autonomously from the grid frequency, providing secondary frequency regulation and an incentive for better solar management and use of energy storage. Such a real-time rate is the first designed for price-reactive control. In simulation, the real-time hybrid rate is compared to conventional TOU and flat rates and the final daily energy costs are calculated for a variety of residential load types with a realistic distributed solar generation curve gathered from Pecan Street Inc. Dataport. Under the one minute hybrid rate, the results indicate a near zero energy bill can be achieved for a prosumer with day-time load and smart use of energy storage.},
keywords={Real-time systems;Energy storage;Frequency measurement;Linear programming;Frequency control;Partial discharges;Schedules;Economic Dispatch;Microgrid;Autonomous;Distributed;Transactive Control;Energy Router;Energy Cell},
doi={10.1109/PESGM.2017.8273834},
ISSN={1944-9933},
month={July},}
@INPROCEEDINGS{8263923,
author={Schug, Ann-Kathrin and Werner, Herbert},
booktitle={2017 IEEE 56th Annual Conference on Decision and Control (CDC)}, title={Active vibration control of an aluminum beam — An experimental testbed for distributed vs. centralized control},
year={2017},
volume={},
number={},
pages={1876-1881},
abstract={This paper presents an experimental test bench for assessing the practicality of distributed control strategies in the framework of spatially-interconnected systems. To compare low-complexity distributed control schemes with multi-input multioutput (MIMO) control and to assess the results experimentally requires a test bed that is sufficiently complex to display on one hand the characteristic features of a distributed system, and yet is not too complex in order not to render the MIMO synthesis problem intractable. Here a 5m long aluminum beam, equipped with 16 collocated pairs of piezoelectric actuators and sensors, is used for vibration control experiments. A MIMO model obtained by finite element modeling techniques is presented and a distributed model in the spatially-interconnected systems framework is derived. Model reduction techniques are applied to the MIMO model and a centralized controller is designed to attenuate the vibration using H∞-loopshaping techniques. Finally, the performance of the controller is experimentally validated on the beam.},
keywords={},
doi={10.1109/CDC.2017.8263923},
ISSN={},
month={Dec},}
@INPROCEEDINGS{8261126,
author={Mwila, Martin K. and Mbewe, Perseverance},
booktitle={2017 Pattern Recognition Association of South Africa and Robotics and Mechatronics (PRASA-RobMech)}, title={Design and implementation of a Node.js based communication framework for an unmanned autonomous ground vehicle},
year={2017},
volume={},
number={},
pages={74-79},
abstract={This paper describes the methodology used to develop a communication platform that enhances interoperability between different types of components irrespective of the manufacturers and of the software platform. This framework is intended to be used on a distributed system where software and hardware modules are designed to control an autonomous Unmanned Ground Vehicle (UGV). A messaging architecture based on the Joint Architecture for Unmanned Systems (JAUS) was developed in Node.js to ensure platform independence. It was deployed on hardware platforms such as the Raspberry Pi and the BeagleBone Black in order to access various sensors on the platform and control hardware like stepper motor. This messaging architecture can also be implemented on conventional laptops running Windows operating system or Linux that run algorithms for localisation, terrain mapping and path planning. Initially regarded as a very limited language, JavaScript's true nature and power have only recently been appreciated in depth, A major move is now underway to apply this language in new and fascinating contexts. The ultimate goal of the framework was to structure communication and inter-operation of UGV components and a sensor suite within a network. The framework was implemented on the G-Bat, a UGV platform developed at CSIR DPSS Landward Sciences to test and simulate the communication part of an autonomous navigation system. The test was a successful step that paves the way for a more robust implementation of the framework in the future work.},
keywords={Software;Hardware;Computer architecture;Browsers;C++ languages;Servers;Unmanned Ground Vehicle;Communication Framework;Socket.IO;Node.js;Asynchronous Waterfall Model;Joint Architecture for Unmanned Systems},
doi={10.1109/RoboMech.2017.8261126},
ISSN={},
month={Nov},}
@INPROCEEDINGS{8226470,
author={Kidanu, Solomon Asres and Chbeir, Richard and Cardinale, Yudith},
booktitle={2017 XLIII Latin American Computer Conference (CLEI)}, title={MAS2DES-onto: Ontology for MAS-based digital ecosystems},
year={2017},
volume={},
number={},
pages={1-8},
abstract={Multi-Agent Systems (MASs) have received much attention in recent years because of their advantages on modeling complex distributed systems, such Digital Ecosystems (DESs). Many existing modeling languages that support the design of such systems are based on ontologies to assist the representation of agents knowledge. However, in the context of DESs, there is still a need for more general conceptual models to represent the specific characteristics of DESs in terms of win-win interaction, engagement, equilibrium, and self-organization. Then, concepts such behavior, roles, rules, and environment are needed. This paper describes an ontology-based approach by proposing MAS2DES-Onto, as the conceptual model, which considers the essential static and dynamic aspects of MASs by a clear representation of their concepts and relationships to support the design and development of DESs. To validate and conduct experimental tests, we integrate MAS2DES-Onto into a framework to automatically generate MAS-based DESs. Results show the efficiency and effectiveness of our approach.},
keywords={Ontologies;Cognition;Ecosystems;Biological system modeling;Organizations;Electronic mail},
doi={10.1109/CLEI.2017.8226470},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8229627,
author={Pruna, Edwin and Mullo, Santiago D. and Caicedo, Jhonathan A. and Zambrano, Xavier and Escobar, Ivón and Gordon, Andrés and Constante, Patricia},
booktitle={2017 CHILEAN Conference on Electrical, Electronics Engineering, Information and Communication Technologies (CHILECON)}, title={Distributed system for the monitoring and control of processes},
year={2017},
volume={},
number={},
pages={1-5},
abstract={A distributed system is presented for the monitoring and control of the primary variables: pressure, level and flow; for this, a wireless system is implemented at the level of sensor-actuators and at the level of controllers an Ethernet / IP network, the mentioned industrial networks are implemented based on the OSI model and TCP / IP respectively, for the visualization an HMI is realized in the software LabVIEW, finally performance tests are performed and the results presented allow to validate the operation of the distributed system.},
keywords={Monitoring;IP networks;Wireless LAN;Wireless communication;Logic gates;Visualization;Wireless sensor networks;industrial networks;ethernet I/P;processes control;distributed control system;HMI (human machine interface)},
doi={10.1109/CHILECON.2017.8229627},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8226222,
author={Mishra, Sambit Kumar and Khan, Md Akram and Sahoo, Bibhudatta and Jena, Sanjay Kumar},
booktitle={2017 2nd International Conference for Convergence in Technology (I2CT)}, title={Time efficient task allocation in cloud computing environment},
year={2017},
volume={},
number={},
pages={715-720},
abstract={Cloud computing is an evolution of Distributed system that has been adopted by worldwide scientifically and commercially. For optimal use of cloud's potential power, effective and efficient algorithm are required, which will select best resources from available cloud resources for different applications. This allocation of user requests to the cloud resource can optimize various parameters like energy consumption, makespan, throughput, etc. This task allocation or mapping problem is a well-known NP-Complete problem. In this paper, we have proposed an algorithm, Task Based allocation to minimize the makespan of the cloud system and also to increase the resource utilization. We have simulated our algorithm, TBA in CloudSim Simulator in a heterogeneous environment. CloudSim is one of the simulation tools of cloud environment which provides evaluation and testing of cloud services and infrastructure before the development of the real world. During the comparison of the algorithm, we provide the sorted tasks to the TBA algorithm once and un-sorted tasks in the second time. We have compared sorted-TBA, unsorted-TBA and random algorithm where the sorted-TBA algorithm performs better.},
keywords={Cloud computing;Resource management;Computational modeling;Virtual machining;Heuristic algorithms;Scheduling;Cloud Computing;Makespan;PM;Task Scheduling;TBA algorithm;VM},
doi={10.1109/I2CT.2017.8226222},
ISSN={},
month={April},}
@INPROCEEDINGS{8170697,
author={Epure, Silviu and Vlad, Ciprian and Păduraru, Romeo},
booktitle={2017 5th International Symposium on Electrical and Electronics Engineering (ISEEE)}, title={Hardware configuration of DC-DC converter for renewable energies conversion},
year={2017},
volume={},
number={},
pages={1-6},
abstract={A DC/DC converter that can be used to charge a lead-acid battery bank from a wide variety of input renewable sources is presented. The motherboard of the converter can be populated in such way that a boost or a SEPIC (Single-Ended Primary-Inductor Converter) structure is obtained. In order to lower the stress on the electronic components, the converter uses four DC/DC stages connected in parallel and driven with interleaved PWM signals. The main advantage resides in the fact that all parameters of the control loops as well as the nominal values for input and output electrical values can be adjusted by the final user. Two converters were built and tested simultaneously inside a low power distributed system, implementing MPPT algorithm. Key aspects of the low level hardware implementation and component dimensioning will be detailed.},
keywords={Voltage control;Pulse width modulation;Batteries;Snubbers;Transistors;Hardware;Wind turbines;DC/DC converter;SEPIC;boost;PCB;signal processor},
doi={10.1109/ISEEE.2017.8170697},
ISSN={},
month={Oct},}
@INPROCEEDINGS{8166387,
author={Plugariu, Ovidiu and Gegiu, Alexandru Dumitru and Petrica, Lucian},
booktitle={2017 9th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)}, title={FPGA systolic array GZIP compressor},
year={2017},
volume={},
number={},
pages={1-6},
abstract={In this paper we present a complete, open-source GZIP compressor implementation for FPGA based on a systolic array architecture. GZIP is one of the most utilized compression algorithms. Besides the usual use-case of compression for data storage, distributed computing systems such as Hadoop utilize compression to reduce the amount of data which is transferred between computing nodes in a cluster. However, compression with GZIP requires significant amounts of CPU processing power, negating some of the advantages of the compressed-transfer approach in distributed systems. We have designed, implemented and tested a hardware architecture and software application for compressing files using a hardware GZIP compressor. The system presented in this paper offloads GZIP compression from the host CPU to one or more systolic GZIP compression cores in FPGA, thereby reducing latency caused by compression and freeing up the CPU for other computing tasks. We implemented and evaluated a single GZIP compression core in a ML605 development board, equipped with a Xilinx Virtex 6 FPGA, utilizing Xillybus for data transfers over PCI Express. Our results indicate the peak compression throughput of our implementation is over 1.3 Gbps and an average throughput of 52 Mbps on the Calgary corpus. Our FPGA compression solution is at least twice as fast as software compression on an Intel Core i7, in all evaluated scenarios, and up to 18× faster for large files. The project source code is publicly available online1.},
keywords={Field programmable gate arrays;Hardware;Computer architecture;Dictionaries;Software;Throughput;Heuristic algorithms;GZIP compression;LZ77 encoder;DEFLATE;FPGA;Xillybus},
doi={10.1109/ECAI.2017.8166387},
ISSN={},
month={June},}
@INPROCEEDINGS{8167280,
author={Zhao, Li and Liu, Zhe and Liu, Wei and He, Huihong and Wang, Yong and Wang, Ze},
booktitle={2017 2nd IEEE International Conference on Computational Intelligence and Applications (ICCIA)}, title={G-FDDS: A graph-based fault diagnosis in distributed systems},
year={2017},
volume={},
number={},
pages={559-567},
abstract={With the rapid development of Internet technologies such as cloud computing and big data, the scales of distributed information systems in big companies have grown to enormous sizes. Automatic detection and diagnosis of system faults in the large-scale information systems is complicated and important in both practice and research. In this paper, we propose a Graph-based Fault Diagnosis approach in Distributed System (G-FDDS), in which availability testing records of distributed systems are represented by a multi-relational graph in order to find out latent root causes of the failures of availability testing. Vertices represent non-repeated failed availability testing records and edges indicate multiple relations between these records, then a clustering method is proposed to group similar vertices into fault causes. Our approach is more appropriate for mining accurate fault causes both in the simulation and real datasets comparing with other commonly used methods such as statistical methods.},
keywords={Testing;Fault diagnosis;Monitoring;Information systems;Servers;Statistical analysis;Distributed databases;fault diagnosis;availability test;distributed system;multi-relational graph;visualization},
doi={10.1109/CIAPP.2017.8167280},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8125975,
author={Sahoo, Sobhana and Ray, Abhishek},
booktitle={2017 International Conference on Advances in Computing, Communications and Informatics (ICACCI)}, title={A framework for optimization of regression testing of web services using slicing},
year={2017},
volume={},
number={},
pages={1017-1022},
abstract={In recent years, web services have become a prominent paradigm for distributed systems and electronic services. Web service provides a framework for application-to-application interaction based on existing standard web protocols and XML technology. The enhanced features of web service such as interoperability, dynamic discovery and composition brings new challenges in testing of web services. In this paper, we propose a framework for optimization of web service regression testing with respect to different service evolution scenarios. First, we have identified the changes using a change detection algorithm on the generated operation tree of both original and modified WSDL files. Next, a dynamic forward slicing algorithm is used to select only the modified and the affected parts of the web service. Further, we have used an optimized regression test algorithm which gives us optimized test cases.},
keywords={Web services;Testing;Heuristic algorithms;Change detection algorithms;Calculators;Detection algorithms;Tin;Web Service;Regression Testing;WSDL;Slicing},
doi={10.1109/ICACCI.2017.8125975},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8104808,
author={Veichtlbauer, Armin and Ortmayer, Martin and Heistracher, Thomas},
booktitle={2017 IEEE 15th International Conference on Industrial Informatics (INDIN)}, title={OPC UA integration for field devices},
year={2017},
volume={},
number={},
pages={419-424},
abstract={In industrial automation systems, deploying the well-established automation pyramid model is best practice. However, the trend to massively distributed systems, which are foreseen to co-operate using standardized protocols and common semantics, shows the limits of these traditional approaches. In order to enable for Industry 4.0 compliant solutions, appropriate means for scalable internetworking have to be developed and utilized. New modelling technologies have been developed to represent such distributed automation systems, incorporating a multidimensional layered approach. At higher hierarchy levels of these automation models, standardization approaches are quite common. However, at the field layer there are still many different field busses, which in most cases do not allow common semantics, but come along with their own object models. In contrast, the use of OPC UA at the field level, with its standardized protocol stack and semantic annotations, would allow for enabling field devives to fully participate in large scaled systems as Industry 4.0 components. This paper evaluates the potentials and limits of integrating OPC UA into legacy field devices with limited communication and calculation resources and provides quantitative measurement results of selected test scenarios.},
keywords={Automation;Semantics;Real-time systems;Industries;Prototypes;Data models;Protocols},
doi={10.1109/INDIN.2017.8104808},
ISSN={2378-363X},
month={July},}
@INPROCEEDINGS{8102089,
author={Theunissen, E. and Kotegawa, T.},
booktitle={2017 IEEE/AIAA 36th Digital Avionics Systems Conference (DASC)}, title={Applying LVC to testing and evaluation of DAA systems},
year={2017},
volume={},
number={},
pages={1-7},
abstract={Testing of Detect and Avoid (DAA) systems ranges from the use of pre-defined inputs, through part-task and full mission simulations up to flight testing. Live, Virtual and Constructive (LVC) data-sources yield the same observation to a client of this data and provide the opportunity to achieve a seamless transition between the various phases of testing. The interface of a system under test is likely to change during early development stages and thus the design of the LVC environment must ensure that such changes do not propagate through the LVC environment in such a way that it unnecessarily affects other components. Modularity is the key enabler for separating functionalities that have different design-evaluation cycle times. The current state-of-the-art in software development for real-time distributed systems enables a modular approach using industry standard middleware. This paper discusses how such an LVC environment has been realized and provides an overview of its use in several flight tests of DAA systems.},
keywords={Testing;Computer architecture;Protocols;Standards;Middleware;Aircraft;Atmospheric modeling;DAA;UAS;LVC},
doi={10.1109/DASC.2017.8102089},
ISSN={2155-7209},
month={Sep.},}
@INPROCEEDINGS{8102604,
author={El Hassan Charaf, My and Azzouzi, Salma},
booktitle={2017 4th International Conference on Control, Decision and Information Technologies (CoDIT)}, title={A colored Petri-net model for control execution of distributed systems},
year={2017},
volume={},
number={},
pages={0277-0282},
abstract={A crucial part of the development of distributed systems process is the test phase. Indeed, in the distributed testing context, the use of multiple testers introduces the possibility of coordination problems amongst remote testers. These potential problems are known as controllability and observability fault detections which are fundamental features of conformance in distributed testing. The paper presents some technical issues for testing such frameworks using rules based System. The proposed approach consists on exploring how a colored Petri net model used in distributed testing prototype realization contribute to design the communication between different components of the distributed test application and by the way capture the complex monitoring tasks of the distributed testers.},
keywords={Testing;Observability;Controllability;Ports (Computers);Synchronization;Prototypes;Distributed test;Rules;Controllability;Observability;Control;CPN Tools},
doi={10.1109/CoDIT.2017.8102604},
ISSN={},
month={April},}
@INPROCEEDINGS{8102448,
author={Efozia, N. F. and Ariwa, E. and Asogwa, D. C. and Awonusi, O. and Anigbogu, S. O.},
booktitle={2017 Seventh International Conference on Innovative Computing Technology (INTECH)}, title={A review of threats and vulnerabilities to cloud computing existence},
year={2017},
volume={},
number={},
pages={197-204},
abstract={The idea that computation may be organized as a public utility, like water and electricity, was formulated in the 1960s by John McCarthy, a visionary computer scientist that championed mathematical logic in artificial intelligence. Cloud computing combines the best techniques and technologies of distributed system, parallel computing and grid computing. Is a utility computing that provides a scalable standard environment for network-centric application development, testing and deployment that distributes and allocates resources via simple user, provider model pattern of pay-per-use system. This paper aims at bringing to light some of the threats and vulnerabilities to cloud computing existence, with significance of enlighten users and providers on what is at stake on moving their business or organisation whole or partially to the cloud.},
keywords={Cloud computing;Software as a service;Computational modeling;Security;Business;Cloud Computing;SaaS;IaaS;PaaS;Private Cloud;Public Cloud;Threats},
doi={10.1109/INTECH.2017.8102448},
ISSN={},
month={Aug},}
@INPROCEEDINGS{8095175,
author={Krejčí, Lukáš and Novák, Jiří},
booktitle={2017 9th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)}, title={Model-based testing of automotive distributed systems with automated prioritization},
year={2017},
volume={2},
number={},
pages={668-673},
abstract={The paper presents a framework for model-based testing of automotive distributed system and a method of automatic assignment of testing priorities used within the framework. The proposed method utilizes classifiers for automatic assignment of testing priorities to specific parts of the tested system. The paper also introduces a set of extraneous data accompanying the modeling language that are exploited by the proposed method during the classification process. It is shown, that advantages of the presented approach, such as lower requirements for the testing operators' knowledge, are valuable for the automotive distributed systems testing process.},
keywords={Testing;Automotive engineering;Data models;Complexity theory;Safety;Automata;Manuals;model-based;testing;timed automata;classification;Support Vector Machine},
doi={10.1109/IDAACS.2017.8095175},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8081077,
author={Tagore, Ashutosh Kumar and Gupta, Atma Ram},
booktitle={2017 International Conference on Power and Embedded Drive Control (ICPEDC)}, title={Harmonic load flow analysis of radial distribution system in presence of distributed generation},
year={2017},
volume={},
number={},
pages={147-151},
abstract={The optimal DG allocation and sizing which results in the reduction of THD and the system losses using the backward and the forward sweep method is shown in this paper. The optimal location of the DG is obtained from various sensitivity indices. The size of DG is determined by the minimum loss criterion. BIBC and BCBV matrices are calculated to perform load flow analysis and harmonic flow analysis in distributed system. The comparative result for the harmonic flow analysis with and without DG placement is described in this paper. The proposed method is tested on IEEE 33 bus radial system.},
keywords={Harmonic analysis;Power system harmonics;Capacitors;Load flow analysis;Power system stability;Mathematical model;Distributed generation;load flow;harmonic load flow;optimal location;radial distribution system},
doi={10.1109/ICPEDC.2017.8081077},
ISSN={},
month={March},}
@INPROCEEDINGS{8069071,
author={El Mhamdi, El Mahdi and Guerraoui, Rachid and Rouault, Sébastien},
booktitle={2017 IEEE 36th Symposium on Reliable Distributed Systems (SRDS)}, title={On the Robustness of a Neural Network},
year={2017},
volume={},
number={},
pages={84-93},
abstract={With the development of neural networks based machine learning and their usage in mission critical applications, voices are rising against the black box aspect of neural networks as it becomes crucial to understand their limits and capabilities. With the rise of neuromorphic hardware, it is even more critical to understand how a neural network, as a distributed system, tolerates the failures of its computing nodes, neurons, and its communication channels, synapses. Experimentally assessing the robustness of neural networks involves the quixotic venture of testing all the possible failures, on all the possible inputs, which ultimately hits a combinatorial explosion for the first, and the impossibility to gather all the possible inputs for the second.In this paper, we prove an upper bound on the expected error of the output when a subset of neurons crashes. This bound involves dependencies on the network parameters that can be seen as being too pessimistic in the average case. It involves a polynomial dependency on the Lipschitz coefficient of the neurons' activation function, and an exponential dependency on the depth of the layer where a failure occurs. We back up our theoretical results with experiments illustrating the extent to which our prediction matches the dependencies between the network parameters and robustness. Our results show that the robustness of neural networks to the average crash can be estimated without the need to neither test the network on all failure configurations, nor access the training set used to train the network, both of which are practically impossible requirements.},
keywords={Neurons;Biological neural networks;Robustness;Computer crashes;Hardware;Neuromorphics;Computational modeling;Neural Networks;Neuromorphic computing;Fault Tolerance;Robustness;Machine Learning;Distributed Systems;Adversarial Machine Learning},
doi={10.1109/SRDS.2017.21},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8065917,
author={Martino, Danilo and Shen, Yun and Paracchini, Marco and Marcon, Marco and Plebani, Emanuele and Pau, Danilo Pietro},
booktitle={2017 IEEE 3rd International Forum on Research and Technologies for Society and Industry (RTSI)}, title={Accurate cyber-physical system simulation for distributed visual search applications},
year={2017},
volume={},
number={},
pages={1-5},
abstract={A Cyber-Physical System (CPS) is defined as a, usually distributed, system that links the digital (cyber) and physical world. They feature different computational cores and heterogeneous sensors linked through networks of different types allowing a deeper interaction with the physical world, collecting, storing and exchanging information intelligently. In this work, an open source CPS simulator called COSSIM is described and a smart mechanism is proposed in order to turn it into a co-simulator. In addition to this, a CPS based on a Computer Vision application, called Mobile Visual Search (MVS), is described and ported to COSSIM in order to test the correctness of the simulation and to prove the benefit of the proposed acceleration. Quantitative and qualitative precision results in both real and simulated scenarios are also presented.},
keywords={Visualization;Databases;Computational modeling;Servers;Buildings;Cyber-physical systems;Mobile communication;Cyber-Physical Systems;COSSIM;Embedded devices;Visual Search},
doi={10.1109/RTSI.2017.8065917},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{8029668,
author={Roth, Thomas and McMillin, Bruce M.},
booktitle={2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)}, title={Physical Attestation in the Smart Grid for Distributed State Verification},
year={2017},
volume={1},
number={},
pages={626-627},
abstract={A malicious process in a distributed system can fabricate its internal state in its communications with its peers. These state fabrications can cause other processes in the distributed system to make incorrect control decisions. Smart grid systems have a unique advantage in the detection of falsified state attacks because process control decisions have an observable effect on a shared physical infrastructure. The physical infrastructure acts as a high-integrity message channel that broadcasts changes in individual process states. This work proposes a new distributed security mechanism called physical attestation that combines physical feedback with methods from computer security to detect state fabrications in the smart grid. The theory of physical attestation is proven using an information flow security property called nondeducibility, and supported with experimental results from a simulation test bed.},
keywords={Security;Smart grids;Topology;State estimation;Distributed databases;information security;distributed computing;smart grids;cyber security},
doi={10.1109/COMPSAC.2017.188},
ISSN={0730-3157},
month={July},}
@ARTICLE{8022876,
author={Rymarczyk, Tomasz and Tchórzewski, Paweł and Adamkiewicz, Przemysław and Duda, Karol and Szumowski, Jakub and Sikora, Jan},
journal={IEEE Sensors Journal}, title={Practical Implementation of Electrical Tomography in a Distributed System to Examine the Condition of Objects},
year={2017},
volume={17},
number={24},
pages={8166-8186},
abstract={This paper presents a nondestructive method to examine the condition of objects, such as brick wall, flood embankments, landfills, air brick, and similar constructions. We used a setup made of specially built a distributed system with measuring devices to determine the moisture level of flood embankments and to test brick walls. It is an innovative solution for the evaluation buildings and tanks, both in terms of the measuring method the reconstruction algorithm. The application of modern tomographic techniques in conjunction with topological algorithms allows us to perform a noninvasive and very accurate spatial assessment of the dampness level. Electrical tomography under various names includes many techniques for tomographic imaging of the electrical parameters of an object placed in an examination area. We propose a new hybrid solution utilizing imaging techniques together with surface electrodes. Although many methods of evaluating dampness and damage exist, there is no universal solution that is optimal under a wide range of measurement conditions. Our proposed solution achieves this using the topology method for optimization. The smart tomographic measurement systems used in our solution were developed by authors belonging to the Netrix Research and Development Laboratory. These systems included measuring devices, distributed systems, algorithms, and applications for image reconstruction. Several types of reconstruction algorithms and models are explored in this paper. The solution of this optimization problem is obtained by combining the finite element method and topological algorithms. Reconstruction of 2-D examples using numerical and experimental data is shown. The proposed tomographic system consists of a central processing unit, a set of sensors (devices) and a software solution that leverages cloud computing and a big data cluster for processing, visualizing, and analyzing data (a cyber-physical system).},
keywords={Electrical capacitance tomography;Reconstruction algorithms;Capacitance measurement;Piecewise linear approximation;Image reconstruction;Electrical impedance tomography;finite element method;inverse problem;level set method;distributed system},
doi={10.1109/JSEN.2017.2746748},
ISSN={1558-1748},
month={Dec},}
@ARTICLE{8013156,
author={Safayet, Ali and Fajri, Poria and Husain, Iqbal},
journal={IEEE Transactions on Industry Applications}, title={Reactive Power Management for Overvoltage Prevention at High PV Penetration in a Low-Voltage Distribution System},
year={2017},
volume={53},
number={6},
pages={5786-5794},
abstract={A new approach for reactive power management with volt-var control, but considering inverters' capacity and sensitivity to the critical bus is presented in this paper. The approach addresses the voltage rise and reverse power flow issues when residential renewable energy sources such as rooftop solar panels produce more energy than the local load demand. The overvoltage is controlled by selective var injection based on the inverter location, capacity, and minimum power factor limit set by regulation. This method improves the voltage regulation of a distributed system with high penetration of renewable energy sources while efficiently utilizing the inverters' reactive power capacity. Simulation results are presented with a ten inverter network supplied by a 60 kVA distribution transformer. Experimental results are presented to validate the effectiveness of this method for overvoltage prevention of a distribution test system with three photovoltaic (PV) inverters.},
keywords={Reactive power;Inverters;Voltage control;Sensitivity;Load flow;Production;Capacitors;Distribution network;overvoltage;reactive power management;reverse power flow;solar panels;voltage regulation},
doi={10.1109/TIA.2017.2741925},
ISSN={1939-9367},
month={Nov},}
@INPROCEEDINGS{8009869,
author={Zhu, Hongyan and Sun, Ruilin},
booktitle={2017 20th International Conference on Information Fusion (Fusion)}, title={Joint detection and estimation fusion in the presence of correlated sensor quantized data},
year={2017},
volume={},
number={},
pages={1-6},
abstract={This paper addresses the problem of joint detection and estimation fusion when sensor quantized data are correlated in the distributed system. The traditional methods to handle this joint problem tend to treat the detection and estimation tasks separately, which put more emphasis on the detection part but treat the estimation part sub-optimally. In this work, the joint detection and estimation fusion model is described based on the idea of multi-objective optimization, providing attainable flexibility between the detection and estimation performance. There into, the critical joint likelihood function for multi-sensor correlated data is evaluated based on the Copula theory. Simulation results show that the copula-based joint model outperforms the dependency-ignoring model. Further, the proposed approach is superior to the comparative GLRT (Generalized likelihood ratio test) and NP (Neyman-Pearson) in term of the average estimation cost.},
keywords={Estimation;Optimization;Correlation;Distributed databases;Bayes methods;Simulation;Noise measurement;joint detection and estimation;correlated quantized data;copula density;Neyman-Pearson;GLRT},
doi={10.23919/ICIF.2017.8009869},
ISSN={},
month={July},}
@INPROCEEDINGS{8005981,
author={Bhave, Siddharth and Tolentino, Matt and Zhu, Henry and Sheng, Jie},
booktitle={2017 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC)}, title={Embedded Middleware for Distributed Raspberry Pi Device to Enable Big Data Applications},
year={2017},
volume={2},
number={},
pages={103-108},
abstract={Applications making use of embedded systems are anticipated to become extremely important as we advance towards realizing the vision of "Internet of Things" with smart devices such as Raspberry Pi, and compute-anywhere paradigm where principles of distributed systems play pivotal roles. A case we envision here is a distributed network of low powered devices to accomplish various tasks autonomously. Driven by a distributed embedded system architecture, each of the devices can work on independent local data, which is device specific, to perform similar compute tasks simultaneously so a common goal can be achieved. This collaborative problem solving in the embedded setting is similar in concept to the big data paradigm now commonly proposed for commodity hardware and large databases. As the embedded devices become more capable and powerful the two concepts will combine. However, they are currently worlds apart, and thus forms the motivation of our research. In this project, a middleware layer is developed and tested to make the devices work collaboratively on local data within a network of Raspberry Pi devices. The middleware layer splits, distributes, computes and merges the computing tasks to accomplish a shared computing goal while performing the operations locally in a "shared nothing" architecture.},
keywords={Middleware;Embedded systems;Universal Serial Bus;Computer architecture;Big Data;Servers;Linux;Raspberry Pi;Embedded Middleware},
doi={10.1109/CSE-EUC.2017.204},
ISSN={},
month={July},}
@INPROCEEDINGS{7986161,
author={Konda, Krishna Reddy and Tefera, Yonas Teodros and Conci, Nicola and De Natale, Francesco G. B.},
booktitle={2017 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)}, title={Real-time moving object detection and segmentation in H.264 video streams},
year={2017},
volume={},
number={},
pages={1-6},
abstract={In this paper we present a novel algorithm for moving object detection and segmentation, operating on H.264 bit streams. Compared to more traditional pixel-based approaches, the novelty of the algorithm consists of directly using the motion features embedded into the H.264 bit stream, thereby achieving real time operational capability. This makes the algorithm ready to be installed in any video surveillance system, enabling for better resource allocation and facilitating the deployment of distributed systems. The method we propose measures the statistical disorder of the motion field at the boundary of the moving objects, achieving at the same time detection and segmentation. In order to refine the segmentation, results, the temporal correlation of motion vectors is analyzed. The algorithm has been tested on the traditional videos used to benchmark video compression algorithms, as well as on a subset of sequences from the iLids dataset, to demonstrate its generalization capabilities.},
keywords={Motion segmentation;Cameras;Discrete cosine transforms;Correlation;Object detection;Streaming media;Motion measurement;H.264;motion detection;segmentation;compressed domain},
doi={10.1109/BMSB.2017.7986161},
ISSN={2155-5052},
month={June},}
@INPROCEEDINGS{7983226,
author={Festa, Dario and Maggiorini, Dario and Ripamonti, Laura Anna and Bujari, Armir},
booktitle={2017 14th IEEE Annual Consumer Communications Networking Conference (CCNC)}, title={Supporting distributed real-time debugging in online games},
year={2017},
volume={},
number={},
pages={737-740},
abstract={In these last few years we are witnessing a tremendous change in the way video games are developed. On the one hand, large development teams with a multi-layered organisation are employed. On the other hand, we see an increasing request for online services and functionalities. Combining these two trends together usually results in large projects involving parallel and distributed systems. Despite the adoption of team-oriented source code and asset repository managers, code testing and debugging is still left to human direct management. In particular, distributed debugging is a complex problem due to the synchronisation required between network nodes to correctly reconstruct the sequence of events leading to a malfunctioning feature. To solve the aforementioned problem, we designed and implemented DREAD: an architecture to support distributed debugging in real-time games. When adopting our architecture, developers will record events on edge systems, collect them on a centralised sync, and then consolidate everything in a synchronous way. Following this approach, it will be easily possible to detect unexpected software behaviours and trace back to their causes.},
keywords={Monitoring;Debugging;Games;Computer architecture;Software;Servers;Sockets},
doi={10.1109/CCNC.2017.7983226},
ISSN={2331-9860},
month={Jan},}
@INPROCEEDINGS{7967193,
author={El Mhamdi, El Mahdi and Guerraoui, Rachid},
booktitle={2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, title={When Neurons Fail},
year={2017},
volume={},
number={},
pages={1028-1037},
abstract={Neural networks have been traditionally considered robust in the sense that their precision degrades gracefully with the failure of neurons and can be compensated by additional learning phases. Nevertheless, critical applications for which neural networks are now appealing solutions, cannot afford any additional learning at run-time. In this paper, we view a multilayer neural network as a distributed system of which neurons can fail independently, and we evaluate its robustness in the absence of any (recovery) learning phase. We give tight bounds on the number of neurons that can fail without harming the result of a computation. To determine our bounds, we leverage the fact that neural activation functions are Lipschitz-continuous. Our bound is given in the form of quantity, we call the Forward Error Propagation, computing this quantity only requires looking at the topology of the network, while experimentally assessing the robustness of a network requires the costly experiment of looking at all the possible inputs and testing all the possible configurations of the network corresponding to different failure situations, facing a discouraging combinatorial explosion. We distinguish the case of neurons that can fail and stop their activity (crashed neurons) from the case of neurons that can fail by transmitting arbitrary values (Byzantine neurons). In the crash case, our bound involves the number of neurons per layer, the Lipschitz constant of the neural activation function, the number of failing neurons, the synaptic weights and the depth of the layer where the failure occurred. In the case of Byzantine failures, our bound involves, in addition, the synaptic transmission capacity. Interestingly, as we show in the paper, our bound can easily be extended to the case where synapses can fail. We present three applications of our results. The first is a quantification of the effect of memory cost reduction on the accuracy of a neural network. The second is a quantification of the amount of information any neuron needs from its preceding layer, enabling thereby a boosting scheme that prevents neurons from waiting for unnecessary signals. Our third application is a quantification of the trade-off between neural networks robustness and learning cost.},
keywords={Neurons;Biological neural networks;Robustness;Computer crashes;Mathematical model;Nonhomogeneous media;Computational modeling;Neural Networks;Neuromorphic computing;Byzantine Fault Tolerance;Robustness;Machine Learning;Distributed Systems},
doi={10.1109/IPDPS.2017.66},
ISSN={1530-2075},
month={May},}
@INPROCEEDINGS{7965437,
author={Alnawasreh, Khaled and Pelliccione, Patrizio and Hao, Zhenxiao and Rånge, Mårten and Bertolino, Antonia},
booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track (ICSE-SEIP)}, title={Online Robustness Testing of Distributed Embedded Systems: An Industrial Approach},
year={2017},
volume={},
number={},
pages={133-142},
abstract={Having robust systems that behave properly even in presence of faults is becoming increasingly important. This is the case of the system we investigate in this paper, which is an embedded distributed system consisting of components that communicate with each other via messages exchange in the RBS (Radio Based Station) at Ericsson AB in Gothenburg, Sweden. Specifically, this paper describes a novel fault injection approach for testing the robustness of distributed embedded systems with very limited computation power. The new approach is inspired by Netflix's ChaosMonkey, a fault injection approach that has been developed for testing distributed systems hosted in the cloud. However, ChaosMonkey cannot be used in the context of RBS since the latter consists of small-embedded components with specific requirements of performance, programming language, and communication paradigm. This paper reports about the approach called Postmonkey we developed, illustrates the results of applying it to RBS, and discusses the potential of utilizing fault injection to test complex, embedded, and distributed systems. The approach and tool are now adopted by Ericsson.},
keywords={Testing;Robustness;Embedded systems;Collaboration;Tools;Chaos;online testing;fault injection;distributed embedded systems},
doi={10.1109/ICSE-SEIP.2017.17},
ISSN={},
month={May},}
@ARTICLE{7932705,
author={Vizcarrondo, Juan and Aguilar, Jose and Exposito, Ernesto and Subias, Audine},
journal={IEEE Latin America Transactions}, title={MAPE-K as a service-oriented architecture},
year={2017},
volume={15},
number={6},
pages={1163-1175},
abstract={The reflective middlewares have been used as a powerful tool to cope with inherent heterogeneous nature of distributed systems, in order to give them greater adaptability capacities. Recently, some papers have extended the reflective middlewares with autonomic capabilities based on the autonomic computing paradigm. One of the main component is the MAPE-K (Monitor-Analyze-Plan-Execute plus Knowledge) component of the autonomic manager. In this paper we develop a MAPE-K component as a service of the autonomic managers of a reflective middleware based on autonomic computing. In this way, the MAPE-K is distributed to each service that is part of the composition, and internally each component of MAPE-K is constructed as a separate service, distributed and weakly coupled, giving great flexibility. We test the MAPE-K component as a service in a reflective middleware architecture based on autonomic computing, for the distributed diagnostic of faults in the services composition.},
keywords={Service-oriented architecture;Semiconductor optical amplifiers;Monitoring;Quality of service;IEEE transactions;autonomic computing;reflective middleware;SOA systems},
doi={10.1109/TLA.2017.7932705},
ISSN={1548-0992},
month={June},}
@INPROCEEDINGS{7915444,
author={Ahmadloo, Fatemeh and Salmasi, Farzad Rajaei},
booktitle={2017 IEEE International Conference on Industrial Technology (ICIT)}, title={A cyber-attack on communication link in distributed systems and detection scheme based on H-infinity filtering},
year={2017},
volume={},
number={},
pages={698-703},
abstract={In this paper, a new cyber-attack is proposed for distributed systems, while detection of such an attack is recommended based on H-infinity filtering. In particular, we study the effect of a specific attack on the communication links between local control centers. Today's cyber-physical systems are complex and large scale, so that they are distributed over a large area. Using distributed control and monitoring centers for such systems results in great advantages. However, cyber-attacks on distributed systems should be considered seriously. First, we address a novel cyber-attack scheme, which employs both the sensor signals and communication links between monitoring centers. Then, the detection scheme for the proposed attack is introduced from an operator point of view. The proposed schemes are simulated in a distributed power system based on IEEE standard test benches. The simulation results show the feasibility of the proposed attack and detection schemes.},
keywords={Monitoring;Software;Filtering;Power systems;Simulation;State estimation;Communication link;cyber-attack;cyber-security;distributed system;H∞ filtering;shared network},
doi={10.1109/ICIT.2017.7915444},
ISSN={},
month={March},}
@INPROCEEDINGS{7899081,
author={Lima, Bruno Miguel Carvalhido and Faria, João Carlos Pascoal},
booktitle={2017 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)}, title={Towards Decentralized Conformance Checking in Model-Based Testing of Distributed Systems},
year={2017},
volume={},
number={},
pages={356-365},
abstract={In a growing number of domains, the provisioning of end-to-end services to the users depends on the proper interoperation of multiple products, forming a new distributed system. To ensure interoperability and the integrity of this new distributed system, it is important to conduct integration tests that verify not only the interactions with the environment but also the interactions between the system components. Integration test scenarios for that purpose may be conveniently specified by means of UML sequence diagrams, possibly allowing multiple execution paths. The automation of such integration tests requires that test components are also distributed, with a local tester deployed close to each system component, and a central tester coordinating the local testers. In such a test architecture, it is important to minimize the communication overhead during test execution. Hence, in this paper we investigate conditions upon which conformance errors can be detected locally (local observability) and test inputs can be decided locally (local controllability) by the local testers, without the need for exchanging coordination messages between the test components during test execution. The conditions are specified in a formal specification language that allows executing and validating the specification. Examples of test scenarios are also presented, illustrating local observability and controllability problems associated with optional messages without corresponding acknowledgment messages, races and non-local choices.},
keywords={Unified modeling language;Testing;Monitoring;Observability;Controllability;Semantics;Visualization;Model-Based Testing;Conformance Checking;Integration Testing;Distributed Systems;UML},
doi={10.1109/ICSTW.2017.64},
ISSN={},
month={March},}
@INPROCEEDINGS{7890173,
author={Hyun, Wook and Park, Ju Young},
booktitle={2017 19th International Conference on Advanced Communication Technology (ICACT)}, title={Implementations of automated testing environment for distributed multimedia streaming applications},
year={2017},
volume={},
number={},
pages={653-656},
abstract={Nowadays, multimedia streaming service is one of the widely used application over Internet. However, it costs much and needs infrastructures to accommodate massive concurrent users in server-client model. Hence, there several solutions and standards are under development based on peer-to-peer networking, and we are implementing prototype based on ITU-T Recommendation. On testing application in distributed systems, operator has to control whole client one-by-one manually on every test scenario, and it was not easy to test time critical scenarios. Hence, we have constructed our own testing environment by using several open source solutions. In this paper, we share our experiences on constructing automated testing environment for distributed multimedia streaming application.},
keywords={Testing;Streaming media;Multimedia communication;Servers;Peer-to-peer computing;Protocols;Standards;automation;testing environment;distributed application;multimedia streaming},
doi={10.23919/ICACT.2017.7890173},
ISSN={},
month={Feb},}
@ARTICLE{7875503,
author={Jaskolka, Jason and Villasenor, John},
journal={IEEE Transactions on Reliability}, title={An Approach for Identifying and Analyzing Implicit Interactions in Distributed Systems},
year={2017},
volume={66},
number={2},
pages={529-546},
abstract={Safety-critical system domains such as critical infrastructures, aerospace, automotive, and industrial manufacturing and control are becoming increasingly dependent on the use of distributed systems to achieve their functionality. These distributed systems can contain many complex interactions among their constituent components. Despite extensive testing and verification of individual components, security vulnerabilities resulting from unintended and unforeseen component interactions (so-called implicit interactions) often remain undetected and can have an impact on the safety, security, and reliability of a system. This paper presents an approach for identifying and analyzing the existence and severity of implicit interactions in distributed systems. The approach is based on the modeling framework known as communicating concurrent Kleene algebra (C2KA). Experimental results confirm that this approach can successfully identify and analyze dependencies in system designs that would otherwise be very hard to find. More broadly, the methods presented in this paper can help address the growing need for rigorous and practical methods and techniques for assuring the safe, secure, and reliable operation of distributed systems in critical domains.},
keywords={Security;Reliability;Algebra;Safety;Complexity theory;System analysis and design;Testing;Assurance;communicating concurrent Kleene algebra (C $^2$ KA);distributed systems;implicit interactions;modeling},
doi={10.1109/TR.2017.2665164},
ISSN={1558-1721},
month={June},}
@ARTICLE{7752832,
author={Zhang, Dan and Wen, Yinghong and Wang, Yansheng and Liu, Dong and He, Xiaodong and Fan, Jun},
journal={IEEE Transactions on Electromagnetic Compatibility}, title={Coupling Analysis for Wires in a Cable Tray Using Circuit Extraction Based on Mixed-Potential Integral Equation Formulation},
year={2017},
volume={59},
number={3},
pages={862-872},
abstract={The China high-speed trains use cable trays to neatly arrange the cables running through the distributed systems. On one hand, they protect cables against external electromagnetic interference and reduce external radiation from the cables. On the other hand, the cable trays create waveguide structures that affect the coupling among cables inside the cable trays. To simplify coupling analysis for cables going through a cable tray, a lumped circuit model was built using admittance blocks extracted from a mixed-potential integral equation (MPIE) formulation. In the MPIE formulation, either the half-free space or the waveguide dyadic Green's function was used depending on the region where the cables were. A test case was investigated. Results by the proposed circuit model were validated by measurement and full-wave simulation results.},
keywords={Power cables;Wires;Electric potential;Couplings;Mathematical model;Green's function methods;Transmission line matrix methods;Cable tray;China high-speed train;circuit extraction;coupling;dyadic Green's function;method of moment (MoM)},
doi={10.1109/TEMC.2016.2626310},
ISSN={1558-187X},
month={June},}
@ARTICLE{7484300,
author={Pham, Cuong and Wang, Long and Tak, Byung Chul and Baset, Salman and Tang, Chunqiang and Kalbarczyk, Zbigniew and Iyer, Ravishankar K.},
journal={IEEE Transactions on Parallel and Distributed Systems}, title={Failure Diagnosis for Distributed Systems Using Targeted Fault Injection},
year={2017},
volume={28},
number={2},
pages={503-516},
abstract={This paper introduces a novel approach to automating failure diagnostics in distributed systems by combining fault injection and data analytics. We use fault injection to populate the database of failures for a target distributed system. When a failure is reported from production environment, the database is queried to find “matched” failures generated by fault injections. Relying on the assumption that similar faults generate similar failures, we use information from the matched failures as hints to locate the actual root cause of the reported failures. In order to implement this approach, we introduce techniques for (i) reconstructing end-to-end execution flows of distributed software components, (ii) computing the similarity of the reconstructed flows, and (iii) performing precise fault injection at pre-specified executing points in distributed systems. We have evaluated our approach using an OpenStack cloud platform, a popular cloud infrastructure management system. Our experimental results showed that this approach is effective in determining the root causes, e.g., fault types and affected components, for 71-100 percent of tested failures. Furthermore, it can provide fault locations close to actual ones and can easily be used to find and fix actual root causes. We have also validated this technique by localizing real bugs that occurred in OpenStack.},
keywords={Databases;Software;Production systems;Fault location;Computer bugs;Fault injection;failure diagnosis;processing flow;distributed system;fault localization},
doi={10.1109/TPDS.2016.2575829},
ISSN={1558-2183},
month={Feb},}
@ARTICLE{7134784,
author={Barazzutti, Raphaël and Felber, Pascal and Mercier, Hugues and Onica, Emanuel and Rivière, Etienne},
journal={IEEE Transactions on Dependable and Secure Computing}, title={Efficient and Confidentiality-Preserving Content-Based Publish/Subscribe with Prefiltering},
year={2017},
volume={14},
number={3},
pages={308-325},
abstract={Content-based publish/subscribe provides a loosely-coupled and expressive form of communication for large-scale distributed systems. Confidentiality is a major challenge for publish/subscribe middleware deployed over multiple administrative domains. Encrypted matching allows confidentiality-preserving content-based filtering but has high performance overheads. It may also prevent the use of classical optimizations based on subscriptions containment. We propose a support mechanism that reduces the cost of encrypted matching, in the form of a prefiltering operator using Bloom filters and simple randomization techniques. This operator greatly reduces the amount of encrypted subscriptions that must be matched against incoming encrypted publications. It leverages subscription containment information when available, but also ensures that containment confidentiality is preserved otherwise. We propose containment obfuscation techniques and provide a rigorous security analysis of the information leaked by Bloom filters in this case. We conduct a thorough experimental evaluation of prefiltering under a large variety of workloads. Our results indicate that prefiltering is successful at reducing the space of subscriptions to be tested in all cases. We show that while there is a tradeoff between prefiltering efficiency and information leakage when using containment obfuscation, it is practically possible to obtain good prefiltering performance while securing the technique against potential leakages.},
keywords={Subscriptions;Cryptography;Containers;Arrays;Cloud computing;Publish/subscribe;confidentiality;security;encrypted processing;Bloom filters},
doi={10.1109/TDSC.2015.2449831},
ISSN={1941-0018},
month={May},}
@INPROCEEDINGS{7991694,
author={Styugin, Mikhail and Zolotarev, Vyacheslav and Prokhorov, Anton and Gorbil, Roman},
booktitle={2016 IEEE 10th International Conference on Application of Information and Communication Technologies (AICT)}, title={New approach to software code diversification in interpreted languages based on the moving target technology},
year={2016},
volume={},
number={},
pages={1-5},
abstract={The paper presents a method based on moving target technology for protecting software components of distributed systems. Practical implementation of this method is a code diversifier, which adds intermediary functions and inserts transformation of key variables into the program code. Theoretical analysis of the method presented here demonstrated its effectiveness even when an adversary had access to a application's program code. The diversifier presented below can also function in a mode of program code obfuscation, which was tested and demonstrated by the example of interpreted programming language functions. Metrics obtained from the program code after obfuscation, showed sufficient level of code modification for independent use in obfuscation mode.},
keywords={Software;Measurement;Computer security;Information security;Software algorithms;Random variables;software security;diversification;obfuscation;moving target defense},
doi={10.1109/ICAICT.2016.7991694},
ISSN={2472-8586},
month={Oct},}
@INPROCEEDINGS{7982335,
author={Lee, Sunghee and Hwang, Bueng Il and Seo, Kang-Bok and Lee, Woo Jin},
booktitle={2016 IEEE Intl Conference on Computational Science and Engineering (CSE) and IEEE Intl Conference on Embedded and Ubiquitous Computing (EUC) and 15th Intl Symposium on Distributed Computing and Applications for Business Engineering (DCABES)}, title={Relative Time Synchronization of Distributed Applications for Software-in-the-Loop Simulation},
year={2016},
volume={},
number={},
pages={753-756},
abstract={As distributed systems such as automotive, medical, manufacturing automation become larger and more complex, it is difficult to test these systems. Also, the synchronization of distributed applications make the testing more difficult. In the Software-in-the-Loop (SiL) simulation, a synchronization method among clock of applications is provided for virtual hardware devices and environment. A typical synchronization technique is that a single global clock synchronizes local clocks of other sites but may make high accuracy but low performance in SiL simulation because synchronization occurs in all nodes when the global clock ticks. Also there are some issues of which node should be selected, how much time the global clock drifts when adding new sites, etc. In the paper, we propose a method of clock synchronization based on relative time to enhance the simulation performance.},
keywords={Clocks;Synchronization;Sensors;Conferences;Testing;Mathematical model;Control systems;Synchronization;Relative;Distributed;Simulation;Software-in-the-loop},
doi={10.1109/CSE-EUC-DCABES.2016.273},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7954417,
author={Kurtz, Andreas and Bauer, Bernhard and Koeberl, Marcel},
booktitle={2016 4th International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, title={Comparing system- and test model with integrated software-based signal simulation},
year={2016},
volume={},
number={},
pages={656-662},
abstract={Test automation in distributed systems requires new methods in signal simulation for the stimulation of the distributed system. Increasing complexity of electric electronic (E/E) systems enhances the testing-effort. The main challenge is reducing the time consuming manual stimulation in consideration of improving the quality of testing. Currently used systems for test automation with a software-based approach have to be adapted to each hardware and software version of the system to be tested. The approach represented shows a new approach through the integration of a simulation service in the AUTOSAR software architecture. By integrating a generic software-based simulation module with an interaction point at the basic software driver layer, the execution of tests can be automated and improved under consideration of adaptivity and reproducibility.},
keywords={Software;Testing;Hardware;Complexity theory;Data models;Automotive engineering;Manuals;Automotive;AUTOSAR;Distributed Systems;Method;Model Based Testing;Path Detection;Path Reduction;Simulation;System Model;Test Automation;Test Model;Testing},
doi={},
ISSN={},
month={Feb},}
@INPROCEEDINGS{7930659,
author={Badri, Sahar and Fergus, Paul and Hurst, William},
booktitle={2016 9th International Conference on Developments in eSystems Engineering (DeSE)}, title={Statistical Analysis Methods for Interdependency Communication in Distributed Systems},
year={2016},
volume={},
number={},
pages={273-278},
abstract={Critical infrastructure assets contribute to the economy and society as a whole. Their impact on the security, economy and health sector are extremely vital. However, their increasing complexity has led to the creation of direct and indirect interdependent connections amongst the infrastructure groupings. In this paper, the development of a distributed support system is presented. The system employs behaviour analysis techniques to support interconnected infrastructures and distribute security advice throughout a distributed system of systems. The approach put forward is tested through a statistical analysis method approach in order to investigate the cascading failure effect whilst taking in to count the independent variables.},
keywords={Coal;Statistical analysis;Power grids;Power cables;Security;Market research;Face;Critical Infrastructure Protection;Interdependency;Statistical Analysis Methods;Big Data},
doi={10.1109/DeSE.2016.42},
ISSN={2161-1343},
month={Aug},}
@INPROCEEDINGS{7910941,
author={Sonkin, D. M. and Meita, R. V. and Khrul, S. A.},
booktitle={2016 2nd International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)}, title={One of possible approaches to estimate availability of distributed hardware and software systems},
year={2016},
volume={},
number={},
pages={1-6},
abstract={Methods of evaluation of condition and operability of complex distributed systems are considered in this paper. Author propose a new conceptual model of multi-stage complex performance evaluation based on multi-criteria function including various performance, reliability, stability characteristics indicating their contribution to general system performance. The approach used makes it possible to obtain an integrated assessment of system availability taking into account hardware and software reliability analysis as well as influence of various factors (hardware, software, operator's errors) on overall system condition. The Complex Population Notification and Warning System was considered as a subject of the research and was used to test the model developed.},
keywords={Hardware;Software;Mathematical model;Software reliability;Communication channels;Industrial engineering;methodology of reliability evaluation;state estimation;distributed systems;multi-state connection elements;hardware-software complex in EMERCOM},
doi={10.1109/ICIEAM.2016.7910941},
ISSN={},
month={May},}
@INPROCEEDINGS{7907494,
author={Donzelli, Corentin and Kidanu, Solomon Asres and Chbeir, Richard and Cardinale, Yudith},
booktitle={2016 12th International Conference on Signal-Image Technology Internet-Based Systems (SITIS)}, title={Onto2MAS: An Ontology-Based Framework for Automatic Multi-Agent System Generation},
year={2016},
volume={},
number={},
pages={381-388},
abstract={Multi-Agent Systems (MASs) have received muchattention in recent years because of their advantages on modelingcomplex distributed systems. Current modeling languages andmethodologies that support the construction of such systemsrequire the use of different tools to complete their design, development, and deployment. However, the development of MASsremains a complicated task, which demands time and specialprogramming skills. This paper proposes a framework, calledOnto2MAS, to provide developers an automatic generation ofMASs, based on an ontology to assist the easy definition of agentsinteraction and knowledge. With Onto2MAS, the complexityof the development process of MASs is reduced and an easyway to design uniform communication, messages exchanging, and generation of new knowledge in the system is provided. Todemonstrate the efficiency of our approach, we also present theresults of the experimental tests that we conducted with a firstimplementation of Onto2MAS, called OnToJade.},
keywords={Ontologies;Unified modeling language;Prototypes;Capability maturity model;Programming;Generators;Multi-agent systems;Multi-Agent Systems;Ontology;Framework;JADE},
doi={10.1109/SITIS.2016.67},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7904278,
author={Singla, Adhish and Desai, Krishnaji and Purini, Suresh and Choppella, Venkatesh},
booktitle={2016 15th International Symposium on Parallel and Distributed Computing (ISPDC)}, title={Distributed Safety Verification Using Vertex Centric Programming Model},
year={2016},
volume={},
number={},
pages={114-120},
abstract={Software is finding place in deeply embedded systems to large scale distributed systems of cloud service providers such as Amazon and Google. Due to the concurrent and distributed nature of this software, it is hard to test for correctness of such systems in a foolproof manner. Explicit state model checking is an approach in which we build a model of the system and specify the properties it should hold. Then we construct a state transition system from the model and check if it satisfies the specified properties. There are two kinds of properties of interest: safety and liveness. In this paper, we focus our attention on safety verification, which involves checking if the states that are generated in the transition system satisfy some predicate formulae specified in the form of assertions. The main problem here is that the number of states in the transition system grows exponentially with the number of bits required to store the state of a model at any given point time. So the available main memory even in a server class machine is not sufficient to model check non-trivial practical models. One approach to address this problem is by using resources from a distributed collection of machines. In this paper, we adopt this approach, by proposing a distributed safety property verification algorithm using the vertex centric programming model.},
keywords={Safety;Computational modeling;Model checking;Object oriented modeling;Programming;Radiation detectors;Java},
doi={10.1109/ISPDC.2016.23},
ISSN={},
month={July},}
@INPROCEEDINGS{7881407,
author={De Doncker, Elise and Almulihi, Ahmed},
booktitle={2016 International Conference on Computational Science and Computational Intelligence (CSCI)}, title={Adaptive Task Partitioning for Bayesian Applications},
year={2016},
volume={},
number={},
pages={572-577},
abstract={The numerical integration of the posterior in Bayesian analysis leads to a class of multivariate integration problems where the integrand has a dominant peak. As Monte-Carlo integration is inadequate, various techniques have been proposed in the literature, involving substantial reformulation with additional analysis, or transformations requiring additional problem parameters. We resort to a black-box approach provided by the ParInt multivariate integration package, which is layered over MPI to run on a distributed system, and utilizes adaptive task partitioning with load balancing to keep high-error subregions distributed over the processes. The performance of the algorithm is demonstrated by detailed test results for Bayesian integrals arising as posterior expectations in linear modeling and for cross-classifications in medical data.},
keywords={Load management;Bayes methods;Monte Carlo methods;Adaptation models;Partitioning algorithms;Process control;Parallel adaptive task partitioning;Bayesian inference;Linear model;Multivariate integration;ParInt package},
doi={10.1109/CSCI.2016.0114},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7839807,
author={Hoque, Endadul and Nita-Rotaru, Cristina},
booktitle={2016 IEEE Cybersecurity Development (SecDev)}, title={Building Robust Distributed Systems and Network Protocols by Using Adversarial Testing and Behavioral Analysis},
year={2016},
volume={},
number={},
pages={138-145},
abstract={We describe our experience over the past five years with building more robust distributed systems and network protocols by using adversarial testing and behavioral analysis. We describe the benefits and disadvantages of both approaches and the design of the tools we have built (Turret, Turret-W, SNAKE, and Chiron). We discuss how we applied them to byzantine-resilient state machine replication, wireless routing protocols, transport protocols, TLS, and IoT implementation of application-level protocols.},
keywords={Testing;Semantics;Computer bugs;Robustness;Transport protocols;Heuristic algorithms;robustness;protocols;implementations;adversarial testing;behavioral analysis},
doi={10.1109/SecDev.2016.038},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7818913,
author={Punt, Marija M.},
booktitle={2016 24th Telecommunications Forum (TELFOR)}, title={Human-computer interaction in an integrated environment combining digital TV, mobile devices and internet},
year={2016},
volume={},
number={},
pages={1-8},
abstract={Applications developed in an integrated environment consisting of digital TV, mobile devices and internet can offer users novel human-computer interaction scenarios compared to the scenarios that a single device could offer. In this paper the SHARP development framework is presented. The framework allows efficient implementation of distributed systems in an integrated environment. The field of game development was chosen as the target domain to verify the benefits of the framework. Conclusions on how the users accepted the identified scenarios were made by examining the experience of test users interacting with the developed applications. The SHARP framework was verified by comparing size, complexity and responsiveness of the applications developed using the framework and without using the framework.},
keywords={Mobile handsets;Games;Human computer interaction;Digital TV;Internet;Sensors;human-computer interaction;interactive TV;development framework;mobile devices;second screen;TV centric gaming},
doi={10.1109/TELFOR.2016.7818913},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7816673,
author={Ding, Chang-Song and Liang, Yang},
booktitle={2016 International Conference on Information System and Artificial Intelligence (ISAI)}, title={Two-Phase Pricing Strategy Based on QoS Constraints in Distributed Computing},
year={2016},
volume={},
number={},
pages={50-55},
abstract={To solve the adverse effects brought by resource node transfering the using right to local task and the difficult problem of resource load balancing, a two-phase pricing strategy based on QoS constraints is proposed in this paper. On the premise of guaranteeing the benefits of the resource provider in the cost price, this strategy balances the load of the resource provider by the profit price. The theoretical analysis proves the effectiveness of the pricing strategy, and the algorithm of the pricing strategy is designed in this paper. Resources node information in the real distributed systems is used as the performance parameters of experimental node in the simulation experiments, and the performance of the pricing strategy is tested in a large-scale grid mission. Experimental results show that, compared with the traditional pricing strategies, the two-phase pricing strategy based on QoS constraints has vastly superior performance on the benefits of the resource provider and the balance of resource utilization.},
keywords={Pricing;Resource management;Quality of service;Load management;Supply and demand;Load modeling;Complexity theory;distributed computing;QoS;two-phase pricing strategy;grid mission},
doi={10.1109/ISAI.2016.0020},
ISSN={},
month={June},}
@INPROCEEDINGS{7814554,
author={Lima, Bruno and Faria, João Pascoal},
booktitle={2016 10th International Conference on the Quality of Information and Communications Technology (QUATIC)}, title={Towards the Online Testing of Distributed and Heterogeneous Systems with Extended Petri Nets},
year={2016},
volume={},
number={},
pages={230-235},
abstract={The growing dependence of our society on increasingly complex software systems makes software testing ever more important and challenging. In many domains, such as healthcare and transportation, several independent systems, forming a heterogeneous and distributed system of systems, are involved in the provisioning of end-to-end services to users. However, existing testing techniques, namely in the model-based testing field, provide little support for properly testing such systems. To bridge the gaps identified in the state of the art we intend to develop a research work where the main goal is to significantly reduce the cost of testing distributed and heterogeneous systems, from the standpoint of time, resources and expertise required, as compared to existing approaches. For that, we propose a preliminary approach and a toolset architecture for automating the testing of end-to-end services in distributed and heterogeneous systems. The tester interacts with a visual modeling frontend to describe key behavioral scenarios, invoke test generation and execution, and visualize test results and coverage information back in the model. The visual modeling notation is converted to a formal notation amenable for runtime interpretation in the backend. A distributed test monitoring and control infrastructure is responsible for interacting with the components of the system under test, as test driver, monitor and stub. At the core of the toolset, a test execution engine coordinates test execution and checks the conformance of the observed execution trace with the expectations derived from the visual model. A real world example from the Ambient Assisted Living domain is presented to illustrate the approach. As future work we intend to develop distributed and incremental algorithms for online testing of distributed and heterogeneous systems based on Extended Petri Nets at runtime and validate them in real world case studies.},
keywords={Unified modeling language;Testing;Petri nets;Runtime;Object oriented modeling;Visualization;System of systems;Software Testing;Distributed algorithms;UML;Petri nets},
doi={10.1109/QUATIC.2016.057},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7811594,
author={Brennand, Celso A. R. L. and Duarte, Joao M. and Silva, Aloizio P.},
booktitle={2016 8th IEEE Latin-American Conference on Communications (LATINCOM)}, title={SimGrid: A simulator of network monitoring topologies for peer-to-peer based computational grids},
year={2016},
volume={},
number={},
pages={1-6},
abstract={A crucial problem in distributed systems is the probability of failure occurrence in resources. Recent studies have been searching different ways for improving the execution time of applications and enhancing fault-tolerant mechanisms. In many cases testing is performed using simulated environments. This work presents the implementation of a simulator for computational grids prone to resource failures. The presented simulator allows the evaluation of different topologies for monitoring services on the grid including job scheduling, automatic failure detection, messages sending and efficient application recovery.},
keywords={Monitoring;Peer-to-peer computing;Biomedical monitoring;Topology;Network topology;Actuators;Scalability},
doi={10.1109/LATINCOM.2016.7811594},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7809381,
author={Al-Mahfoudh, Mohammed and Gopalakrishnan, Ganesh and Stutsman, Ryan},
booktitle={2016 IEEE/ACM 4th FME Workshop on Formal Methods in Software Engineering (FormaliSE)}, title={Toward Rigorous Design of Domain-Specific Distributed Systems},
year={2016},
volume={},
number={},
pages={42-48},
abstract={The advent of data center, cloud computing and IoT has thrust distributed systems building into the programming mainstream. Building correct distributed systems is notoriously hard, yet today's developers have little training and few tools to aid them in reasoning about these complex systems. To that end, we present DS2 - a domain specific language and integrated framework for specifying, synthesizing, and reasoning. The DS2 language is parsimonious, and comes with an operational semantics that lends semantic clarity and enables formal analysis. A variety of techniques for model exploration, active testing, and synthesis of detailed implementations from higher level specifications are being developed. This paper details these aspects of DS2 and provides a roadmap of its evolution.},
keywords={Semantics;Testing;Object oriented modeling;Context;Buildings;Cognition;Fault tolerance;Distributed Systems;Fault Tolerance;Formal Methods;Concurrency;Actors},
doi={10.1109/FormaliSE.2016.015},
ISSN={},
month={May},}
@INPROCEEDINGS{7805063,
author={El Hassan Charaf, Moulay and Azzouzi, Salma},
booktitle={2016 4th IEEE International Colloquium on Information Science and Technology (CiSt)}, title={Timed distributed testing rules for the distributed test architecture},
year={2016},
volume={},
number={},
pages={314-319},
abstract={The paper presents some technical issues for testing distributed systems. The proposed approach consists on exploring the temporal properties that specify the time required to exchange messages between the various components of the distributed test application. Thereby taking into consideration the temporal properties in the specification of the behavior of conformance testing, it will provide a higher degree of expressiveness and flexibility. In other side, the paper show how to cope with problems encountered in distributed testing using timed distributed testing rules. Afterwards, we present our algorithm allowing the generation of such rules and a prototype of test using a Distributed Algorithmic and Rule-Based System (DARBS).},
keywords={Testing;Clocks;Ports (Computers);Automata;Synchronization;Controllability;Distributed testing;Controllability;Observability;Synchronization;Timing Constraints;Rules;DARBS},
doi={10.1109/CIST.2016.7805063},
ISSN={2327-1884},
month={Oct},}
@INPROCEEDINGS{7796516,
author={Tanaka, Ikki and Ohmori, Hiromitsu},
booktitle={2016 IEEE Innovative Smart Grid Technologies - Asia (ISGT-Asia)}, title={Scenario generation with clustering for optimal allocation of renewable DG},
year={2016},
volume={},
number={},
pages={966-971},
abstract={Active introduction of renewable distributed generation (DG) to the distribution system has become one of the challenging research in recent years. In the introduction plan of the renewable energy in Japan, long-term goal for high penetration of DG towards 2030 is shown. However, specific implementation plan such as the allocation, capacity, and timing has not been decided. In this paper, for decision-making of introducing DG, a novel scenario generation method with K-means is proposed. This method generates electric load, wind, and PV-production scenarios representing uncertainty, which are used as input for stochastic programming. Then, the problem is formulated as two-stage multiperiod mixed-integer linear programming (MILP) model. In this model, investment variables are decided in the first stage and operation and maintenance variables that depends on scenarios are solved in the second stage. This model minimize the total distributed system cost that is affected by demand growth. Constraints include power flow constraints, substation and feeders capacities, investment constraints, voltage, and current limits. In numerical simulation, several scenarios are generated using historical weather and demand data of Japan and the model is tested on 34-bus system.},
keywords={Uncertainty;Stochastic processes;Programming;Planning;Resource management;Load modeling;Wind speed},
doi={10.1109/ISGT-Asia.2016.7796516},
ISSN={2378-8542},
month={Nov},}
@INPROCEEDINGS{7792490,
author={Rossini, Rosaria and Ferrera, Enrico and Conzon, Davide and Pastrone, Claudio},
booktitle={2016 8th IFIP International Conference on New Technologies, Mobility and Security (NTMS)}, title={WSNs Self-Calibration Approach for Smart City Applications Leveraging Incremental Machine Learning Techniques},
year={2016},
volume={},
number={},
pages={1-7},
abstract={The diffusion of the Internet of Things paradigm, in the last few years, has led to the need of deploying and managing large-scale Wireless Sensor Networks (WSNs), composed by a multitude of geographically distributed sensors, like the ones needed for Smart City applications. The traditional way to manage WSNs is not suitable for this type of applications, because manually managing and monitoring every single sensor would be too expensive, time consuming and error prone. Moreover, unattended sensors may suffer of several issues that progressively make their measures unreliable and consequently useless. For this reason, several automatically techniques have been studied and implemented for the detection and correction of measurements from sensors which are affected by errors caused by aging and/or drift. These methods are grouped under the name of self-calibration techniques. This paper presents a distributed system, which combines an incremental machine learning technique with a non-linear Kalman Filter estimator, which allows to automatically re-calibrate sensors leveraging the correlation with measurements made by neighbor sensors. After the description of the used model and the system implementation details, the paper describes also the proof-of-concept prototype that has been built for testing the proposed solution.},
keywords={Calibration;Intelligent sensors;Sensor phenomena and characterization;Wireless sensor networks;Estimation;Kalman filters},
doi={10.1109/NTMS.2016.7792490},
ISSN={2157-4960},
month={Nov},}
@INPROCEEDINGS{7795076,
author={Biller, M. and Jaeger, J. and Mladenovic, I. and Schacherer, C. and Wolter, D. and Stoetzel, M.},
booktitle={13th International Conference on Development in Power System Protection 2016 (DPSP)}, title={Protection systems in distribution grids with variable short-circuit conditions},
year={2016},
volume={},
number={},
pages={1-5},
abstract={This paper investigates the protection system of a German 20kV distribution grid in terms of fault clearing times for volatile short-circuit conditions. For this purpose, an automated protection security assessment software tool is deployed. Firstly, the fault clearing times for 250 faults in two protection areas in the prevailing grid are attained for a case with no distributed generation infeed. Two subcases are investigated: In the first step, the short-circuit power of the subtransmission network is as it has been in year 2012. In the next step, the short-circuit power is reduced to 70% to depict the scheduled shutdown of several nuclear power plants in Germany. Secondly, the functionality of the same protection system is tested for DG share in the distributed system as it is assumed to be in year 2020. This forecast is merged from several studies. To overcome overloadings and to reduce losses without conventional grid expansion, meshing by closing normally open disconnectors can be suitable. This new grid topology is investigated in the same way as the other configurations. Fort the new topology, simulation results for a protection system equipped with distance protection relays are compared to a loose coupling strategy where the open loop topology is restored in case of fault.},
keywords={protection security assessment;distribution system protection;volatile short-circuit contribution;loose coupling},
doi={10.1049/cp.2016.0086},
ISSN={},
month={March},}
@INPROCEEDINGS{7752361,
author={Lu, Junnan and Thomo, Alex},
booktitle={2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)}, title={An experimental evaluation of giraph and GraphChi},
year={2016},
volume={},
number={},
pages={993-996},
abstract={We focus on the vertex-centric (VC) model introduced in Pregel, a Google system for distributed graph processing. In particular, we consider two popular implementations of the VC model: Apache Giraph and GraphChi. The first is a VC system for cluster computing, while the second is a VC system for a single PC. Apache Giraph became very popular after careful engineering by Facebook researchers in 2012 to scale the computation of PageRank to a trillion-edge graph of user interactions using 200 machines. On the other hand, GraphChi became popular, around the same time in 2012, as it made possible to perform intensive graph computations in a single PC, in just under 59 minutes, whereas the distributed systems were taking 400 minutes using a cluster of about 1,000 computers (as reported also by MIT Technology Review). Since then, new versions of Apache Giraph and GraphChi have been released, where new ideas and optimizations have been implemented. Therefore, it is time to validate again the claims made four years ago. In this work, we embark in this validation. We consider three cornerstone graph problems: computing PageRank, shortest-paths, and weakly-connected-components. Based on current experiments, we conclude that in the present, even for a moderate number of simple machines, Apache Giraph outperforms GraphChi for all the algorithms and datasets tested. This is in contrast to the claims of the GraphChi authors in 2012.},
keywords={Computational modeling;Google;Facebook;Computers;Optimization;Web pages},
doi={10.1109/ASONAM.2016.7752361},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7755661,
author={Malla, J. M. R. and Sahu, Manoj Kumar and Subudhi, P K},
booktitle={2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)}, title={Applications of multilevel inverter in GTO converter based DSTATCOM},
year={2016},
volume={},
number={},
pages={4936-4940},
abstract={Distributed Static Synchronous Compensator (DSTATCOM) plays a key role in maintaining power quality on load side in distribution system. The paper investigates the dynamic operation of novel control scheme for DSTATCOM with 9-level inverter. The performance of proposed controller is simulated by the MATLAB/Simulink. The application of multilevel inverter is investigated in this paper. As per as advantages of 9 level converter, it is used instead of 48 pulse converter for DSTATCOM operation in distributed system. In general 48 pulse converter requires 8 transformers and 8 six pulse converters. Hence, it is expensive and complex in operation. In order to overcome these problems, proposed 9 level multilevel inverter based DSTATCOM and it is tested under balanced and unbalanced load condition. Moreover, results also presented for reactive power and non-linear load compensation. In our proposed structure have only one transformer and single 9 level converter, therefore, operation in controlling is very easy and cost effective compare to general configuration.},
keywords={Inverters;Voltage control;Reactive power;Capacitors;Power quality;Harmonic analysis;DSTATCOM;9 level converter;reactive power compensation;unbalance load compensation},
doi={10.1109/ICEEOT.2016.7755661},
ISSN={},
month={March},}
@INPROCEEDINGS{7755548,
author={Malla, Siva Ganesh and Malla, J. M. R. and Deepu, D. Jaya and Kumar, D. Pavan},
booktitle={2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)}, title={Performance evaluation of DSTATCOM with 9-level GTO converter},
year={2016},
volume={},
number={},
pages={4392-4396},
abstract={The paper investigates the dynamic operation of novel control scheme for Distributed Static Synchronous Compensator (DSTATCOM) with 9-level inverter. The performance of proposed controller is simulated by the MATLAB/Simulink. The DSTATCOM scheme and the electric grid network are modeled by specific electric blocks from the power system block set, while the control system is modeled using Simulink. In this paper 9 level converter is used instead of 48 pulse converter for DSTATCOM operation in distributed system. In general 48 pulse converter requires 8 transformers and 8 six pulse converters. Hence, it is expensive and complex in operation. In order to overcome these problems, proposed 9 level multilevel inverter based DSTATCOM and it is tested under balanced and unbalanced load condition. Moreover, results also presented for reactive power and non-linear load compensation. In our proposed structure have only one transformer and single 9 level converter, therefore, operation in controlling is very easy and cost effective compare to general configuration.},
keywords={Inverters;Voltage control;Reactive power;Capacitors;Power quality;Harmonic analysis;DSTATCOM;9 level converter;reactive power compensation;unbalance load compensation},
doi={10.1109/ICEEOT.2016.7755548},
ISSN={},
month={March},}
@INPROCEEDINGS{7745003,
author={Franco, Felipe and Stella, Gilson D. and Neme, João H. and Santos, Max M. D. and Stevan, Sergio L. and da Rosa, João N. H.},
booktitle={2016 IEEE 25th International Symposium on Industrial Electronics (ISIE)}, title={Teaching Model-In-the Loop: A case study for controller of distributed dashboard in a road vehicle},
year={2016},
volume={},
number={},
pages={863-868},
abstract={Model Based Design is a powerfull tool that offer the possibility of test an early concept without a physical implementation. One of the benefits is a fast way to validate a design before a real implementation, saving money and time. Model In the Loop follow as a tool to test the model and lastly validate the model. This philosophy is utilized to test different types of projects, including distributed systems. The dashboard electronic system in an automobile provides the driver real-time information on the physical variables, status and warnings. While the vehicle is a set of mechatronic subsystems, they are controlled by a number of distributed electronic control unities (ECU). Here, we demonstrated how model-based development methodology can help you to validate an application by means of a use case where a control software function in a dashboard is validated, considering a daschboard with CAN interface, explaining the implementation and the test utilized to validate the model. Using the same design environment, we established a set of requirements to be satisfied.},
keywords={Vehicles;Software;Ignition;Mathematical model;Testing;Batteries;Instruments;dashboard;instrument cluster;model-based design;can bus;embedded software and testing},
doi={10.1109/ISIE.2016.7745003},
ISSN={2163-5145},
month={June},}
@INPROCEEDINGS{7586619,
author={Haibin, Yuan and Qicai, Wu},
booktitle={2016 8th IEEE International Conference on Communication Software and Networks (ICCSN)}, title={Reconfigurable simulation platform for application design and development},
year={2016},
volume={},
number={},
pages={723-726},
abstract={Dealing with the prototype development and application design for distributed system in the field of industry automation, a method of establishing a control area network based platform with integrated simulation, test and development is discussed in order to achieve a standardized design and support network reconfiguration. At first, the physical elements that have different IO bus modules are installed, secondly, the functions are accomplished with software that presents a detailed configuration for the user to monitor and simulation. By testing the network requirement for real-time data transmission, the application simulation for the use of equipment fault monitor is presented as case study at last. The platform has the versatility, scalability and good real-time performance. Simulation result shows that the average delay of a single module is in within the threshold. The platform provides the technical basis and reference for the development of other distributed, networked equipment control systems.},
keywords={Computers;Data communication;Monitoring;Software;Prototypes;Vehicles;Automation;network;field bus;simulation;distributed system;monitor},
doi={10.1109/ICCSN.2016.7586619},
ISSN={},
month={June},}
@INPROCEEDINGS{7561432,
author={Chanakitkarnchok, Adsadawut and Nakorn, Kulit Na and Rojviboolchai, Kultida},
booktitle={2016 13th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)}, title={Autonomous website categorization with pre-defined dictionary},
year={2016},
volume={},
number={},
pages={1-6},
abstract={In this technology emerging era, the number of websites is increasing dramatically. The content and category of information are overflowing the Internet World. Finding the right information from almost a billion of websites is considerably hard, but finding the accurate and quality one is even harder. Hence, the need of website categorization's demand is increasing tremendously. Unfortunately, the website categorization techniques in previous works are still immature and not good enough to satisfy the need. Additionally, a training dataset is a limitation of supervised learning algorithm and unsupervised learning algorithm also have a complex algorithm. Regularly, they can categorize into only 1 category but the content usually contains various types. Therefore, in this paper, we propose the simple yet powerful algorithm for website categorization which can give a multi-category results with confidence level, distributed systems supported, and does not even need to be trained because the algorithm uses word frequency in the content of each website to match with the categories in a pre-defined dictionary. The result shows that the accuracy of our proposed algorithm is over 95% when tested with Reuters dataset. The comparison of our algorithm and another Text Analysis API shows that our algorithm has more accuracy with less computation time. The accuracy can also be increased by improving the pre-defined dictionary and filtering noise words.},
keywords={Dictionaries;Algorithm design and analysis;Classification algorithms;HTML;Machine learning algorithms;Uniform resource locators;Text categorization;text categorize;text classification;text analysis;URL categorize;Web categorize},
doi={10.1109/ECTICon.2016.7561432},
ISSN={},
month={June},}
@ARTICLE{7555268,
author={Cifuentes, Francisco and Bustos Jimenez, Javier and Simmonds, Jocelyn},
journal={IEEE Latin America Transactions}, title={Formal verification of distributed system using an executable C model},
year={2016},
volume={14},
number={6},
pages={2874-2878},
abstract={Formal verification means to rigorously explore the correctness of system designs expressed as mathematical models, most likely with the assistance of modern computers. Original approaches were to model and express a distributed system using existing theoretical tools such as Petri Nets. Nevertheless the main problems of such approaches are the restrictions imposed by formal tools and the human factor of simplify and model a distributed system. We propose a way to do formal verification of a distributed system by modeling the communication of the system as a concurrent program, instantiating the distributed system using threads and atomic queues and testing/verifying directly to the source code with specialized verifiers for concurrent programs. As an example, we show the verification of a distributed threshold signer using CBMC verifying properties such as memory leaks, index out of bounds, and data races.},
keywords={Computational modeling;Formal verification;Instruction sets;System recovery;Instruments;Mathematical model;Concurrent computing;concurrency;distributed systems;formal verification;model checking;symbolic execution},
doi={10.1109/TLA.2016.7555268},
ISSN={1548-0992},
month={June},}
@INPROCEEDINGS{7536567,
author={Jia, Qi and Guo, Linke and Jin, Zhanpeng and Fang, Yuguang},
booktitle={2016 IEEE 36th International Conference on Distributed Computing Systems (ICDCS)}, title={Privacy-Preserving Data Classification and Similarity Evaluation for Distributed Systems},
year={2016},
volume={},
number={},
pages={690-699},
abstract={Data classification is a widely used data mining technique for big data analysis. By training massive data collected from the real world, data classification helps learners discover hidden data patterns. In addition to data training, given a trained model from collected data, a user can classify whether a new incoming data belongs to an existing class, or, multiple distributed entities may collaborate to test the similarity of their trained results. However, due to data locality and privacy concerns, it is infeasible for large-scale distributed systems to share each individual's datasets with each other for data similarity check. On the one hand, the trained model is an entity's private asset and may leak private information, which should be well protected from all other non-collaborative entities. On the other hand, the new incoming data may contain sensitive information which cannot be disclosed directly for classification. To address the above privacy issues, we propose a privacy-preserving data classification and similarity evaluation scheme for distributed systems. With our scheme, neither new arriving data nor trained models are directly revealed during the classification and similarity evaluation procedures. The proposed scheme can be applied to many fields using data classification and evaluation. Based on extensive real-world experiments, we have also evaluated the privacy preservation, feasibility, and efficiency of the proposed scheme.},
keywords={Training;Support vector machines;Data models;Distributed databases;Data privacy;Protocols;Receivers;Privacy Preservation;Data Classification;Similarity Evaluation;Machine Learning},
doi={10.1109/ICDCS.2016.94},
ISSN={1063-6927},
month={June},}
@INPROCEEDINGS{7531625,
author={Fu, Kaiming- and Fang, Bin and Yafen-Li and Huijie-Li},
booktitle={2016 Chinese Control and Decision Conference (CCDC)}, title={Research on event-B based formal modeling and verification of automatic production line},
year={2016},
volume={},
number={},
pages={3690-3695},
abstract={Before the automatic production line put into practical use in industrial domain, it must go through a large number of long-term rigorous test to detect errors in the design process. However in the actual test process, the same mistake leads to different results due to the different test methods and there are some special conditions resulting from the limitation of field test environment what can't be used to be tested. The formal method, by using the method of discrete mathematics to practical system mathematical modeling and validation, can replace the methods what can not been tested under the conditions of system verification. The formal method is also suitable for developing large reactive and distributed systems. In this paper, the specific formal method which is called Event-B improves the high security and reliability of system. Moreover the related tools such as Rodin are used for modeling, refinement and verification in the PLC (Programmable Logic Controller) automatic production line. The result in this paper shows that our approach contributes to reducing system details during the early development stage and leads to simpler proofs and more automated proofs. Thus it provides a new method for reference for higher requirements in the reliability in engineering projects, so as to ensure the correctness of the designed software.},
keywords={Process control;Mathematical model;Fixtures;Reliability;Electronic mail;Formal verification;automatic production line;PLC;model;verify;Event-B;Formal method},
doi={10.1109/CCDC.2016.7531625},
ISSN={1948-9447},
month={May},}
@INPROCEEDINGS{7520493,
author={Rinaldi, Stefano and Ferrari, Paolo and Flammini, Alessandra and Loda, Matteo and Rizzi, Mattia},
booktitle={2016 IEEE International Instrumentation and Measurement Technology Conference Proceedings}, title={Distributed measurement system for the assesment of IEC 61850 transfer time in smart grid},
year={2016},
volume={},
number={},
pages={1-6},
abstract={The rise of Smart Grids based on IEC61850 standard requires specialized measurement systems in order to characterize the main performance parameters. In this paper, a new distributed measurement system for the estimation of IEC61850 Transfer Time over Smart Grid communication infrastructure is introduced. The structure of a possible implementation, based on specialized sampling devices coordinated by a virtual instrument, is described in details. The synchronization uncertainty of the distributed system is below 30ns, (thanks to the use of IEE1588 Precision Time Protocol), while the measurement extended uncertainty is below 100ns. A test case is used to demonstrate the system feasibility and the effectiveness of the obtained results: the evaluation of sample IEC61850 devices highlighted their weak points, allowing for a correct classification of their Transfer Time Class (according to IEC61850).},
keywords={Synchronization;Smart grids;Standards;Clocks;Probes;IEEE1588;Transfer Time;Smart Grid;IEC 61850;GOOSE;distributed measurement system},
doi={10.1109/I2MTC.2016.7520493},
ISSN={},
month={May},}
@INPROCEEDINGS{7518881,
author={Deore, Ujjwala D. and Waghmare, Vijaya},
booktitle={2016 International Conference on Information Communication and Embedded Systems (ICICES)}, title={Cyber security automation for controlling distributed data},
year={2016},
volume={},
number={},
pages={1-4},
abstract={Cyber security policy making should seek solutions that leverage the expertise of both the private sector and federal Government and should be results-oriented and technology-neutral. Automated and semi-automated solutions are needed to keep up with the deluge of modern threats, but designing such systems requires a distributed architecture to support development and testing. Several such architectures exist, but most only focus on providing a platform for running cyber security experiments as opposed to automating experiment processes. In response to this need, we have built a distributed framework based on software agents which can manage system roles, automate data collection, analyze results, and run new experiments without human intervention. The contribution of this work is the creation of a model for experiment automation and control in a distributed system environment.},
keywords={Computer security;Servers;Automation;Software agents;Computers;Data collection;Cyber security;malicious attack;one time password},
doi={10.1109/ICICES.2016.7518881},
ISSN={},
month={Feb},}
@INPROCEEDINGS{7515441,
author={Willnecker, Felix and Krcmar, Helmut},
booktitle={2016 12th International ACM SIGSOFT Conference on Quality of Software Architectures (QoSA)}, title={Optimization of Deployment Topologies for Distributed Enterprise Applications},
year={2016},
volume={},
number={},
pages={106-115},
abstract={Enterprise applications are typically implemented as distributed systems composed of several components. Deciding where to deploy which component is a difficult task that today is usually assisted by logical topology recommendations. Choosing inefficient topologies allocates the wrong amount of resources, leads to unnecessary operation costs, or results in poor performance. Testing different topologies to find good solutions takes a lot of time and might delay productive operations. Therefore, this work introduces a software based deployment topology optimization approach for distributed enterprise applications. We use an enhanced performance model generator that extracts models from running applications. The extracted model is used to simulate performance metrics (e.g., resource utilization, response times, throughput) of an enterprise application. Subsequently, we introduce a deployment topology optimizer, which selects an optimized topology for a specified workload. The following two optimization goals are presented in this work: (i) minimum response time for an optimized user experience and (ii) maximize resource utilization for cost-effective topologies. To evaluate the approach we use the SPECjEnterpriseNEXT industry benchmark as distributed enterprise application. The evaluation demonstrates the accuracy of the simulation compared to the actual deployment and the pre-eminence of the selected topology compared to runner-up topologies.},
keywords={Topology;Optimization;Containers;Load modeling;Network topology;Memory management;performance model generation;architecture optimization;enterprise applications;deployment topology optimization;memory management simulation},
doi={10.1109/QoSA.2016.11},
ISSN={},
month={April},}
@INPROCEEDINGS{7515609,
author={Pedroza, Gabriel and Le Gall, Pascale and Gaston, Christophe and Bersey, Fabrice},
booktitle={2016 IEEE 19th International Symposium on Real-Time Distributed Computing (ISORC)}, title={Timed-Model-Based Method for Security Analysis and Testing of Smart Grid Systems},
year={2016},
volume={},
number={},
pages={35-42},
abstract={The progressive integration of software-based components into the electricity grid has given raise to what is known as Smart Grids. As long as Smart Grids gain on connectivity and automation, new concerns on their safety and security have arisen. It is agreed that non-negligible risks and enlarged impact due to misbehaviors and intrusions exist. Following a model driven paradigm, a method is proposed to reinforce the security of these complex widely distributed systems. The method guides system re-engineering and is based upon timed models. It encompasses reverse engineering, symbolic, and testing techniques to model, analyze, and deploy attack testing. In early stages of the method, a reference timed model to support security analyses is designed via reverse engineering and symbolic execution. During latter stages, the nominal models are enriched so as to specify attack scenarios which are symbolically executed to prove the ability of the system to detect attacker intrusions. In final stages, the attack scenarios are used to specify test cases which are later deployed to test the system. The method and main outcomes are presented relying upon a Smart Grid subsystem analyzed in the scope of a joint academy-industry project.},
keywords={Smart grids;Security;Registers;Safety;Reverse engineering;Analytical models;Monitoring;model driven re-engineering;security testing;reverse engineering for modeling;model-based testing;symbolic analysis;timed UML model;smart grids},
doi={10.1109/ISORC.2016.15},
ISSN={2375-5261},
month={May},}
@INPROCEEDINGS{7516039,
author={Lai, Pan and Fan, Rui and Zhang, Wei and Liu, Fang},
booktitle={2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, title={Utility Maximizing Thread Assignment and Resource Allocation},
year={2016},
volume={},
number={},
pages={433-442},
abstract={Achieving high performance in many distributed systems, ranging from clouds to multicore processors, requires finding good assignments of threads to servers as well as effectively allocating each server's resources to its assigned threads. Both the assignment and allocation component of this problem have been studied extensively, though separately in the literature. In this paper, we introduce the assign and allocate (AA) problem, which seeks to simultaneously find an assignment and allocations that maximize the total utility of the threads. We first show that this problem is NP-hard, even when there are only two servers. We then present a 2(2√-1)>0.828 factor approximation algorithm, which runs in O(mn2 + n(logmC)2) time for n threads and m servers with C amount of resources each. We also present a faster algorithm with the same approximation ratio and O(n(logmC)2) running time. We conducted experiments to test the performance of our algorithm on different types of threads, and found that it achieves over 99% of the optimal utility on average. We also compared our algorithm against several other assignment and allocation algorithms, and found that it achieves up to 5.7 times better total utility.},
keywords={Distributed processing;Resource allocation;thread assignment;algorithms;distributed systems},
doi={10.1109/IPDPS.2016.82},
ISSN={1530-2075},
month={May},}
@INPROCEEDINGS{7500684,
author={Insaurralde, Carlos C.},
booktitle={2016 IEEE Aerospace Conference}, title={Remote integration of time-critical aerospace applications},
year={2016},
volume={},
number={},
pages={1-8},
abstract={Time-critical aerospace applications demand more and more large engineering teams. It also includes manufacturing fragmentation across geographically-dispersed locations. The integration required is usually for tightly-coupled hardware-software components/subsystems. Early system verifications/validation based on remote integration can significantly reduce costs and risks since tests can be remotely conducted until a physical integration takes place in a central location. The challenge is to allow fragmented system parts to work together as if they were co-located. This paper reviews information and communication technologies to remotely integrate time-critical applications deployed across distributed parts of complex systems. The review allows for analysis the real-time performance (mainly latency) from middleware candidates so that they can be compared. It also discusses the impact of such performance on time-critical control applications by showing results from a simple case study. The outcomes from this research establish interesting directions for further investigations to develop a framework for remote integration of distributed system management/development. This paper presents some example applications and a case study. Concluding remarks and future works is also presented.},
keywords={Computer architecture;Object oriented modeling;Time factors;Computational modeling;Service-oriented architecture},
doi={10.1109/AERO.2016.7500684},
ISSN={},
month={March},}
@INPROCEEDINGS{7473022,
author={Vögler, Michael and Schleicher, Johannes M. and Inzinger, Christian and Nickel, Bernhard and Dustdar, Schahram},
booktitle={2016 IEEE Symposium on Service-Oriented System Engineering (SOSE)}, title={Non-intrusive Monitoring of Stream Processing Applications},
year={2016},
volume={},
number={},
pages={162-171},
abstract={Stream processing applications have emerged as a popular way for implementing high-volume data processing tasks. In contrast to traditional data processing models that persist data to databases and then execute queries on the stored data, stream processing applications continuously execute complex queries on incoming data to produce timely results in reaction to events observed in the processed data. To cope with the request load, components of a stream processing application are usually distributed across multiple machines. In this context, performance monitoring and testing are naturally important for stakeholders to understand as well as analyze the runtime characteristics of deployed applications to identify issues and inform decisions. Existing approaches for monitoring the performance of distributed systems, however, do not provide sufficient support for targeted monitoring of stream processing applications, and require changes to the application code to enable the integration of application-specific monitoring data. In this paper we present MOSAIC, a service oriented framework that allows for in-depth analysis of stream processing applications by non-intrusively adding functionality for acquiring and publishing performance measurements at runtime, to the application. Furthermore, MOSAIC provides a flexible mechanism for integrating different stream processing frameworks, which can be used for executing and monitoring applications independent from a specific operator model. Additionally, our framework provides an extensible approach for gathering and analyzing measurement data. In order to evaluate our solution, we developed a scenario application, which we used for testing and monitoring its performance on different stream processing engines.},
keywords={Monitoring;Runtime;Engines;Weaving;Distributed databases;Measurement;Testing;non-intrusive monitoring;stream processing applications;service oriented framework;AOP},
doi={10.1109/SOSE.2016.11},
ISSN={},
month={March},}
@ARTICLE{7420638,
author={Carvajal, Gonzalo and Araneda, Luis and Wolf, Alejandro and Figueroa, Miguel and Fischmeister, Sebastian},
journal={IEEE Transactions on Industrial Informatics}, title={Integrating Dynamic-TDMA Communication Channels into COTS Ethernet Networks},
year={2016},
volume={12},
number={5},
pages={1806-1816},
abstract={Real-time Ethernet (RTE) is widely recognized for its potential to provide a unified communication backbone for next-generation heterogeneous distributed systems. However, most of the existing research in RTE technologies has traditionally focused on formal models and theoretical analyzes of timing properties, usually omitting the associated implementation challenges for testing them in practice. This gap between theory and practice prevents experimental validation of the claimed properties, which in turn hinders the pace of innovation and adoption of the technology in industrial settings. This paper aims at narrowing the theory-practice gap by characterizing a comprehensive open-source RTE framework that explores emerging challenges in real-time networking, including the provision of ultra-low latency and jitter, dynamic bandwidth management, and segmentation within large networks. This work integrates research on formal abstractions for dynamic time-division multiple access arbitration and technological insights from modern hardware infrastructure, and uses a representative distributed video processing application to provide reproducible evidence of the achieved properties in multihop Ethernet settings. By leveraging readily available technology and an open-source design, the proposed framework facilitates further exploration and experimental validation of properties that are beyond the scope of current commercial technologies, encouraging evidence-based discussions to accelerate development and adoption of new standards for next-generation industrial networks.},
keywords={Real-time systems;Schedules;Time division multiple access;Sensors;Open source software;Dynamic scheduling;Tunneling magnetoresistance;Dynamic time-division multiple access (TDMA);industrial networking;open-source platform;real-time Ethernet (RTE);state-based scheduling},
doi={10.1109/TII.2016.2535255},
ISSN={1941-0050},
month={Oct},}
@ARTICLE{7401140,
author={Hoque, Endadul and Lee, Hyojeong and Potharaju, Rahul and Killian, Charles and Nita-Rotaru, Cristina},
journal={IEEE/ACM Transactions on Networking}, title={Automated Adversarial Testing of Unmodified Wireless Routing Implementations},
year={2016},
volume={24},
number={6},
pages={3369-3382},
abstract={Numerous routing protocols have been designed and subjected to model checking and simulations. However, model checking the design or testing the simulator-based prototype of a protocol does not guarantee that the implementation is free of bugs and vulnerabilities. Testing implementations beyond their basic functionality (also known as adversarial testing) can increase protocol robustness. We focus on automated adversarial testing of real-world implementations of wireless routing protocols. In our previous work we created Turret, a platform that uses a network emulator and virtualization to test unmodified binaries of general distributed systems. Based on Turret, we create Turret-W designed specifically for wireless routing protocols. Turret-W includes new functionalities such as differentiating routing messages from data messages to enable evaluation of attacks on the control plane and the data plane separately, support for several additional protocols (e.g., those that use homogeneous/heterogenous packet formats, those that run on geographic forwarding (not just IP), those that operate at the data link layer instead of the network layer), support for several additional attacks (e.g., replay attacks) and for establishment of adversarial side-channels that allow for collusion. Turret-W can test not only general routing attacks, but also wireless specific attacks such as wormhole. Using Turret-W on publicly available implementations of five representative routing protocols, we (re-)discovered 37 attacks and 3 bugs. All these bugs and 5 of the total attacks were not previously reported to the best of our knowledge.},
keywords={Routing protocols;Wireless communication;Testing;Routing;Communication system security;Computer bugs;Automatic testing;routing protocols;security;wireless communication},
doi={10.1109/TNET.2016.2520474},
ISSN={1558-2566},
month={December},}
@INPROCEEDINGS{7582656,
author={Pourhaji, Soheil and Moattar, Mohammad Hossein},
booktitle={2015 International Congress on Technology, Communication and Knowledge (ICTCK)}, title={Developing a pieces of data allocation method in distributed databases using Bayesian networks},
year={2015},
volume={},
number={},
pages={117-122},
abstract={Determining the location of the storage of data on different nodes of a distributed system is called allocation of pieces of data and is considered as the most important factor influencing the cost of the implementation of distributed database. Allocation of piece of data is a NP-hard problem that needs heuristic solutions. The main purpose of the process is to allocate parts with the lowest cost to the node that has the most access to the data segment. In this article, in terms of a structural oriented learning method, using analysis of the factors influencing the process of allocating a piece of data to a node, (i.e. distance from apiece of data, cost, node loading, availability of data, etc), the dependence relations between adjacent nodes to target nodes was determined and the structure of Bayesian network was obtained. In the second stage, the obtained Bayesian network is trained using gradient method and tested using the data collected from the data collection x. The proposed model efficiency is compared with the results of Huang and Chen's method which have used a heuristic approach using the imperialist competitive algorithm. The results indicate that this model is an efficient tool in the optimization process of allocating Pieces of Data in the distributed systems.},
keywords={Bayes methods;Distributed databases;Resource management;Heuristic algorithms;Bandwidth;Maximum likelihood estimation;Data collection;Distributed Databases;Allocation of Pieces of Data;Bayesian Networks;Structured Learning},
doi={10.1109/ICTCK.2015.7582656},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7576572,
author={Muzi, Francesco and Gimenez De Lorenzo, Mario and De Gasperis, Giovanni},
booktitle={2015 17th UKSim-AMSS International Conference on Modelling and Simulation (UKSim)}, title={A Predictive Model for the Automated Management of Conditioning Systems in Smart Buildings},
year={2015},
volume={},
number={},
pages={380-385},
abstract={The paper describes a technological basis for dynamic actions targeted to an effective, real-time control of air conditioning systems in smart buildings with a focus on energy management. The proposed procedure could be extended to more complex systems, usually including a number of prosumer (producer and consumer) nodes, connected to a smart grid and remotely controlled by a Distributed System Operator (DSO) in distributed control and monitoring systems. Accurate, continuously-recorded local weather data are then used to make decisions aimed at both reducing energy consumption and assuring pre-established comfort levels. The amount of saved energy can be estimated by observing a building's energy performance under the action of different meteorological agents through data mining and machine learning methods. Moreover, some possible advantages from real-time exploitation of a building's thermal inertia are shown. The proposed on-line management was also validated through laboratory experimental tests, whose results are reported and discussed.},
keywords={Temperature measurement;Buildings;Predictive models;Temperature sensors;Load modeling;Air conditioning;Monitoring;building automation;smart grids;load modelling;estimation and forecast;data mining;prosumer node},
doi={10.1109/UKSim.2015.22},
ISSN={},
month={March},}
@INPROCEEDINGS{7521134,
author={Lima, Bruno and Faria, João Pascoal},
booktitle={2015 10th International Joint Conference on Software Technologies (ICSOFT)}, title={An approach for automated scenario-based testing of distributed and heterogeneous systems},
year={2015},
volume={1},
number={},
pages={1-10},
abstract={The growing dependence of our society on increasingly complex software systems, makes software testing ever more important and challenging. In many domains, such as healthcare and transportation, several independent systems, forming a heterogeneous and distributed system of systems, are involved in the provisioning of end-to-end services to users. However, existing testing techniques, namely in the model-based testing field, provide little tool support for properly testing such systems. Hence, in this paper, we propose an approach and a toolset architecture for automating the testing of end-to-end services in distributed and heterogeneous systems. The tester interacts with a visual modeling frontend to describe key behavioral scenarios, invoke test generation and execution, and visualize test results and coverage information back in the model. The visual modeling notation is converted to a formal notation amenable for runtime interpretation in the backend. A distributed test monitoring and control infrastructure is responsible for interacting with the components of the system under test, as test driver, monitor and stub. At the core of the toolset, a test execution engine coordinates test execution and checks the conformance of the observed execution trace with the expectations derived from the visual model. A real world example from the Ambient Assisted Living domain is presented to illustrate the approach.},
keywords={Unified modeling language;Testing;Visualization;Adaptation models;Monitoring;Automation;Object oriented modeling;Software testing;Distributed systems;Scenario-based Testing;Heterogeneous Systems;Systems of Systems},
doi={},
ISSN={},
month={July},}
@INPROCEEDINGS{7489343,
author={Jadhav, Sachin and Suryawanshi, Shubhangi},
booktitle={2015 International Conference on Information Processing (ICIP)}, title={Knowledge acquisition using parallel rough set and mapreduce from big data},
year={2015},
volume={},
number={},
pages={16-20},
abstract={These days, the volume of information is developing at an uncommon rate, enormous information mining, and learning revelation have turned into another test in the time of information mining and machine learning. Large set hypothesis for learning procurement has been effectively connected in information mining. The Map Reduce strategy got more consideration from academic group and also industry for its pertinence in unstructured huge information examination. Clusters are viably utilized for parallel handling application and obtained information speak to into different groups. In this venture we have introduced working and execution stream of the Map Reduce programming ideal model with map and reduce capacity and unpleasant set hypothesis. In this work we have design distributed system. Likewise quickly talk about diverse issues and difficulties that are confronted by Map Reduce while taking care of the huge data. Also, finally we have introduced a few focal points of the Map reduce Programming model.},
keywords={Rough sets;Google;Yarn;Adaptation models;Programming profession;Knowledge acquisition;Big Data;Big Data Analytic;Knowledge Acquisition;Map Reduce;Rough Sets},
doi={10.1109/INFOP.2015.7489343},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7443743,
author={Das, Aurojyoti and Yaswanth, Tavva},
booktitle={2015 Annual IEEE India Conference (INDICON)}, title={A low-cost, portable alternative for a digital Lock-In Amplifier using TMS320C5535 DSP},
year={2015},
volume={},
number={},
pages={1-4},
abstract={A Lock-In Amplifier (LIA) is used to remove noise from an input signal by the use of a reference signal. Generally the sampling rate of LIAs is much higher than the required frequency in typical applications. Moreover the hardware configuration of LIAs is much better than the requirement. The signals are processed using high-end signal processors which increase the cost of LIA. This paper aims to develop an inexpensive and portable alternative for the digital lock-in amplifier. This will enable application of LIAs in remote locations and in distributed systems which require multiple signal processors simultaneously. Similar implementation exists on microcontrollers and embedded processors. This paper utilizes a Texas Instruments (TI) TMS320 series Digital Signal Processor (DSP) which is a low-power signal processor. The lock-in amplifier is implemented on a TMS320C5535 eZDSP board. Unlike other LIAs the reference signal is generated in the DSP, hence obviating the requirement for an external reference input. The output is obtained through UART interface and observed on a computer. It is tested with a noisy sinusoidal signal given as input and the output signal is plotted and verified.},
keywords={Digital signal processing;Ports (Computers);Amplifiers;Frequency modulation;Hardware;Codecs;Program processors;DSP;C5535;eZDSP;Lock-In Amplifier;TMS320},
doi={10.1109/INDICON.2015.7443743},
ISSN={2325-9418},
month={Dec},}
@INPROCEEDINGS{7441065,
author={Blaszczyńska, Marzena and Mazurek, Cezary and Pawałowski, Piotr and Szymaniak, Piotr and Zieliński, Sergiusz},
booktitle={eChallenges e-2015 Conference}, title={medVC — A remote collaboration solution enhanced with cloud services and future internet capacities},
year={2015},
volume={},
number={},
pages={1-9},
abstract={This paper describes medVC, a scalable telepresence platform designed to be deployed in hospitals to enable real-time audio-video communication and remote collaboration of doctors. Furthermore, the paper treats of CloudCONFetti, an experiment aiming to extend medVC with Future Internet capacities and to put it to trial in federated testbed facilities. The enhancement will be based on an innovative system for intelligent routing of videoconferencing streams by a distributed system of packet reflectors. The Fed4FIRE testbed federation was chosen performing the trials because previous experiences have shown that there are many benefits to performing evaluation using testbeds through a set of common tools. The complete and tested solution will be offered on the commercial market as there already are first potential customers in European healthcare markets.},
keywords={Streaming media;Teleconferencing;Three-dimensional displays;Collaboration;Real-time systems;High definition video;Hospitals},
doi={10.1109/eCHALLENGES.2015.7441065},
ISSN={2166-1677},
month={Nov},}
@INPROCEEDINGS{7430116,
author={Marroquin, Alberto and Gonzalez, Douglas and Maag, Stephane},
booktitle={2015 7th IEEE Latin-American Conference on Communications (LATINCOM)}, title={Testing distributed systems with test cases dependencies architecture},
year={2015},
volume={},
number={},
pages={1-6},
abstract={In this work, we present a novel distributed testing architecture based on a formal definition of test cases dependencies to test the conformance of distributed systems in a black box context. Utilizing the European Telecommunication Standards Institute, Test Description Language standard, we apply our approach to a real Internet Multimedia Subsystem (IMS)/ SIP (Session Initiation Protocol) test bed and perform the tests through two use cases. This crucial activity belongs to the conformance testing context. Which aims at stimulating the communication system under test (SUT) to detect errors and unexpected behaviors with regards to the standards. When handling distributed systems, a major difficulty arises when testing these, due to the joint and linked stimulation of distributed entities. The main reason for it is the correlation of verdicts obtained from these entities.},
keywords={Testing;Protocols;Standards;Context;Synchronization;Correlation;Conformance Testing;Distributed Systems;Test Cases;SIP;TDL},
doi={10.1109/LATINCOM.2015.7430116},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7402379,
author={Liu, Qin and Gross, Joseph and Werner, Herbert},
booktitle={2015 54th IEEE Conference on Decision and Control (CDC)}, title={Distributed identification of spatially-distributed systems based on finite element modelling},
year={2015},
volume={},
number={},
pages={1230-1235},
abstract={In structural engineering, the application of the finite element (FE) method to model a spatially-distributed system exploits physical insight into the plant dynamics and often results in a lumped model of large order, which is undesirable or even impossible for model-based controller design. This paper bridges the gap between FE modelling and distributed identification by proposing a novel identification technique to extract a distributed model from an FE model for both parameter-invariant and parameter-varying systems. The obtained models define the system dynamics on subsystems and capture the interacted behaviour among neighbouring sub-systems. The proposed approach is tested on an experimental beam structure equipped with an array of piezo actuators and sensors, and the model accuracy is compared to that of black-box identification.},
keywords={Iron;Sensors;Mathematical model;Actuators;Analytical models;Computational modeling;Resonant frequency},
doi={10.1109/CDC.2015.7402379},
ISSN={},
month={Dec},}
@INPROCEEDINGS{7388571,
author={Vaskova, A. and Portela-García, M. and López-Ongil, C. and Sánchez, E. and Reorda, M. Sonza},
booktitle={2015 Conference on Design of Circuits and Integrated Systems (DCIS)}, title={About the functional test of permanent faults in distributed systems},
year={2015},
volume={},
number={},
pages={1-6},
abstract={The effects of permanent faults, arising along working life of digital electronic systems, may impact their reliability and performance. In-field test may help to detect these faults and to prevent serious effects in safety-critical applications. Distributed electronic systems introduce further complexity in this scenario, as the low observability and the lack of maintenance make difficult the detection as well as the identification of failing elements and their repairing. Functional workloads are often used for on-line tests of distributed systems to detect permanent faults. Suitable techniques for test generation and early identification of functionally untestable permanent faults are critical issues that are faced in this work.},
keywords={Circuit faults;Fault diagnosis;Solid modeling;Testing;Hardware;Clocks;Permanent Faults;LIN Bus;On-line Testing;Graceful Degradation},
doi={10.1109/DCIS.2015.7388571},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7371720,
author={Anceaume, Emmanuelle and Castella, François and Mostéfaoui, Achour and Sericola, Bruno},
booktitle={2015 IEEE 14th International Symposium on Network Computing and Applications}, title={A Message-Passing and Adaptive Implementation of the Randomized Test-and-Set Object},
year={2015},
volume={},
number={},
pages={167-175},
abstract={This paper presents a solution to the well-known Test-and-Set operation in asynchronous systems prone to process crashes. Test-and-Set is a synchronization operation that, when invoked by a set of processes, returns "yes" to a unique process and returns "no" to all the others. Recently many advances in implementing Test and Set objects have been achieved, however all of them uniquely target the shared memory model. In this paper we propose an implementation of a Test-and-Set object for message passing distributed systems. This implementation can be invoked by any number p of processes. It has an expected step complexity in O(p) and an expected message complexity in O(np), where n is the total number of processes in the system. The proposed Test and Set object is built atop a new basic building block that allows to select a winning group among two groups of processes.},
keywords={Complexity theory;Computer crashes;Registers;Adaptation models;Synchronization;Protocols;Memory management;Test-and-Set;synchronization;asynchronous message-passing system;crash failures;randomized algorithm},
doi={10.1109/NCA.2015.27},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7371487,
author={Wang, Xuefei and Ma, Hengtai and Yang, Ke and Liang, Hongliang},
booktitle={2015 IEEE 2nd International Conference on Cyber Security and Cloud Computing}, title={An Uneven Distributed System for Dynamic Taint Analysis Framework},
year={2015},
volume={},
number={},
pages={237-240},
abstract={Dynamic taint analysis has been widely used in software testing, debugging, vulnerability detection and other fields. A popular idea is that we can combine dynamic taint analysis with symbolic execution techniques or fuzz techniques forming the testing framework to test automatically. When testing large applications which costs longer time, a distributed system can be very practical. However, the common distributed system is load balancing which distributes tasks without considering the various performance of each machine, resulting that some machines with poor configuration will burden too much load. In this paper, we present an uneven distributed system, which splits the dynamic taint analysis framework into some modules, and then distributes the modules to different machines classified by their performance. The design and distribution method are all based on the feature of each module. In the studies, we applied the system to test 5 applications compared with the load balancing distributed system, and the results shows it can indeed distribute tasks uneven according to different performance.},
keywords={Servers;Load management;Performance analysis;Amplitude shift keying;Testing;Heuristic algorithms;Algorithm design and analysis;uneven distribution;distributed systems;dynamic taint analysis},
doi={10.1109/CSCloud.2015.20},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7368740,
author={Douas, Bilal},
booktitle={2015 First International Conference on New Technologies of Information and Communication (NTIC)}, title={VoIP traffic support in mobile ad hoc networks},
year={2015},
volume={},
number={},
pages={1-6},
abstract={Mobile ad hoc networks (MANETs) are temporary networks which are formed for a specific purpose. They are infrastructureless, autonomous and entirely distributed systems where different wireless mobile nodes are capable of arbitrary movement and power switching without breaking the network connectivity since it is physically possible. In these networks, nodes must operate both as hosts and as routers. Their use is suited when it is impossible or undesirable to use fixed infrastructures. The flexibility offered by ad hoc mobile networks drive researchers to investigate the possibilities of allowing real-time multimedia applications on such networks. However, support for such demanding applications requires quality of service (QoS) guarantees especially regarding latency and losses while those networks are very constrained and poor comparing to conventional networks. Accordingly, this work aims at investigating MANETs performance with VoIP applications through simulation (ns2) in an IEEE 802.11/802.11b based network. We also show through it the important optimization that can be obtained from early dropping of expiring packets in the network. We choose VoIP for our tests as a real-time multimedia application which is having a big growth these last years.},
keywords={Ad hoc networks;Delays;Mobile computing;IEEE 802.11 Standard;Codecs;GSM;MANET;VoIP;QoS;802.11;802.11b},
doi={10.1109/NTIC.2015.7368740},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7356461,
author={Troshynski, Troy},
booktitle={2015 IEEE AUTOTESTCON}, title={High speed Shared Memory Networks in hardware in the loop applications},
year={2015},
volume={},
number={},
pages={27-32},
abstract={As aircraft avionics systems become more and more integrated and complex, the overall costs and duration of system verification and validation efforts continue to increase and contribute to the rising costs of new system design. In order to control costs, reduce schedules, and improve overall quality, Distributed Hardware-in-the-Loop (HIL) simulations are increasingly being used to verify integrated modular avionics systems. As these systems become increasingly complex, the distributed system must interface with an ever increasing number of aircraft interfaces, sensors, and actuators which often drives the requirement for a distributed, real-time processing architecture for the HIL system. This paper provides a brief technical overview of distributed HIL test systems and also explores the use of a high speed Shared Memory Network for data sharing and time synchronization between real time processors in a distributed HIL test system.},
keywords={Cable TV;Hardware;Synchronization;Redundancy;Physical layer;Optical receivers},
doi={10.1109/AUTEST.2015.7356461},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7341450,
author={Byvalkevich, Roman V. and Radchenko, Serhii P. and Sudakov, Olexandr O.},
booktitle={2015 IEEE 8th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)}, title={Tools for ultrasonic diagnostic image classification},
year={2015},
volume={2},
number={},
pages={977-981},
abstract={Classification tool based on the Kohonen artificial neural network was designed and implemented. The tool attended to be used at high performance computing clusters and distributed systems like computing Grids. Proposed tools were applied to classification of ultrasonic medical diagnostic data. Testing of the classifying system showed more than 90% recognition accuracy for certain diagnoses.},
keywords={Neurons;Acoustics;Glands;Algorithm design and analysis;Feature extraction;Distributed databases;Classification algorithms;Kohonen neural network;clustering;ultrasonic image;texture;statistical characteristics},
doi={10.1109/IDAACS.2015.7341450},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7330524,
author={Schaum, Alexander and Moreno, Jaime A. and Alvarez, Jesus and Meurer, Thomas},
booktitle={2015 European Control Conference (ECC)}, title={A simple observer scheme for a class of 1-D semi-linear parabolic distributed parameter systems},
year={2015},
volume={},
number={},
pages={49-54},
abstract={In this paper, a simple observer design scheme is proposed for a class of 1-D semi-linear parabolic distributed parameter systems with in-domain measurement. The main idea of the design resides in imposing the measurement on the observer dynamics in a similar way as it is done in the finite-dimensional reduced order observer design. For the distributed system this is achieved through an additional algebraic constraint in the domain instead of an output injection mechanism. Correspondingly, the observer convergence is not induced by the correction scheme, but by the sensor location and the system parameters, and thus amounts on the underlying detectability property rather than the system observability. Possible applications to the case of unknown inputs are discussed. The proposed estimation approach is illustrated and tested through simulations with two representative examples: an unstable diffusive linear system with Dirichlet boundary conditions, and a stable isothermal single-species nonlinear tubular reactor with non-monotonic kinetics and Robin-Newman (i.e., Danckwerts) boundary conditions.},
keywords={Observers;Eigenvalues and eigenfunctions;Boundary conditions;Convergence;Mathematical model;Distributed parameter systems;Inductors},
doi={10.1109/ECC.2015.7330524},
ISSN={},
month={July},}
@ARTICLE{7302580,
author={De Angelis, Alessio and Dionigi, Marco and Moschitta, Antonio and Carbone, Paolo and Sisinni, Emiliano and Ferrari, Paolo and Flammini, Alessandra and Rinaldi, Stefano},
journal={IEEE Transactions on Instrumentation and Measurement}, title={On the Use of Magnetically Coupled Resonators for Chirp-Based Timestamping},
year={2015},
volume={64},
number={12},
pages={3536-3544},
abstract={The knowledge of the time value at which an event of interest occurred is the typical feature needed to perform many kinds of postprocessing in distributed sensor networks. Consequently, network-based distributed systems must support time dissemination in order to ensure that relative clock errors among nodes remain below a certain threshold. Despite many protocols being well established for wired connections, the wireless sensor network paradigm required a new approach to address adverse characteristics of the wireless medium. In this paper, the adoption of low-frequency signaling is proposed for overcoming some issues of radio frequency transmissions when used for timestamping events. The proposed approach is characterized using numerical simulations and an experimental prototype, based on inductive coupling of tuned resonators. Experimental tests confirm the effectiveness and feasibility of this approach that achieves microsecond-order time recovery performance in practical system configurations.},
keywords={Chirp;Synchronization;Jitter;Numerical simulation;Wireless sensor networks;Magnetic coupling;synchronization;time dissemination;timestamping;wireless sensor networks (WSNs).;Magnetic coupling;synchronization;time dissemination;timestamping;wireless sensor networks (WSNs)},
doi={10.1109/TIM.2015.2463332},
ISSN={1557-9662},
month={Dec},}
@INPROCEEDINGS{7266965,
author={Frömmgen, Alexander and Rehner, Robert and Lehn, Max and Buchmann, Alejandro},
booktitle={2015 IEEE International Conference on Autonomic Computing}, title={Fossa: Learning ECA Rules for Adaptive Distributed Systems},
year={2015},
volume={},
number={},
pages={207-210},
abstract={The development of adaptive distributed systems is complex. Due to a large amount of interdependencies and feedback loops between network nodes and software components, distributed systems respond nonlinearly to changes in the environment and system adaptations. Although Event Condition Action (ECA) rules allow a crisp definition of the adaptive behavior and a loose coupling with the actual system implementation, defining concrete rules is nontrivial. It requires specifying the events and conditions which trigger adaptations, as well as the selection of appropriate actions leading to suitable new configurations. In this paper, we present the idea of Fossa, an ECA framework for adaptive distributed systems. Following a methodology that separates the adaptation logic from the actual application implementation, we propose learning ECA rules by automatically executing a multitude of tests. Rule sets are generated by algorithms such as genetic programming, and the results are evaluated using a utility function provided by the developer. Fossa therefore provides an automated offline learner that derives suitable ECA rules for a given utility function.},
keywords={Adaptive systems;Adaptation models;Genetic programming;Engines;Monitoring;Computational modeling;Object oriented modeling},
doi={10.1109/ICAC.2015.37},
ISSN={},
month={July},}
@INPROCEEDINGS{7256936,
author={Carneiro, Sávio Mota and da Silva, Thiago A. R. and de A. L. Rabêlo, Ricardo and Silveira, Francisca R. V. and de Campos, Gustavo A. L.},
booktitle={2015 IEEE Congress on Evolutionary Computation (CEC)}, title={Artificial Immune Systems in intelligent agents test},
year={2015},
volume={},
number={},
pages={536-543},
abstract={Intelligent agents consist in a promising computing technology for the development of complex distributed systems. Despite the available theoretical references for guiding the designer of these agents, there are few proposed testing techniques to validate these systems. It's known that this validation depends on all the selected test cases, which should provide information regarding the components in the structure of the agent that show unsatisfactory performance. This article presents the application of Artificial Immune Systems (AIS), through Clonal Selection Algorithm (CLONALG), for the problem of optimization of selection of test cases for testing computing systems that are based on intelligent agents. In order to validate the use of CLONALG, comparisons between the Genetic Algorithms (GA) and Ant Colony Optimization Algorithms (ACO) techniques were performed. In the experiments with the approach testing intelligent agents with different types of architecture in partially and completely observable environments, the approach selected a group of satisfactory test cases in terms of the generated information about the irregular performance of the agent. From this result, the approach enables the identification of problematic episodes, allowing the designer to make objective changes in the internal structure of the agent in such a way to improve its performance.},
keywords={Sociology;Statistics;Cloning;Genetic algorithms;Intelligent agents;Search problems;Testing},
doi={10.1109/CEC.2015.7256936},
ISSN={1941-0026},
month={May},}
@INPROCEEDINGS{7250474,
author={Anza, Luis E. and Ferreira, Enrique D. and Suárez, Hamlet E.},
booktitle={2015 IEEE 6th Latin American Symposium on Circuits Systems (LASCAS)}, title={Design and construction of a prototype system for gait analysis for research in subjects with balance problems},
year={2015},
volume={},
number={},
pages={1-4},
abstract={This work presents the design and construction of a prototype of a distributed system for gait analysis with the aim in helping diagnosis and rehabilitation treatment of elderly population with balance disorders. A suitable set of requirements for the recording and subsequent analysis of the human gait parameters were defined. The prototype should be able to record the movements of the body where it is placed, transmit the acquired data and work with complete autonomy during the time it takes to make records. The prototype was built on a PCB of reduced weight and dimensions. It was tested and compared with a commercial device and the results were satisfactory.},
keywords={Prototypes;Batteries;IEEE 802.11 Standard;Sensors;Servers;Acceleration;Angular velocity},
doi={10.1109/LASCAS.2015.7250474},
ISSN={},
month={Feb},}
@INPROCEEDINGS{7232391,
author={Jakus, Damir and Vasilj, Josip and Sarajcev, Petar},
booktitle={2015 IEEE Eindhoven PowerTech}, title={Voltage control in MV distribution networks through coordinated control of tap changers and renewable energy sources},
year={2015},
volume={},
number={},
pages={1-6},
abstract={The significant penetration of renewable energy sources (RES), like wind and solar power plants, into distribution networks has changed their operation considerably and brought new challenges for distributed system operators (DSOs). One of the main problems associated with high penetration of variable RES into distribution networks is voltage rise problem along the radial distribution feeders, with high production variability causing frequent operations of the onload tap changers (OLTC), thereby causing significant deterioration of their operation life expectancy. This paper proposes new method for the Volt Var control in the MV distribution network with the high share of RES relative to the distribution system load. The problem is casted as the mixedinteger nonlinear programming (MINLP) problem associated with the voltage control in the distribution network with the objective to maximize daily RES production and through reactive power control to minimize the daily distribution system losses using the Benders' decomposition method. In order to reduce the number of OLTC switching operations, the time instances in which the change is possible are set a priori. The effectiveness of the proposed method is tested on well-known IEEE 69-bus system.},
keywords={Reactive power;Voltage control;Production;Linear programming;Power generation;Reactive power control;Loading;Distribution Networks;Renewable Energy Sources;Voltage Control;Tap Changer;Bender's Decomposition},
doi={10.1109/PTC.2015.7232391},
ISSN={},
month={June},}
@INPROCEEDINGS{7194328,
author={Messina, Fabrizio and Mikkilineni, Rao and Morana, Giovanni},
booktitle={2015 IEEE 24th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises}, title={CDCGM 2015 Track Report: Convergence of Distributed Clouds, Grids and their Management},
year={2015},
volume={},
number={},
pages={47-50},
abstract={This track started with the first Cloud Computing session in WETICE2009 with the observation that the cloud computing evolution depends on research efforts from the infrastructure providers creating next generation hardware that is service friendly, service developers that embed business service intelligence in the network to create distributed business workflow execution, assure service delivery on a massive scale with global interoperability while dealing with non-functional requirements such as security, availability, performance, compliance, cost and fluctuations both in resources and workloads. It was pointed out that the architecture and evolution of the cloud is increasing data enter complexity by piling up new layers of management over the many layers that already exist. Current session proves the epigram by Jean-Baptiste (1849) "plus ça change, plus c'est la mêeme chose." Literally "The more it changes, the more it's the same thing". In this conference, one paper presents the result of the discussions started in these sessions in 2009 that led to a policy based dynamic workflow orchestration independent of the infrastructure orchestration which eliminates the need for moving Virtual Machine images and interfaces to myriad infrastructure management systems at runtime. The architecture is derived from the well-understood and time-tested distributed systems such as cellular organisms, human organizational structures and telecommunication networks. In addition, eight full papers and three short papers continue to make progress on current state of the art. Based on the papers presented in these sessions over the last six years, we boldly predict that we are on the verge of a synthesis of the thesis of current state of the art and the anti-thesis of increasing complexity to address scaling and fluctuations in distributed systems.},
keywords={Cloud computing;Computer architecture;Business;Complexity theory;Quality of service;Security;Cloud Computing;grid computing;Distributed Intelligent Managed Element Networks;Distributed Services Management;Services Virtualization;Parallel Computing},
doi={10.1109/WETICE.2015.40},
ISSN={1524-4547},
month={June},}
@INPROCEEDINGS{7193148,
author={Purnimaa G.K and Devi, D. Gayathri},
booktitle={2015 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS)}, title={An analysis of SDL based interoperability and performance testing using CSMA protocol in wireless network},
year={2015},
volume={},
number={},
pages={1-5},
abstract={Specification and Description Language based Interoperability and Performance Testing using CSMA protocol in wireless networks is used to achieve the maximal throughput in wireless communication software application. Specification and Description Language (SDL) is a specification language targeted at the unambiguous specification and description of the behavior of reactive and distributed systems. Performance testing is performed to determine how a system performs in terms of responsiveness and stability under a particular workload. The important performance parameters are message response time, throughput, reliability and cost. The lack of network traffic is the main problem in the wireless network, to rectify that the CSMA protocol is used. Carrier Sense Multiple Access protocol helps to improve the performance of each network path. The performance of the network traffic will be calculated by throughput.},
keywords={Multiaccess communication;Throughput;Protocols;Wireless networks;Measurement;Testing;Validation;Performance;SDL;CSMA;MSCs},
doi={10.1109/ICIIECS.2015.7193148},
ISSN={},
month={March},}
@INPROCEEDINGS{7168172,
author={Zhang, Yue and Zhang, Zhiliang and Yang, Gang},
booktitle={2015 9th International Conference on Power Electronics and ECCE Asia (ICPE-ECCE Asia)}, title={A novel control method for photovoltaic grid-connected micro-inverters to achieve high efficiency in light load},
year={2015},
volume={},
number={},
pages={2826-2831},
abstract={Boundary Conduction Mode (BCM) and Discontinuous Conduction Mode (DCM) control strategies are widely used for the photovoltaic micro-inverter. For the interleaved flyback micro-inverter, BCM and DCM control strategies are investigated concentrating on the loss analysis under different load condition. The dominant losses with heavy load include the conduction loss of the MOSFETs and diodes, and the core loss and copper loss of the transformer, while the dominant losses with light load include the driving loss, turn-off loss of the MOSFETs and the transformer core loss. Based on the loss analysis, a new Multi-mode control strategy combing the two-phase DCM, one-phase DCM and phase skipping mode control is proposed to improve the efficiency in wide load range without increasing the cost or complexity of the design by reducing the dominant losses depending on the load current. A distributed system prototype with two micro-inverters in parallel has been built and tested. The experimental results verified the effectiveness of the proposed method.},
keywords={Capacitors;Mathematical model;Inverters;Integrated circuit modeling;Logic gates;Digital signal processing;Photovoltaic systems;Photovoltaic;Multi-mode control;Phase skipping mode},
doi={10.1109/ICPE.2015.7168172},
ISSN={2150-6086},
month={June},}
@INPROCEEDINGS{7161537,
author={Mostafa, Menna and Bonakdarpour, Borzoo},
booktitle={2015 IEEE International Parallel and Distributed Processing Symposium}, title={Decentralized Runtime Verification of LTL Specifications in Distributed Systems},
year={2015},
volume={},
number={},
pages={494-503},
abstract={Runtime verification is a lightweight automated formal method for specification-based runtime monitoring as well as testing of large real-world systems. While numerous techniques exist for runtime verification of sequential programs, there has been very little work on specification-based monitoring of distributed systems. In this paper, we propose the first sound and complete method for runtime verification of asynchronous distributed programs for the 3-valued semantics of LTL specifications defined over the global state of the program. Our technique for evaluating LTL properties is inspired by distributed computation slicing, an approach for abstracting distributed computations with respect to a given predicate. Our monitoring technique is fully decentralized in that each process in the distributed program under inspection maintains a replica of the monitor automaton. Each monitor may maintain a set of possible verification verdicts based upon existence of concurrent events. Our experiments on runtime monitoring of a simulated swarm of flying drones show that due to the design of our Algorithm, monitoring overhead grows only in the linear order of the number of processes and events that need to be monitored.},
keywords={Monitoring;Automata;Runtime;Lattices;Algorithm design and analysis;Semantics;Iron;Runtime monitoring;Asynchronous distributed systems;Formal methods},
doi={10.1109/IPDPS.2015.95},
ISSN={1530-2075},
month={May},}
@INPROCEEDINGS{7161574,
author={Tomsic, Alejandro and Sens, Pierre and Garcia, João and Arantes, Luciana and Sopena, Julien},
booktitle={2015 IEEE International Parallel and Distributed Processing Symposium}, title={2W-FD: A Failure Detector Algorithm with QoS},
year={2015},
volume={},
number={},
pages={885-893},
abstract={Failure detection plays a central role in the engineering of distributed systems. Furthermore, many applications have timing constraints and require failure detectors that provide quality of service (QoS) with some quantitative timeliness guarantees. Therefore, they need failure detectors that are fast and accurate. We introduce the Two-Windows Failure Detector (2W-FD), an algorithm able to react to sudden changes in network conditions, property that currently existing algorithms do not satisfy. We ran tests on real traces and compared the 2W-FD to state-of-art algorithms. Our results show that our algorithm presents the best performance in terms of speed and accuracy in unstable scenarios.},
keywords={Detectors;Heart beat;Quality of service;Estimation;Computers;Delays;Failure Detectors;Quality of Service;Fault Tolerance;Distributed Algorithms;Reliability;Quiescence},
doi={10.1109/IPDPS.2015.74},
ISSN={1530-2075},
month={May},}
@INPROCEEDINGS{7158101,
author={Luzuriaga, Jorge E. and Perez, Miguel and Boronat, Pablo and Cano, Juan Carlos and Calafate, Carlos and Manzoni, Pietro},
booktitle={2015 12th Annual IEEE Consumer Communications and Networking Conference (CCNC)}, title={A comparative evaluation of AMQP and MQTT protocols over unstable and mobile networks},
year={2015},
volume={},
number={},
pages={931-936},
abstract={Message oriented middleware (MOM) refers to the software infrastructure supporting sending and receiving messages between distributed systems. AMQP and MQTT are the two most relevant protocols in this context. They are extensively used for exchanging messages since they provide an abstraction of the different participating system entities, alleviating their coordination and simplifying the communication programming details. These protocols, however, have not been thoroughly tested in the context of mobile or dynamic networks like vehicular networks. In this paper we present an experimental evaluation of both protocols in such scenarios, characterizing their behavior in terms of message loss, latency, jitter and saturation boundary values. Based on the results obtained, we provide criteria of applicability of these protocols, and we assess their performance and viability. This evaluation is of interest for the upcoming applications of MOM, especially to systems related to the Internet of Things.},
keywords={Protocols;Jitter;Mobile communication;Mobile computing;Production;Security;Method of moments},
doi={10.1109/CCNC.2015.7158101},
ISSN={2331-9860},
month={Jan},}
@INPROCEEDINGS{7154882,
author={Husain, Moula and Meena S M and Sabarad, Akash K and Hebballi, Harish and Nagaralli, Shiddu M and Shetty, Sonal},
booktitle={2015 IEEE International Advance Computing Conference (IACC)}, title={Counting occurrences of textual words in lecture video frames using Apache Hadoop Framework},
year={2015},
volume={},
number={},
pages={1144-1147},
abstract={In recent years, on-line lecture videos are becoming significant pedagogical tool for both course instructors and students. Text present in lecture video will act as an important modality for retrieving videos as it is closely related to its content. In this paper, we present a distributed system for counting occurrences of each textual word from video frames using Apache Hadoop framework. As Hadoop framework is suitable for batch processing operations and, the processing of images is highly concurrent, we can implement batch processing operation of reading text information and counting the occurrence of each word by using MapReduce framework. We tested the working of text recognition and word count algorithms on Hadoop framework of cluster size 1, 5 and 10 nodes. Also we compared the performances of multimode clusters with a single node machine. On a data set of size around 3GB lecture video frames, Hadoop with a cluster size of 10 nodes executes 5 times faster than a single node system. Our results prove the advantage of using Hadoop for improving computational speed of processing image and video processing applications.},
keywords={Clustering algorithms;Hardware;Electronic mail;Character recognition;Optical character recognition software;Feature extraction;Biomedical imaging;Hadoop;HDFS;Namenode;Datanode;Job-Tracker;TaskTracker;MapReduce;OCR},
doi={10.1109/IADCC.2015.7154882},
ISSN={},
month={June},}
@INPROCEEDINGS{7152605,
author={Srinivasan, Lakshminarayanan and Varma, Vasudeva},
booktitle={2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing}, title={Adaptive Load-Balancing for Consistent Hashing in Heterogeneous Clusters},
year={2015},
volume={},
number={},
pages={1135-1138},
abstract={In a distributed system, there is always a question of how to distribute any data across the constituent systems. Consistent hashing is often used to get this system mapping based on a hash function and the data. Hence it becomes solely responsible for the distribution of the workload across the different systems. The default consistent hashing partitioning scheme doesn't factor in the load distribution or the heterogeneity of the cluster (in terms of disk speed, power, network, load, etc.) and hence creates performance bottlenecks. This paper proposes an adaptive partitioning scheme for consistent hashing that factors in the heterogeneity of the different systems and also dynamically adapts to the changing workload resulting in better performance of the system. A prototype was built to test the performance and the system was 20% faster in the benchmark compared to the regular consistent hashing apart from better load distribution across the systems.},
keywords={Servers;Load management;Adaptive systems;Prototypes;Thigh;Peer-to-peer computing;Throughput;distributed;heterogeneous;cluster;adaptive;consistent;hashing;partitioning;scheme;load;balancing},
doi={10.1109/CCGrid.2015.34},
ISSN={},
month={May},}
@INPROCEEDINGS{7148447,
author={Gupta, Keshav and Saini, Jaspal Kaur},
booktitle={International Conference on Computing, Communication Automation}, title={Novel approach for distributed file system with multiple layers of fault tolerance},
year={2015},
volume={},
number={},
pages={616-619},
abstract={Numerous distributed file systems have been implemented for individuals and companies still users continue to face problems in using these systems and it is a still a big challenge for the researchers. In this paper, a novel Fault Tolerant Distributed File System (FTDFS) with multiple levels of fault tolerance is presented. At first level we are trying to filter faulty nodes/clients before they enter into system or we can say register with the server. At second level, to avoid failure we are maintaining chunk replicas so that if a client possessing file chunks gets disconnected, the file can still get downloaded. For security purpose, AES algorithm has been used for encryption and decryption. Many different concepts, various implementations and design strategies have been demonstrated by a survey along with different distributed systems for file sharing. Testing and performance evaluation of the newly developed FTDFS system is presented that improves load balancing by distributing the file over the network and increases the overall system performance.},
keywords={File systems;Fault tolerance;Fault tolerant systems;Servers;Encryption;Computers;FTDFS;DFS;AES},
doi={10.1109/CCAA.2015.7148447},
ISSN={},
month={May},}
@INPROCEEDINGS{7146288,
author={Khalili, Azade and Babamir, Seyed Morteza},
booktitle={2015 23rd Iranian Conference on Electrical Engineering}, title={Makespan improvement of PSO-based dynamic scheduling in cloud environment},
year={2015},
volume={},
number={},
pages={613-618},
abstract={The latest generation of distributed systems is cloud computing that has been acclaimed scientifically and commercially. CloudSim is one of the simulation tools that enables the evaluation and testing cloud services and infrastructures before development on a real cloud. For optimal use of the cloud's potential power, efficient and effective scheduling algorithms are required which can select the best resources for execution tasks. The matter of mapping and scheduling the tasks is assigning tasks to run on the existing resources in the manner that helps to maximize utilization and minimize makespan. The total time that is needed for all tasks/jobs to be finished is known as makespan. And utilization is the measure of how well the overall capacity of the cloud (network resources) is used. Due to the heterogeneity and dynamic resources, task scheduling is known as NP-complete problem and metaheuristics are needed to find the best scheduling combination. The main objective of this paper is to optimize task scheduling that uses the particle swarm optimization algorithm to minimize the makespan. Different inertia weights have been used. The Linear Descending Inertia Weight (LDIW) with an average 22.7% reduction in makespan shows the best performance.},
keywords={Decision support systems;Electrical engineering;Conferences;Genetic algorithms;cloud computing;Particle Swarm Optimization (PSO);inertia weight;task scheduling;utilization;makespan;CloudSim;cloudlet},
doi={10.1109/IranianCEE.2015.7146288},
ISSN={2164-7054},
month={May},}
@INPROCEEDINGS{7119040,
author={Sagardia, Mikel and Hertkorn, Katharina and Hulin, Thomas and Schätzle, Simon and Wolff, Robin and Hummel, Johannes and Dodiya, Janki and Gerndt, Andreas},
booktitle={2015 IEEE Aerospace Conference}, title={VR-OOS: The DLR's virtual reality simulator for telerobotic on-orbit servicing with haptic feedback},
year={2015},
volume={},
number={},
pages={1-17},
abstract={The growth of space debris is becoming a severe issue that urgently requires mitigation measures based on maintenance, repair, and de-orbiting technologies. Such on-orbit servicing (OOS) missions, however, are delicate and expensive. Virtual Reality (VR) enables the simulation and training in a flexible and safe environment, and hence has the potential to drastically reduce costs and time, while increasing the success rate of future OOS missions. This paper presents a highly immersive VR system with which satellite maintenance procedures can be simulated interactively using visual and haptic feedback. The system can be used for verification and training purposes for human and robot systems interacting in space. Our framework combines unique realistic virtual reality simulation engines with advanced immersive interaction devices. The DLR bimanual haptic device HUG is used as the main user interface. The HUG is equipped with two light-weight robot arms and is able to provide realistic haptic feedback on both human arms. Additional devices provide vibrotactile and electrotactile feedback at the elbow and the fingertips. A particularity of the realtime simulation is the fusion of the Bullet physics engine with our haptic rendering algorithm, which is an enhanced version of the Voxmap-Pointshell Algorithm. Our haptic rendering engine supports multiple objects in the scene and is able to compute collisions for each of them within 1 msec, enabling realistic virtual manipulation tasks even for stiff collision configurations. The visualization engine ViSTA is used during the simulation to achieve photo-realistic effects, increasing the immersion. In order to provide a realistic experience at interactive frame rates, we developed a distributed system architecture, where the load of computing the physics simulation, haptic feedback and visualization of a complex scene is transferred to dedicated machines. The implementations are presented in detail and the performance of the overall system is validated. Additionally, a preliminary user study in which the virtual system is compared to a physical test bed shows the suitability of the VR-OOS framework.},
keywords={Computational modeling;Force;Haptic interfaces;Robots;Satellites;Solid modeling;Space vehicles},
doi={10.1109/AERO.2015.7119040},
ISSN={1095-323X},
month={March},}
@INPROCEEDINGS{7104356,
author={Gui, Han-Dong and Zhang, Yue and Zhang, Zhiliang and Liu, Yan-Fei},
booktitle={2015 IEEE Applied Power Electronics Conference and Exposition (APEC)}, title={An optimized efficiency-based control strategy for islanded paralleled PV micro-converters},
year={2015},
volume={},
number={},
pages={229-234},
abstract={This paper analyzes the operation of PV modules in islanded paralleled PV applications under different solar irradiance condition in detail. When the batteries in the system are fully charged, the PV panels only need to generate power depending on the load and may not necessarily track the maximum power point (MPP), which exists extensively. In order to optimize the system efficiency in this situation, an efficiency-based model is built and a control strategy that can distribute the output power of different PV modules to find minimum total loss of the system is proposed. The system efficiency can be optimized adaptively under different load and irradiance condition with the proposed strategy. A distributed system prototype with three converters in parallel has been built and tested. The experimental results verified the benefits of the proposed strategy.},
keywords={Power generation;Batteries;Load modeling;Voltage control;Mathematical model;Resistance;Simulation;islanded paralleled PV applications;micro-converter;system efficiency;power distribution},
doi={10.1109/APEC.2015.7104356},
ISSN={1048-2334},
month={March},}
@ARTICLE{7097679,
author={Cataliotti, Antonio and Cosentino, Valentina and Di Cara, Dario and Guaiana, Salvatore and Panzavecchia, Nicola and Tinè, Giovanni},
journal={IEEE Transactions on Instrumentation and Measurement}, title={A New Solution for Low-Voltage Distributed Generation Interface Protection System},
year={2015},
volume={64},
number={8},
pages={2086-2095},
abstract={In this paper, a new solution is proposed for the remote monitoring and control of distributed generators (DGs) and energy storage systems (ESSs) connected to low-voltage distribution networks. The proposed system fulfills the in-force standard requirements for the connection of DGs to the utility grid. Moreover, it allows implementing some enhanced functions for the remote control of inverters of DGs and ESSs, not only in terms of disconnection but also in terms of voltage regulation and power shuttering. The proposed solution is based on a new interface protection system and a dedicated secondary substation concentrator, which allows the communication between the distributed system operator and the inverters of DGs or ESSs. The proposed system was tested on field in the electrical network of the island of Ustica. An experimental characterization was carried out to find the best communication conditions, i.e., the frequency band with the lowest noise and attenuation, considering the signal amplitude and signal-to-noise ratio constraints, as well as the desired transmission data rate and transfer time.},
keywords={Standards;Voltage measurement;Inverters;Threshold voltage;Time-frequency analysis;Prototypes;Communication system interfaces;distributed power generation;interface protection systems (IPSs);power line communication (PLC);power system communications;power system protection;smart grids.;Communication system interfaces;distributed power generation;interface protection systems (IPSs);power line communication (PLC);power system communications;power system protection;smart grids},
doi={10.1109/TIM.2015.2421691},
ISSN={1557-9662},
month={Aug},}
@INPROCEEDINGS{7092946,
author={Rey, Javier and Cogorno, Matias and Nesmachnow, Sergio and Steffenel, Luiz Angelo},
booktitle={2015 IEEE International Conference on Cloud Engineering}, title={Efficient Prototyping of Fault Tolerant Map-Reduce Applications with Docker-Hadoop},
year={2015},
volume={},
number={},
pages={369-376},
abstract={Prototyping and testing distributed systems is considered to be a hard task because it is not always possible to reproduce a given sequence of events. While simulations may help on this task, they cannot replace test and validation with real systems. In this paper we present Docker-Hadoop, a container-based virtualization platform designed to prototype, test and deploy MapReduce applications and systems. This tool allowed us to test and reproduce fault-tolerance scenarios that are especially interesting in the context of the PER-MARE project, which aims at adapting the Hadoop framework to the case pervasive systems. Indeed, we developed a fault-tolerant component that can circumvent the limitations from original Hadoop and prevent the job scheduling stall in the case of failures or network disconnections. Thanks to Docker-Hadoop, we could easily prototype and test our improved Hadoop, with the first scalability and speedup results being presented in this paper.},
keywords={Containers;Virtualization;Fault tolerance;Fault tolerant systems;Virtual machining;Kernel;Scalability;Virtualization; Containers; Prototyping; Hadoop; ZooKeeper},
doi={10.1109/IC2E.2015.73},
ISSN={},
month={March},}
@INPROCEEDINGS{7092760,
author={Guthmuller, Marion and Quinson, Martin and Corona, Gabriel},
booktitle={2015 23rd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing}, title={System-Level State Equality Detection for the Formal Dynamic Verification of Legacy Distributed Applications},
year={2015},
volume={},
number={},
pages={451-458},
abstract={The ever increasing complexity of distributed systems mandates to formally verify their design and implementation. Unfortunately, the common approaches and existing tools to formally establish the correctness of these systems remain hardly applicable to the kind of legacy applications that are commonly found in the HPC community. We present how system-level memory introspection can be achieved directly at runtime without relying on the source code analysis. We use this mechanism to detect the equality of the application's state at system level. As the storage of the system state may be memory expensive, we compact the memory by sharing unchanged memory pages between snapshots. This enables the automated verification of safety and liveness properties on legacy distributed applications written in Fortran or C/C++ using the MPI standard. We demonstrate the effectiveness of our approach on several programs from the MPICH3 test suite.},
keywords={Semantics;Debugging;Protocols;Memory management;Context;Syntactics;Standards;Formal Dynamic Verification;State equality;Legacy applications;SimGrid},
doi={10.1109/PDP.2015.95},
ISSN={2377-5750},
month={March},}
@ARTICLE{7069095,
author={Palma Salas, Marcelo Invert and Martins, Eliane},
journal={IEEE Latin America Transactions}, title={A Black-Box Approach to Detect Vulnerabilities in Web Services Using Penetration Testing},
year={2015},
volume={13},
number={3},
pages={707-712},
abstract={Web services work over dynamic connections among distributed systems. This technology was specifically designed to easily pass SOAP message through firewalls using open ports. These benefits involve a number of security challenges, such as Injection Attacks, phishing, Denial-of-Services (DoS) attacks, and so on. The difficulty to detect vulnerabilities,before they are exploited, encourages developers to use security testing like penetration testing to reduce the potential attacks. Given a black-box approach, this research use the penetration testing to emulate a series of attacks, such as Cross-site Scripting (XSS), Fuzzing Scan, Invalid Types, Malformed XML, SQL Injection, XPath Injection and XML Bomb. In this way, was used the soapUI vulnerability scanner in order to emulate these attacks and insert malicious scripts in the requests of the web services tested. Furthermore, was developed a set of rules to analyze the responses in order to reduce false positives and negatives. The results suggest that 97.1% of web services have at least one vulnerability of these attacks. We also determined a ranking of these attacks against web services.},
keywords={XML;Simple object access protocol;Testing;Security;Servers;Weapons;Cross-site Scripting;Fuzzing Scan;Invalid Types;Malformed XML;penetration testing;SQL Injection;web services;XML Bomb;XPath Injection;XSS},
doi={10.1109/TLA.2015.7069095},
ISSN={1548-0992},
month={March},}
@ARTICLE{7061920,
author={Schneider, Daniel and Beltle, Michael and Siegel, Martin and Tenbohlen, Stefan and Köhler, Wolfgang},
journal={IEEE Transactions on Electromagnetic Compatibility}, title={Radiated Emissions of an Electric Drive System Estimated on a Bench Using Disturbance Currents and Transfer Functions},
year={2015},
volume={57},
number={3},
pages={311-321},
abstract={Electric drive systems are going to be the future of the automotive industry. Electromagnetic compatibility (EMC) issues and challenges arise with its growing entry into modern cars. This contribution provides an overview of the field dominating disturbance currents of an automotive high-voltage (HV) power inverter within an electric drive system. In order to minimize the cost and time needed for EMC measurements and engineering, a precompliance measurement method is introduced and applied to the inverter's setup. The method presented allows the calculation of radiated emissions, according to CISPR 25, needed during the development process without the need for an expensive anechoic chamber. Transfer functions in combination with measured disturbance currents on the attached HV harnesses are used to obtain a cost-efficient estimation of the radiated emissions occurring during a component test. Its theory is discussed and demonstrated using a derived model. The method is verified using a rudimentary test setup, which represents a distributed system, and is later validated by the active electric drive system test setup introduced. Examinations of repeatability and reproducibility of disturbance current measurements as well as applied EMC counter measures complete the contribution.},
keywords={Current measurement;Inverters;Antenna measurements;Electromagnetic compatibility;Cable shielding;Probes;Transfer functions;Automotive high-voltage (HV) power inverter;CISPR 25;distributed system;electric drive system;estimation;precompliance;radiated emission;transfer function;Automotive high-voltage (HV) power inverter;CISPR 25;distributed system;electric drive system;estimation;precompliance;radiated emission;transfer function},
doi={10.1109/TEMC.2015.2401735},
ISSN={1558-187X},
month={June},}
@ARTICLE{6983586,
author={Chen, Yucong and Wen, Ziyu and Wen, Jiangtao and Tang, Minhao and Tao, Pin},
journal={IEEE Transactions on Circuits and Systems for Video Technology}, title={Efficient Software H.264/AVC to HEVC Transcoding on Distributed Multicore Processors},
year={2015},
volume={25},
number={8},
pages={1423-1434},
abstract={The latest High Efficiency Video Coding (HEVC) standard achieves a significant compression efficiency improvement over the H.264/Advanced Video Coding (AVC) standard, but with a much higher computational complexity. In this paper, we propose a novel framework for software-based H.264/AVC to HEVC transcoding, integrated with tools such as wavefront parallel processing that are useful for achieving higher levels of parallelism on multicore processors and distributed systems. By utilizing information extracted from the input H.264/AVC bitstream, the transcoding process can be greatly accelerated with a visual quality loss that is modest for many applications. Based on the HEVC HM 14.0 reference software and using standard HEVC test bitstreams, the proposed transcoder can achieve up to 60× speedup on a Quad Core 8-thread server over decoding-re-encoding based on FFMPEG and the HM software with a BD-rate loss of 15%-20%. By implementing a group of picture-level task distribution on a distributed system with nine processing units, the proposed software transcoder can achieve a speed for transcoding 720 p at 30 Hz in real time.},
keywords={Video coding;Transcoding;Parallel processing;Standards;Program processors;HEVC;H.264/AVC;transcoding;Wavefront Parallel Processing;mode decision;H264/Advanced Video Coding (AVC);High Efficiency Video Coding (HEVC);mode decision;transcoding;wavefront parallel processing (WPP)},
doi={10.1109/TCSVT.2014.2380231},
ISSN={1558-2205},
month={Aug},}
@ARTICLE{6863682,
author={Pastina, Debora and Santi, Fabrizio and Bucciarelli, Marta},
journal={IEEE Geoscience and Remote Sensing Letters}, title={MIMO Distributed Imaging of Rotating Targets for Improved 2-D Resolution},
year={2015},
volume={12},
number={1},
pages={190-194},
abstract={This letter deals with the multiple-input-multiple-output distributed radar imaging of rotating targets. The distributed system is based on formation-flying platforms, which can be configured with proper cross-track and along-track displacements. The platforms carry active radar systems transmitting almost orthogonal waveforms; exploiting both monostatic and bistatic acquisitions, range and cross-range resolution improvement can be achieved, and a maximum theoretical 2-D resolution cell improvement factor can be obtained significantly greater than the number of flying platforms. The required distributed focusing technique is developed, and its effectiveness and robustness is tested against simulated data. The proposed distributed system could be suitable to the application of ship target imaging, using a reconfigurable formation of platforms, for maritime surveillance in wide sea areas.},
keywords={MIMO;Synthetic aperture radar;Image resolution;Sensors;Geometry;Focusing;Radar imaging;Inverse synthetic aperture radar (ISAR);multiple-input–multiple-output (MIMO);resolution;synthetic aperture radar (SAR);Inverse synthetic aperture radar (ISAR);multiple-input–multiple-output (MIMO);resolution;synthetic aperture radar (SAR)},
doi={10.1109/LGRS.2014.2331754},
ISSN={1558-0571},
month={Jan},}
@INPROCEEDINGS{7154006,
author={Ammar, Mohammed},
booktitle={2014 European Modelling Symposium}, title={Model Identification of a Paper Machine Cross-Directional Process under Model Predictive Control},
year={2014},
volume={},
number={},
pages={245-250},
abstract={The paper machine cross-directional (CD) process is an industrial spatially distributed system. Paper properties are controlled by a set of different actuator arrays acting in the cross direction (CD) as the paper sheet moves along the machine direction (MD). The industrial custom is to identify CD models from bump tests that are run in open-loop. This article presents a technique for CD response shape, alignment and dynamics model identification. The spatial response is identified by a no causal spatial FIR model that accounts for the actuator response in the cross-direction (CD). A novel technique for CD alignment detection is presented through disconsolation of the measurement profile. The spatial model identification is followed by estimating the CD process dynamics. The CD model identification technique is extended to feedback loops running under model predictive control (MPC). CD process models are identified in closed-loop from short identification experiments in a low signal-to-noise ratio (SNR). The proposed technique is validated by conducting identification experiments on an industrial paper machine model running under MPC.},
keywords={Actuators;Process control;Finite impulse response filters;Mathematical model;Shape;Deconvolution;Signal to noise ratio;CD Process;Identification;Alignment;Noncausal FIR model;MPC},
doi={10.1109/EMS.2014.47},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7102446,
author={Navarro, José M. and Parada, G. Hugo A. and Dueñas, Juan C.},
booktitle={2014 2nd International Conference on Artificial Intelligence, Modelling and Simulation}, title={System Failure Prediction through Rare-Events Elastic-Net Logistic Regression},
year={2014},
volume={},
number={},
pages={120-125},
abstract={Predicting failures in a distributed system based on previous events through logistic regression is a standard approach in literature. This technique is not reliable, though, in two situations: in the prediction of rare events, which do not appear in enough proportion for the algorithm to capture, and in environments where there are too many variables, as logistic regression tends to over fit on this situations, while manually selecting a subset of variables to create the model is error-prone. On this paper, we solve an industrial research case that presented this situation with a combination of elastic net logistic regression, a method that allows us to automatically select useful variables, a process of cross-validation on top of it and the application of a rare events prediction technique to reduce computation time. This process provides two layers of cross-validation that automatically obtain the optimal model complexity and the optimal model parameters values, while ensuring even rare events will be correctly predicted with a low amount of training instances. We tested this method against real industrial data, obtaining a total of 60 out of 80 possible models with a 90% average model accuracy.},
keywords={Predictive models;Data models;Logistics;Computational modeling;Training;Prediction algorithms;Complexity theory;Online Failure Prediction;Machine Learning;System Management;Automatic Feature Selection;Logistic Regression;Multivariable Prediction},
doi={10.1109/AIMS.2014.19},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7097454,
author={Konno, Tomoyuki and Itoh, Ryosuke and Nakao, Mikihiko and Suzuki, Soh Y. and Yamada, Satoru},
booktitle={2014 19th IEEE-NPSS Real Time Conference}, title={The slow control and data quality monitoring systems for the Belle II experiment},
year={2014},
volume={},
number={},
pages={1-4},
abstract={The Belle II experiment will start physics data taking in 2016 at the SuperKEKB factory to search for new physics beyond the Standard Model from flavor physics channels. A network distributed data acquisition (DAQ) system was developed for huge data size from the Belle II detector. In order to manage the distributed systems, software control and monitoring systems for the DAQ subsystems were newly developed. A slow control system handles with operations of the distributed subsystems including parameter management with relational databases while a data quality monitor (DQM) system analyzes event samples in online data flow to monitor status of the detector and data taking. Graphical user interfaces were also developed for both of the software systems. We report designs of the slow control and the DQM systems and results of operation at a beam test with Belle II PXD and SVD prototypes in the DESY test beam facility in beginnings of 2014.},
keywords={Histograms;Data acquisition;Monitoring;Control systems;Detectors;Graphical user interfaces;Databases;Data acquisition;data quality monitor;distributed system;elementary particles;graphical user interface;relational database;slow control},
doi={10.1109/RTC.2014.7097454},
ISSN={},
month={May},}
@INPROCEEDINGS{7090169,
author={Suciu, George and Halunga, Simona and Ochian, Adelina and Suciu, Victor},
booktitle={Proceedings of the 2014 6th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)}, title={Network management and monitoring for cloud systems},
year={2014},
volume={},
number={},
pages={1-4},
abstract={Monitoring represents an important factor in improving the quality of the services provided in cloud computing, given the fact that it allows scaling resource utilization in an adaptive manner. It is widely used for detecting critical events and abnormalities of distributed systems and also it helps identifying the faults within the system, discovering application patterns for the users. As cloud systems increase their architecture, the degree of workload also grows in datacenters, causing node failures and performance issues. This paper aims to provide a solution for the monitoring of cloud computing systems and services, allowing users and also providers to optimize the usage of the computational resources according to the constantly changing business requirements inside an organization. The main contribution of the paper consists of the integration of the monitoring system, which is based on Nagios and NConf with a test cloud architecture. Finally, the paper discusses the main findings for a reference implementation using the OpenStack cloud platform.},
keywords={Monitoring;Cloud computing;Computer architecture;Servers;Databases;Computational modeling;cloud;monitoring system;datacenter;distributed systems},
doi={10.1109/ECAI.2014.7090169},
ISSN={},
month={Oct},}
@INPROCEEDINGS{7044327,
author={Osipov, Evgeny and Mousavi, Arash},
booktitle={2014 IEEE Frontiers in Education Conference (FIE) Proceedings}, title={How to make a distributed programming course a big fun},
year={2014},
volume={},
number={},
pages={1-6},
abstract={This article presents experiences of teachers from Luleâ University of Technology when enhancing the teaching approach and depth of an undergraduate course on network programming and distributed applications. During the trial run of the course in the fall of 2013 agent-oriented programming and cloud technologies were married to provide students an exciting practical scenario and capability to test the performance of truly large scale distributed systems under extremely high traffic loads.},
keywords={Programming profession;Cloud computing;Multi-agent systems;Computer architecture;Educational institutions},
doi={10.1109/FIE.2014.7044327},
ISSN={2377-634X},
month={Oct},}
@INPROCEEDINGS{7039638,
author={Wang, Yuh-Shyang and Matni, Nikolai and Doyle, John C.},
booktitle={53rd IEEE Conference on Decision and Control}, title={Localized LQR optimal control},
year={2014},
volume={},
number={},
pages={1661-1668},
abstract={This paper introduces a receding horizon like control scheme for localizable distributed systems, in which the effect of each local disturbance is limited spatially and temporally. We characterize such systems by a set of linear equality constraints, and show that the resulting feasibility test can be solved in a localized and distributed way. We also show that the solution of the local feasibility tests can be used to synthesize a receding horizon like controller that achieves the desired closed loop response in a localized manner as well. Finally, we formulate the Localized LQR (LLQR) optimal control problem and derive an analytic solution for the optimal controller. Through a numerical example, we show that the LLQR optimal controller, with its constraints on locality, settling time, and communication delay, can achieve similar performance as an unconstrained ℋ2 optimal controller, but can be designed and implemented in a localized and distributed way.},
keywords={Transfer functions;Optimal control;Delays;Mathematical model;Finite impulse response filters;State feedback;Equations},
doi={10.1109/CDC.2014.7039638},
ISSN={0191-2216},
month={Dec},}
@INPROCEEDINGS{7038378,
author={Schlösser, Tim and Stinner, Sebastian and Monti, Antonello and Müller, Dirk},
booktitle={2014 Power Systems Computation Conference}, title={Analyzing the impact of home energy systems on the electrical grid},
year={2014},
volume={},
number={},
pages={1-7},
abstract={This paper shows an approach to analyze the behavior of the electrical low-voltage grid. Here, the focus lies on home energy systems acting as heat supplier for buildings and photovoltaic systems as domestic electrical energy sources. Electrical quantities of the grid like voltage, load flow through the distribution transformer, grid losses and the utilization of the cables are analyzed in a case study. This case study includes two scenarios. The first one is built up with an actual penetration of distributed systems and the second (generic) one includes higher shares of these systems. As a result so far, we cannot find any violation of critical values in the considered test cases.},
keywords={Buildings;Load modeling;Cogeneration;Load flow;Resistance heating;Heat pumps;Atmospheric modeling;Cogeneration;Heat Pumps;Energy Storage;Thermal analysis;Load flow;Stochastic processes},
doi={10.1109/PSCC.2014.7038378},
ISSN={},
month={Aug},}
@INPROCEEDINGS{7022482,
author={Kar, Asutosh and Chandra, Mahesh and Goel, Pankaj and Gupta, V. K.},
booktitle={TENCON 2014 - 2014 IEEE Region 10 Conference}, title={A low-cost, portable alternative for a digital Lock-In Amplifier using TMS320C5535 DSP},
year={2014},
volume={},
number={},
pages={1-5},
abstract={A Lock-In Amplifier (LIA) is used to remove noise from an input signal by the use of a reference signal. Generally the sampling rate of LIAs is much higher than the required frequency in typical applications. Moreover the hardware configuration of LIAs is much better than the requirement. The signals are processed using high-end signal processors which increase the cost of LIA. This paper aims to develop an inexpensive and portable alternative for the digital lock-in amplifier. This will enable application of LIAs in remote locations and in distributed systems which require multiple signal processors simultaneously. Similar implementation exists on microcontrollers and embedded processors. This paper utilizes a TMS320 series Digital Signal Processor (DSP) which is a low-power signal processor. The lock-in amplifier is implemented on A TMS320C5535 eZDSP board. Unlike other LIAs the reference signal is generated in the DSP, hence obviating the requirement for an external reference input. The output is obtained through UART interface and observed on a computer. It is tested with a noisy sinusoidal signal given as input and the output signal is plotted and verified.},
keywords={Digital signal processing;Amplifiers;Frequency modulation;Ports (Computers);Codecs;Hardware;DSP;C5535;eZDSP;Lock-In Amplifier;TMS320},
doi={10.1109/TENCON.2014.7022482},
ISSN={2159-3450},
month={Oct},}
@INPROCEEDINGS{7024563,
author={Voicu, Cristiana and Pop, Florin and Dobre, Ciprian and Xhafa, Fatos},
booktitle={2014 Ninth International Conference on P2P, Parallel, Grid, Cloud and Internet Computing}, title={MOMC: Multi-objective and Multi-constrained Scheduling Algorithm of Many Tasks in Hadoop},
year={2014},
volume={},
number={},
pages={89-96},
abstract={Even though scheduling in a distributed system was debated for many years, the platforms and the job types are changing everyday. This is why we need special algorithms based on new applications requirements, especially when a application is deployed in a Cloud environment. One of the most important framework used for large-scale data processing in Clouds is Hadoop and its extensions. Hadoop framework comes with default algorithms like FIFO, Fair Scheduler or Capacity Scheduler, and Hadoop on Demand. These scheduling algorithms are focused on a different and single constraint. It is hard to satisfy multiple constraints and to have a lot of objectives in the same time. After summarizing the most common schedulers, showing the need of each one in the moment it appeared on the market, this paper presents MOMC, a multi-objective and multi-constrained scheduling algorithm of many tasks in Hadoop. MOMC implementation focuses on two objectives: avoiding resource contention and having an optimal workload of the cluster, and two constraints: deadline and budget. To compare the algorithms based on different metrics, we use Scheduling Load Simulator, which is integrated in Hadoop framework and helps the developers to spend less time on testing. As killer application that generate many tasks we have chosen processing task for the Million Song Dataset, which is a set of data contains metadata for one million commercially-available songs.},
keywords={Scheduling algorithms;Measurement;Scheduling;History;Clustering algorithms;Containers;Task Scheduling;Hadoop;Map Reduce;Big Data;Cloud Computing},
doi={10.1109/3PGCIC.2014.40},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7018532,
author={Ulrich, Andreas and Jell, Sylvia and Votintseva, Anjelika and Kull, Andres},
booktitle={2014 2nd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)}, title={The ETSI Test Description Language TDL and its application},
year={2014},
volume={},
number={},
pages={601-608},
abstract={The wide-scale introduction of model-based testing techniques in an industrial context faces many obstacles. One of the obstacles is the existing methodology gap between informally described test purposes and formally defined test descriptions used as the starting point for test automation. The provision of an explicit test description becomes increasingly essential when integrating complex, distributed systems and providing support for conformance and interoperability tests of such systems. The upcoming ETSI standard on the Test Definition Language (TDL) covers this gap. It allows describing scenarios on a higher abstraction level than programming or scripting languages. Furthermore, TDL can be used as an intermediate representation of tests generated from other sources, e.g. simulators, test case generators, or logs from previous test runs. TDL is based on a meta-modelling approach that expresses its abstract syntax. Deploying this design approach, individual concrete syntaxes of TDL can be designed for different application domains. The paper provides an overview of TDL and discusses its application on a use case from the rail domain.},
keywords={Testing;Unified modeling language;Concrete;Telecommunication standards;Syntactics;Semantics;Abstracts;Model-based Testing;Domain-Specific Languages;Meta-modelling;Rail Application},
doi={},
ISSN={},
month={Jan},}
@INPROCEEDINGS{7019604,
author={Chahar, Kishore and Maurya, Ajay Kumar and Chaturvedi, Abhishek},
booktitle={2014 International Conference on Contemporary Computing and Informatics (IC3I)}, title={Performance analysis of fuel cell assisted distributed generation system},
year={2014},
volume={},
number={},
pages={426-429},
abstract={This paper investigates performance analysis of Solid oxide fuel cell assisted seven-level cascaded flying capacitor multilevel inverter (FCMI) based distributed system. A hybrid modulation technique proposes for generating thyristor gate pulses. This hybrid modulation technique is achieved by combining two well-known modulation techniques: phase disposition (PD) and phase shift (PS). This paper also consist modeling of SOFC. The whole model simulated and tested on MATLAB/ SIMULINK environment to verify the theoretical consideration.},
keywords={Fuel cells;Inverters;Modulation;Insulated gate bipolar transistors;Fuels;Topology;Mathematical model;Distributed Generating system;Fuel Cell;hybrid Modulation Strategy;Multilevel Inverter},
doi={10.1109/IC3I.2014.7019604},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7005128,
author={Craciunas, Silviu S. and Oliver, Ramon Serna and Ecker, Valentin},
booktitle={Proceedings of the 2014 IEEE Emerging Technology and Factory Automation (ETFA)}, title={Optimal static scheduling of real-time tasks on distributed time-triggered networked systems},
year={2014},
volume={},
number={},
pages={1-8},
abstract={Mixed-criticality and high availability distributed systems, like those on large industrial deployments, strongly rely on deterministic communication in order to guarantee the realtime behavior. The time-triggered paradigm-as in TTEthernet-guarantees the deterministic delivery of messages with fixed latency and limited jitter. We look closely at industrial deployments in which production as well as consumption of messages is carried out within software tasks running on distributed embedded nodes (i.e. end-systems). We present an approach to minimize the end-to-end latency of such tasks, respecting their precedence constraints as well as the scheduled communication in an underlying switched TTEthernet network. The approach is based on and validated by a large industrial use-case for which we analyze a test bed implementing our solution.},
keywords={Schedules;Optimal scheduling;Real-time systems;Job shop scheduling;Dynamic scheduling},
doi={10.1109/ETFA.2014.7005128},
ISSN={1946-0759},
month={Sep.},}
@INPROCEEDINGS{7000999,
author={Capodieci, Nicola and Hart, Emma and Cabri, Giacomo},
booktitle={2014 IEEE Eighth International Conference on Self-Adaptive and Self-Organizing Systems}, title={Artificial Immune System Driven Evolution in Swarm Chemistry},
year={2014},
volume={},
number={},
pages={40-49},
abstract={Morphogenetic engineering represents an interesting field in which models, frameworks and algorithms can be tested in order to study how self-* properties and emergent behaviours can arise in potentially complex and distributed systems. In this field, the morphogenetic model we will refer to is swarm chemistry, since a well known challenge in this dynamical process concerns discovering mechanisms for providing evolution within coalescing systems of particles. These systems consist in sets of moving particles able to self-organise in order to create shapes or geometrical formations that provide robustness towards external perturbations. We present a novel mechanism for providing evolutionary features in swarm chemistry that takes inspiration from artificial immune system literature, more specifically regarding idiotypic networks. Starting from a restricted set of chemical recipes, we show that the system evolves to new states, using an autonomous method of detecting new shapes and behaviours free from any human interaction.},
keywords={Kinetic theory;Chemistry;Immune system;Shape;Negative feedback;Mathematical model;Algorithm design and analysis;Autonomic Computing;Self-Organizing Systems;Artificial Immune System;Morphogenetic engineering},
doi={10.1109/SASO.2014.16},
ISSN={1949-3681},
month={Sep.},}
@INPROCEEDINGS{6993970,
author={Hu, Lijuan and Song, Xiaohui and Sheng, Wanxing and Meng, Xiaoli},
booktitle={2014 International Conference on Power System Technology}, title={An improved three-phase power flow calculation method based on virtual neutral point},
year={2014},
volume={},
number={},
pages={1092-1097},
abstract={In order to improve the accuracy of the calculation model so as to improve distribution automation level, this paper presented a new three-phase power flow calculation method for three-phase distribution network. Based on the special network structure of distributed system and the relation between current in the three phases, virtual neutral point was brought in the model. By the virtual neutral point, it is easy to get the relation between the load in each node and the injection current it caused in the according node. And then the three-phase power flow calculation model could be built by back/forward sweep method. In order to obtain the solution of the presented model, a new kind of mixed iterative algorithm was proposed. The mixed iterative algorithm combined the back/forward sweep method with improved Newton Raphson method. In the last, the presented method was tested by a practical distribution network with six buses and IEEE 33-bus system and the power flow results was given to show the accuracy and validity of the proposed model and algorithm.},
keywords={Bismuth;Nickel;Load flow;Electric potential;Load modeling;Iterative methods;Distribution network;Three-phase power flow;Virtual neutral point;Mixed iterative algorithm},
doi={10.1109/POWERCON.2014.6993970},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6984244,
author={Heyman, Thomas and Preuveneers, Davy and Joosen, Wouter},
booktitle={2014 International Conference on Future Internet of Things and Cloud}, title={Scalar: Systematic Scalability Analysis with the Universal Scalability Law},
year={2014},
volume={},
number={},
pages={497-504},
abstract={Analyzing the scalability and quality of service of large scale distributed systems requires a highly scalable benchmarking framework with built-in communication and synchronisation functionality, which are features that are lacking in current load generation tools. This paper documents Scalar, our distributed, extensible scalability analysis tool that can generate high request volumes using multiple communicating, coordinated nodes. We show how Scalar offers analytics capabilities that support the Universal Scalability Law. We illustrate Scalar on an electronic payment case study, and find that the framework supports complex work flows and is able to characterize and give predictive insights into the quality of service and relative capacity of the system under test in function of the user load.},
keywords={Scalability;Synchronization;Benchmark testing;Browsers;Business;Distributed databases;scalability;load testing;benchmarking;distributed systems},
doi={10.1109/FiCloud.2014.88},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6984504,
author={Hmida, Faten Ben and Chaari, Wided Lejouad and Dupas, Rémy and Séguy, Anne},
booktitle={2014 IEEE 26th International Conference on Tools with Artificial Intelligence}, title={Evaluation of Communication in Multiagent Systems for Supply Chain Planning and Control},
year={2014},
volume={},
number={},
pages={408-412},
abstract={Multi Agent Systems (MAS) are particularly suitable for the modelling and analysis of the structure and dynamics of industrial complex systems. They are often used in supply chains as a support for decision-making and seem especially helpful to test and analyze different control design alternatives. However, the lack of evaluation methods of MAS is a major disincentive to their use in such real world applications. In this paper, a MAS evaluation method is first presented. It focuses on communication as it is one of the most important functional characteristics of distributed systems in general and MAS in particular. Then three multiagent models representing different supply chain control architectures (distributed, centralized and mixed) are presented, evaluated and compared according to the advanced method. Experimentation is carried out with reference to a test-case supply chain network in the domain of furniture manufacturing.},
keywords={Supply chains;Indexes;Multi-agent systems;Complexity theory;Educational institutions;Manufacturing;multiagent systems;evaluation;measurement;graph theory;communication;supply chain;decision;control;planning},
doi={10.1109/ICTAI.2014.68},
ISSN={2375-0197},
month={Nov},}
@INPROCEEDINGS{6984869,
author={Garcia, Kemilly Dearo and Naldi, Murilo Coelho},
booktitle={2014 Brazilian Conference on Intelligent Systems}, title={Multiple Parallel MapReduce k-Means Clustering with Validation and Selection},
year={2014},
volume={},
number={},
pages={432-437},
abstract={Dealing with big amounts of data is one of the challenges for clustering, which causes the need for distribution and management of huge data sets in separate repositories. New distributed systems have been designed to scale up from a single server to thousands of machines. The MapReduce framework allows to divide a job and combine the results seamlessly. The k-means is one of the few clustering algorithms that satisfies the MapReduce constrains, but it requires the previous specification of the number of clusters and is sensitive to their initialization. In this work, we propose a MapReduce clustering algorithm to execute multiple parallel runs of k-means with different initializations and number of clusters. Additionally, a MapReduce version of a cluster relative validity index is implemented and used to find the best result. The proposed algorithm is experimentally compared with the Apache Mahout Project's MapReduce implementation of k-means. Statistical tests applied on the results indicate that the proposed algorithm can outperform the Mahout's implementation when multiple k-means partitions are required.},
keywords={Clustering algorithms;Partitioning algorithms;Data structures;Indexes;Vectors;Big data;Parallel processing;MapReduce Clustering;Clustering Validation;k-means;Cluster Selection},
doi={10.1109/BRACIS.2014.83},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6983049,
author={Soundararajan, Vijayaraghavan and Agrawal, Banit and Herndon, Bruce and Sethuraman, Priya and Taheri, Reza},
booktitle={2014 IEEE International Symposium on Workload Characterization (IISWC)}, title={Benchmarking a virtualization platform},
year={2014},
volume={},
number={},
pages={99-109},
abstract={Traditional benchmarking of new architectures often involves running a workload on a single system with a single OS. In such a setup, the objective is typically to stress a single resource (e.g., CPU) and produce a single number used to characterize the performance of the system. Newer benchmarks have extended this paradigm by testing the performance of distributed systems like Hadoop clusters or cloud-style workloads such as big data analytics. These benchmarks are invaluable for evaluating physical systems, but have numerous drawbacks and gaps when used to evaluate virtualized systems. A virtualized system must be evaluated in an end-to-end way beyond what is done for physical systems, with measurements and workloads for the hypervisor platform, the application, and the management layer that supports virtualization-related services like live migration or rapid VM provisioning. In this paper, we describe an end-to-end approach to evaluating virtualization platforms. We begin with a discussion of traditional virtualization benchmarking, involving comparing performance on real systems vs. performance on virtualized systems. We next describe three benchmarks that we have developed specifically for virtualized environments. We discuss a system-level benchmark, VMmark, which incorporates workloads that simultaneously stress a number of different resources (CPU, memory, and IO). We then outline two additional benchmark suites designed for measuring virtualization management performance and application performance: VCBench and ViewPlanner. For each workload, we describe how it was developed, what it tests, and the key insights it has provided in terms of optimizing virtualized platforms. We conclude with a discussion of next-generation virtualization benchmarks.},
keywords={Benchmark testing;Virtualization;Virtual machine monitors;Hardware;Servers;Measurement;Stress;Virtual Machine management;cloud computing;datacenter management;management workload;benchmarking},
doi={10.1109/IISWC.2014.6983049},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6976167,
author={Fokaefs, Marios},
booktitle={2014 IEEE International Conference on Software Maintenance and Evolution}, title={WSDarwin: A Framework for the Support of Web Service Evolution},
year={2014},
volume={},
number={},
pages={668-668},
abstract={Service-oriented architectures have emerged as a popular development paradigm for modular, extendible distributed systems, relying on interoperable components. However, their evolution is a challenging problem, due to the fact that their constituent parts typically reside outside a single entity's domain of ownership and control. In my thesis, I have developed WSDarwin, a framework to support the evolution of service systems. WSDarwin posits a novel conceptual model of the technical and economic relationships between service providers and clients, based on which, a game-theoretic method determines the best evolution decision for them as distinct economic players and for the ecosystem as a whole. Furthermore, WSDarwin provides a complete toolkit for client adaptation (including service-interface differencing, client-proxy adaptation and client unit testing), to simplify the client's software evolution task in response to service changes.},
keywords={Web services;Ecosystems;Mathematical model;Accuracy;Usability;Companies;Conferences},
doi={10.1109/ICSME.2014.123},
ISSN={1063-6773},
month={Sep.},}
@INPROCEEDINGS{6974864,
author={Yasasin, Emrah and Rauchecker, Gerhard and Prester, Julian and Schryen, Guido},
booktitle={2014 25th International Workshop on Database and Expert Systems Applications}, title={A Fuzzy Security Investment Decision Support Model for Highly Distributed Systems},
year={2014},
volume={},
number={},
pages={291-295},
abstract={The economic aspect of information security is a comparatively new discipline so that there is hardly any extensive research work. This applies in particular to measures in highly distributed systems which have been neglected in previous research. The present paper focuses on the security investments in such systems. We augment an existing research about a fuzzy decision support model by defining appropriate operators in order to applicate this work in practice. The proposed model includes uncertainty with respect to the impact of investments on the achieved security levels of components of the distributed system. We further develop a heuristic to solve the problem and test the heuristic experimentally. The paper concludes with a discussion and gives an outlook to future work in the context of security investments.},
keywords={Investment;Economics;Information security;Monte Carlo methods;Fuzzy sets;Measurement;fuzzy security investments;decision support model;highly distributed systems;fuzzy optimization;monte carlo heuristic},
doi={10.1109/DEXA.2014.65},
ISSN={2378-3915},
month={Sep.},}
@INPROCEEDINGS{6956935,
author={Jansen, Norman and Krämer, Daniel and Spielmann, Marc},
booktitle={2014 IEEE Military Communications Conference}, title={Testbeds for IT Systems in Tactical Environments},
year={2014},
volume={},
number={},
pages={1293-1298},
abstract={We present a tool box for functional testing of complex distributed IT systems specially tailored for the tactical environment. In order to construct suitable test beds for such systems we capture their 'ambient' infrastructure by means of virtualization and real-time simulation. As an example, we describe how our approach can be used for testing a complex distributed system consisting of (multiple instances of) a tactical command and control information system, a communications server system and a radio communications system. The combined system is part of the standard equipment of various German Army mobile platforms. Aside testing, our tool box also has many potential applications in the domain of training support.},
keywords={Vehicles;Virtualization;Real-time systems;Testing;Software;Emulation;Sensors;tactical IT systems;virtualization;real-time simulation;testing;training support},
doi={10.1109/MILCOM.2014.215},
ISSN={2155-7586},
month={Oct},}
@INPROCEEDINGS{6956948,
author={Cleveland, Jeffrey and Loyall, Joseph and Hanna, James},
booktitle={2014 IEEE Military Communications Conference}, title={An Aspect-Oriented Approach to Assessing Fault Tolerance},
year={2014},
volume={},
number={},
pages={1374-1381},
abstract={Fault tolerance and survivability are important aspects of many business-critical and mission-critical systems but it is still difficult to assess how well fault tolerance techniques work. Ensuring fault tolerance in military communication systems is particularly important due to the inevitability of hardware failure, data corruption, or service interruption and the risk that cascading failures could jeopardize critical military operations. In this paper, we present a fault tolerance assessment framework designed for distributed systems that provides automated injection of faults without changes to client or server code and automated assessment of whether the injected faults are tolerated. The framework applies aspect-oriented programming, specifically AspectJ, to inject faults and weave in assessment criteria. The framework supports both assessing the tolerance of direct faults, such as crashes and corruption, like traditional fault injectors, and conditional faults, which can be probabilistically, randomly, or periodically injected at runtime. This latter class of faults is not historically supported by fault injectors, but enables the assessment of tolerance to many important classes of faults threatening modern distributed military communication systems, including timing faults, resource exhaustion (e.g., Denial-of-service), and integrity faults that are traditionally difficult to tolerate and assess. Additionally, the framework provides a centralized view for users enabling them to monitor and script coordinated tests comprising performance metrics and injected faults spanning services, applications, and hosts.},
keywords={Fault tolerant systems;Fault tolerance;Measurement;Prototypes;Programming;Weaving;Java;fault tolerance;assessment;testing;aspect-oriented programming;survivability},
doi={10.1109/MILCOM.2014.228},
ISSN={2155-7586},
month={Oct},}
@INPROCEEDINGS{6956560,
author={Brubaker, Chad and Jana, Suman and Ray, Baishakhi and Khurshid, Sarfraz and Shmatikov, Vitaly},
booktitle={2014 IEEE Symposium on Security and Privacy}, title={Using Frankencerts for Automated Adversarial Testing of Certificate Validation in SSL/TLS Implementations},
year={2014},
volume={},
number={},
pages={114-129},
abstract={Modern network security rests on the Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols. Distributed systems, mobile and desktop applications, embedded devices, and all of secure Web rely on SSL/TLS for protection against network attacks. This protection critically depends on whether SSL/TLS clients correctly validate X.509 certificates presented by servers during the SSL/TLS handshake protocol. We design, implement, and apply the first methodology for large-scale testing of certificate validation logic in SSL/TLS implementations. Our first ingredient is "frankencerts," synthetic certificates that are randomly mutated from parts of real certificates and thus include unusual combinations of extensions and constraints. Our second ingredient is differential testing: if one SSL/TLS implementation accepts a certificate while another rejects the same certificate, we use the discrepancy as an oracle for finding flaws in individual implementations. Differential testing with frankencerts uncovered 208 discrepancies between popular SSL/TLS implementations such as OpenSSL, NSS, CyaSSL, GnuTLS, PolarSSL, MatrixSSL, etc. Many of them are caused by serious security vulnerabilities. For example, any server with a valid X.509 version1 certificate can act as a rogue certificate authority and issue fake certificates for any domain, enabling man-in-the-middle attacks against MatrixSSL and GnuTLS. Several implementations also accept certificate authorities created by unauthorized issuers, as well as certificates not intended for server authentication. We also found serious vulnerabilities in how users are warned about certificate validation errors. When presented with an expired, self-signed certificate, NSS, Safari, and Chrome (on Linux) report that the certificate has expired - a low-risk, often ignored error - but not that the connection is insecure against a man-in-the-middle attack. These results demonstrate that automated adversarial testing with frankencerts is a powerful methodology for discovering security flaws in SSL/TLS implementations.},
keywords={Testing;Servers;Protocols;Browsers;Authentication;Computer bugs;SSL;certificate validation;automated testing},
doi={10.1109/SP.2014.15},
ISSN={2375-1207},
month={May},}
@INPROCEEDINGS{6910484,
author={Ríos, Anasol Peña and Callaghan, Vic and Gardner, Michael and Alhaddad, Mohammed J.},
booktitle={2014 International Conference on Intelligent Environments}, title={Interactions within Distributed Mixed Reality Collaborative Environments},
year={2014},
volume={},
number={},
pages={382-383},
abstract={Traditionally virtual worlds have been regarded as standalone entities. However, the world moves fast towards a mixed reality collective environment, joining virtual and real world by incorporating accessible ubiquitous computing for people. Mobile and wearable computers act as a door to connect people to virtuality, e.g. The use of fitness/activity trackers, which collect real world information helping users to complement reality with virtuality improving their health and fitness. A different example is the use of mobile devices to connect people that do not share the same physical location in a virtual way, thought phone calls, videoconferences, chat and social media applications. These examples show that currently we live in two realities, processing information of both worlds in real time. Our video submission presents a work-in-progress research prototype towards the creation of a Blended Reality Distributed System, complementing the paper [1] submitted to the main track of the conference. The test bed scenario proposed is a mixed reality collaborative laboratory activity, performed by learners within geographically dispersed locations. The goal of the activity is to construct a small robot emphasising computing fundamentals. The video is available at: http://youtu.be/akKPHnDY9bw.},
keywords={Virtual reality;Collaboration;Real-time systems;Robot sensing systems;Educational institutions;intelligent environments;mixed reality;interreality;hyperreality;ubiquitous virtual reality;blended reality;HCI},
doi={10.1109/IE.2014.66},
ISSN={},
month={June},}
@INPROCEEDINGS{6903597,
author={Schiper, Nicolas and Rahli, Vincent and Van Renesse, Robbert and Bickford, Marck and Constable, Robert L.},
booktitle={2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks}, title={Developing Correctly Replicated Databases Using Formal Tools},
year={2014},
volume={},
number={},
pages={395-406},
abstract={Fault-tolerant distributed systems often contain complex error handling code. Such code is hard to test or model-check because there are often too many possible failure scenarios to consider. As we will demonstrate in this paper, formal methods have evolved to a state in which it is possible to generate this code along with correctness guarantees. This paper describes our experience with building highly-available databases using replication protocols that were generated with the help of correct-by-construction formal methods. The goal of our project is to obtain databases with unsurpassed reliability while providing good performance. We report on our experience using a total order broadcast protocol based on Paxos and specified using a new formal language called Event ML. We compile Event ML specifications into a form that can be formally verified while simultaneously obtaining code that can be executed. We have developed two replicated databases based on this code and show that they have performance that is competitive with popular databases in one of the two considered benchmarks.},
keywords={Clocks;Protocols;Distributed databases;Computational modeling;Cognition;Semantics;Fault-tolerance;database replication;correct-by-construction distributed protocols;formal tools},
doi={10.1109/DSN.2014.45},
ISSN={2158-3927},
month={June},}
@INPROCEEDINGS{6903118,
author={Rush, George and Tauritz, Daniel R. and Kent, Alexander D.},
booktitle={2014 IEEE 38th International Computer Software and Applications Conference Workshops}, title={DCAFE: A Distributed Cyber Security Automation Framework for Experiments},
year={2014},
volume={},
number={},
pages={134-139},
abstract={Cyber security has quickly become an overwhelming challenge for governments, businesses, private organizations, and individuals. In an increasingly connected world, the trend is for resources to be accessible from anywhere at any time. Greater access to resources implies more targets and potentially a larger surface area for attacks, which makes securing systems more difficult. Automated and semi-automated solutions are needed to keep up with the deluge of modern threats, but designing such systems requires a distributed architecture to support development and testing. Several such architectures exist, but most only focus on providing a platform for running cyber security experiments as opposed to automating experiment processes. In response to this need, we have built a distributed framework based on software agents which can manage system roles, automate data collection, analyze results, and run new experiments without human intervention. The contribution of this work is the creation of a model for experiment automation and control in a distributed system environment, and this paper provides a detailed description of our framework based on that model.},
keywords={Servers;Automation;Clustering algorithms;Software agents;Computer security;Data collection;cyber security;distributed system;experiment framework;automation;software agent},
doi={10.1109/COMPSACW.2014.26},
ISSN={},
month={July},}
@INPROCEEDINGS{6888941,
author={Lee, Hyojeong and Seibert, Jeff and Hoque, Endadul and Killian, Charles and Nita-Rotaru, Cristina},
booktitle={2014 IEEE 34th International Conference on Distributed Computing Systems}, title={Turret: A Platform for Automated Attack Finding in Unmodified Distributed System Implementations},
year={2014},
volume={},
number={},
pages={660-669},
abstract={Security and performance are critical goals for distributed systems. The increased design complexity, incomplete expertise of developers, and limited functionality of existing testing tools often result in bugs and vulnerabilities that prevent implementations from achieving their design goals in practice. Many of these bugs, vulnerabilities, and misconfigurations manifest after the code has already been deployed making the debugging process difficult and costly. In this paper, we present Turret, a platform for automatically finding performance attacks in unmodified implementations of distributed systems. Turret does not require the user to provide any information about vulnerabilities and runs the implementation in the same operating system setup as the deployment, with an emulated network. Turret uses a new attack finding algorithm and several optimizations that allow it to find attacks in a matter of minutes. We ran Turret on 5 different distributed system implementations specifically designed to tolerate insider attacks, and found 30 performance attacks, 24 of which were not previously reported to the best of our knowledge.},
keywords={Operating systems;Testing;Protocols;Libraries;Real-time systems;Algorithm design and analysis;Optimization;automatic attack finding;distributed systems},
doi={10.1109/ICDCS.2014.73},
ISSN={1063-6927},
month={June},}
@INPROCEEDINGS{6883984,
author={Wilcox, James and Kaleshi, Dritan and Sooriyabandara, Mahesh},
booktitle={2014 IEEE International Conference on Communications (ICC)}, title={DIRECTOR: A distributed communication transport manager for the Smart Grid},
year={2014},
volume={},
number={},
pages={4227-4232},
abstract={This paper presents experimental results which support our proposition that the increase in bandwidth efficiency achievable by dynamically and transparently matching transport protocols and messaging patterns to application requirements outweighs the necessary increase in complexity and overhead of the proposed architecture. In order to validate this claim, DIRECTOR, a middleware solution for managing Smart Grid applications communication requirements has been developed. The results show that distributed systems, such as the Smart Grid data network, can benefit from the scalability and transport layer flexibility provided by the proposed middleware solution by being able to rapidly negotiate transport level communication options at run time. This will allow applications to operate with increased network efficiency and better meet the Quality of Service requirements specified by the applications. The middleware solution will abstract these additional complexities from the applications and provide a simple communications interface which requires no network infrastructure modifications to be supported. The fundamental components of the middleware solution are demonstrated through the use of a real time implementation of the Open Automated Demand Response (OpenADR) Real Time Pricing (RTP) data model. The experimental test bed includes both real hardware nodes (Raspberry Pi Model B) and virtualised nodes residing in a custom network emulator.},
keywords={Middleware;Sockets;Bandwidth;Transport protocols;Real-time systems;Smart grids;Load management;Smart Grid;Middleware;Transport Layer;OpenADR;Real Time Pricing},
doi={10.1109/ICC.2014.6883984},
ISSN={1938-1883},
month={June},}
@INPROCEEDINGS{6882065,
author={Kondikoppa, Praveenkumar and Platania, Richard and Park, Seung-Jong and Keyes, Tom and Kim, Jaegil and Kim, Nayong and Kim, Joohyun and Bai, Shuju},
booktitle={2014 6th International Workshop on Science Gateways}, title={MapReduce-Based RESTMD: Enabling Large-Scale Sampling Tasks with Distributed HPC Systems},
year={2014},
volume={},
number={},
pages={30-35},
abstract={A novel implementation of Replica Exchange Statistical Temperature Molecular Dynamics (RESTMD), belonging to a generalized ensemble method and also known as parallel tempering, is presented. Our implementation employs the MapReduce (MR)-based iterative framework for launching RESTMD over high performance computing (HPC) clusters including our test bed system, Cyber-infrastructure for Reconfigurable Optical Networks (CRON) simulating a network-connected distributed system. Our main contribution is a new implementation of STMD plugged into the well-known CHARMM molecular dynamics package as well as the RESTMD implementation powered by the Hadoop that scales out in a cluster and across distributed systems effectively. To address challenges for the use of Hadoop MapReduce, we examined contributing factors on the performance of the proposed framework with various runtime analysis experiments with two biological systems that differ in size and over different types of HPC resources. Many advantages with the use of RESTMD suggest its effectiveness for enhanced sampling, one of grand challenges in a variety of areas of studies ranging from chemical systems to statistical inference. Lastly, with its support for scale-across capacity over distributed computing infrastructure (DCI) and the use of Hadoop for coarse-grained task-level parallelism, MapReduce-based RESTMD represents truly a good example of the next-generation of applications whose provision is increasingly becoming demanded by science gateway projects, in particular, backed by IaaS clouds.},
keywords={Temperature distribution;Parallel processing;Biological system modeling;Computational modeling;Logic gates;Distributed computing;Scalability;MapReduce;RESTMD;Distributed},
doi={10.1109/IWSG.2014.12},
ISSN={},
month={June},}
@INPROCEEDINGS{6880630,
author={Moonian, Oveeyen and Khedo, Kavi Kumar and Baichoo, Shakuntala and Doomun, Razvi and Cheerkoot-Jalim, Sudha and Nagowah, Soulakshmee D. and Cadersaib, Zarine and Meetoo-Appavoo, Anuja and Doomun, Bibi Rubeena},
booktitle={2014 IST-Africa Conference Proceedings}, title={A secure data access model for the Mauritian healthcare service},
year={2014},
volume={},
number={},
pages={1-9},
abstract={The volume of the data involved in healthcare systems and the sensitivity of the data call for strict, non-obtrusive and efficient access control. This paper presents the design and implementation of a software prototype to demonstrate how Role-Based Access Control (RBAC), supported by context-awareness, can be applied in the Mauritian healthcare service for providing efficient and effective access control to patient's data. The work has consisted of studying different models of Role-Based and Context-Based access control used elsewhere and applying it to the Mauritian healthcare sector. The software prototype is based on information flow in a collaborator healthcare institution. The prototype has been implemented as a distributed system based on the client-server model, with the location of users and time of access being forms of context considered. The prototype has been successfully implemented and tested under different scenarios of data access.},
keywords={Medical services;Access control;Prototypes;Software;Context;Computational modeling;Healthcare;RBAC;context-aware;e-health},
doi={10.1109/ISTAFRICA.2014.6880630},
ISSN={},
month={May},}
@INPROCEEDINGS{6877546,
author={Velusamy, Gandhimathi and Gurkan, Deniz and Narayan, Sandhya and Baily, Stuart},
booktitle={2014 Third GENI Research and Educational Experiment Workshop}, title={Fault-Tolerant OpenFlow-Based Software Switch Architecture with LINC Switches for a Reliable Network Data Exchange},
year={2014},
volume={},
number={},
pages={43-48},
abstract={The switches are essential for forwarding the packets in a local area network. If a switch fails, then the packets are not able to reach their destination, in spite of their long journey from the source. The new trend in Software Defined Networking (SDN) has made the use of software switches such as the OpenvSwitch quite popular. These software switches are used in data centers to connect virtual machines on which application servers are deployed. Such switches have the advantages of software: ease of development and flexibility, with less optimal testing and reliability measures than hardware systems. The Software switches are required to be resilient to failure because the applications servers which are running from the VMs which are connected through them should always be connected with its clients. So fault-tolerance becomes an important aspect in the use of software switches. In this paper, we explore one mechanism for fault tolerance of LINC (Link Is Not Closed), an open source OpenFlow switch, which is written in Erlang programming language. Distributed system, concurrency, and fault-tolerance are built-in features of Erlang. We leverage these features of Erlang to realize a fault-tolerant distributed LINC switch system.},
keywords={Switches;Fault tolerance;Fault tolerant systems;Protocols;Loss measurement;Kernel;OpenFlow;software switch;Erlang;fault-tolerance;reliability;distributed switch network},
doi={10.1109/GREE.2014.17},
ISSN={},
month={March},}
@ARTICLE{6868884,
author={Soares Boaventura, Ricardo and Yamanaka, Keiji and Prado Oliveira, Gustavo},
journal={IEEE Latin America Transactions}, title={Performance Analysis of Algorithms for Virtualized Environments on Cloud Computing},
year={2014},
volume={12},
number={4},
pages={792-797},
abstract={Cloud computing emerges as a new dominant paradigm for distributed systems, as a model that allows users to access the network on demand to a shared pool of computing resources that can be configured, e.g., networks, servers, storage, applications and services. In cloud computing the infrastructure can become available as a service through virtualization using hypervisors. Virtualization is a mechanism to abstract hardware resources and system of a given operating system. Therefore, this type of technology is used in cloud environments through a large set of servers using virtual machine monitors that are located between the hardware and the operating system. However there is a wide spread of hypervisors, each with its own advantages and disadvantages. Therefore, this study is one of the proposals, conducting experiments with algorithms among different classes of algorithms available to virtual environments in a cloud. The goal of these experiments is to determine which factors (virtual machines, operating system and hardware) with of its configurations influence the performance of a deterministic algorithm. These experiments were planned and executed with a basic theory of experimental design. The experimental design is a set of tests using pre-established criteria and scientific statistical mostly with the aim of determining the influence of various factors on the results of a system or process, identifying and noting the reasons that led to changes in the expected response. All experiments results will be analyzed and will assist cloud computing users in the discovery of a possible virtual environment configuration that will allow and to avoid programming execution deft, computational resources loss and monthly costs.},
keywords={Hardware;Algorithm design and analysis;Cloud computing;Clocks;Performance analysis;Virtualization;Virtual machine monitors;Algorithms Design and Analysis;Cloud computing;Experimental Design;Experiments with Algorithms;Virtualization},
doi={10.1109/TLA.2014.6868884},
ISSN={1548-0992},
month={June},}
@INPROCEEDINGS{6846533,
author={Carrozza, Gabriella and Battaglia, Luigi and Manetti, Vittorio and Marotta, Antonio and Canonico, Roberto and Avallone, Stefano},
booktitle={2014 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing}, title={On the Evaluation of VM Provisioning Time in Cloud Platforms for Mission-Critical Infrastructures},
year={2014},
volume={},
number={},
pages={802-810},
abstract={Cloud Computing has risen a great interest over the last years as it represents an enabling technology for flexible and ubiquitous access over the network to a set of shared computing resources. This work comes from an industrial experience aiming at exploiting the cloud potential for virtualizing complex infrastructures such as an entire Air Traffic Center (ATC), a clear example of complex SoS (System of Systems). The use of virtualization is convenient because industry can leverage in-house test-beds to perform distributed testing campaigns in pre-operational phases or to design new automatic fail-over mechanisms for fully distributed systems. In order to realize such mitigation and recovery techniques in the ATC field, indeed, a cloud platform is required to guarantee a low VM provisioning time with the objective of minimizing the service disruption. In this perspective, after having introduced the principal concepts and factors of the provisioning time, we propose a deep analysis and comparison for two different Infrastructure-as-a-Service (IaaS) platforms, namely Open Stack and Open Nebula (using KVM as hypervisor), that were selected through a preliminary scouting phase.},
keywords={Virtual machining;Cloud computing;Benchmark testing;Random access memory;Hardware;Servers;Computational modeling;cloud computing;performance;provisioning time;IaaS;mission-critical},
doi={10.1109/CCGrid.2014.94},
ISSN={},
month={May},}
@INPROCEEDINGS{6845074,
author={Freire, Joào and Postolache, Octavian and Girão, Pedro Silva},
booktitle={2014 21st International Conference on Telecommunications (ICT)}, title={IEEE1451.4 multi-sensing platform for wheelchair users assessment},
year={2014},
volume={},
number={},
pages={27-31},
abstract={The article presents a prototype of a distributed system for wheelchair users assessment characterized by multi-sensing, embedded processing compatible IEEE 1451.4 standard for smart sensors and wireless communication based on IEEE 802.15.4. A set of smart sensing channels are connected in a 1-Wire network supported by an embedded platform that assures the acquisition and appropriate digital processing of photoplethysmogram (PPG), ballistocardigram (BCG) and skin conductivity signals. The wheelchair user identification is performed using LF RFID technology and the information is uploaded in a Electronic Health Record framework. The designed and implemented distributed measurement system was tested and experimental results are included in the paper.},
keywords={Intelligent sensors;Wheelchairs;IEEE 802.15 Standards;Transducers;Wireless communication;multi-sensing;IEEE 1451.4;TEDS;digital filters;photoplethysmography;ballistocardiogram;skin conductance;IEEE 802.15.4 wireless communication},
doi={10.1109/ICT.2014.6845074},
ISSN={},
month={May},}
@INPROCEEDINGS{6838785,
author={Farruque, Nawshad and Osborn, Wendy},
booktitle={2014 IEEE 28th International Conference on Advanced Information Networking and Applications}, title={Efficient Distributed Spatial Semijoins and Their Application in Multiple-Site Queries},
year={2014},
volume={},
number={},
pages={1089-1096},
abstract={Applications exist today that require the management of distributed spatial data. Since spatial data is more complex than non-spatial data, performing distributed queries on it requires the consideration of both local processing (i.e. CPU and I/O) time and data transmission cost. To reduce these costs, one can use a distributed spatial semi join as it eliminates unnecessary objects before their transmission to other sites and the query site. In this paper, we propose both new approaches for representing the spatial semi join in a distributed setting, and their use for processing distributed queries consisting of any number of sites. We have tested our algorithms for four sites, which are a part of an actual working distributed system. We compare our algorithms with respect to data transmission cost, CPU time, I/O time and false positive results. We show that our algorithms are superior in many cases at optimizing the above criteria.},
keywords={Query processing;Spatial databases;Distributed databases;Data communication;Partitioning algorithms;Arrays;Indexes},
doi={10.1109/AINA.2014.132},
ISSN={2332-5658},
month={May},}
@INPROCEEDINGS{6825638,
author={Dhadyalla, Gunwant and Kumari, Neelu and Snell, Timothy},
booktitle={2014 IEEE Seventh International Conference on Software Testing, Verification and Validation Workshops}, title={Combinatorial Testing for an Automotive Hybrid Electric Vehicle Control System: A Case Study},
year={2014},
volume={},
number={},
pages={51-57},
abstract={Embedded electrical systems for passenger vehicles are highly complex distributed systems with varying system boundaries. The surge towards further electrification of vehicles demands the deployment of high voltage systems that provide propulsion through an electric motor as part of a hybrid electric or pure electric drive train. This demands additional care and robust deployment to ensure the safety of the end user and the environment around them. Exhaustive testing is not feasible for large systems and the use of formal approaches can be restrictive. In the presented work a combinatorial test approach is applied to a real Hybrid Electric Vehicle control system as part of a hardware-in-the-loop test system. 2-way, 3-way and mixed strength up to 4-way testing is carried out. The concept of CAN main and local is devised to intercept CAN messages and replace them with the generated combinatorial tests using the HIL simulator's own processor to assure real-time testing. Early results indicate that the approach is effective in exposing incidents in system behavior not normally found during traditional functional testing.},
keywords={Testing;Automotive engineering;Real-time systems;Software;Control systems;Hybrid electric vehicles;Robustness;automotive real-time embedded systems;combinatorial testing;hardware-in-the-loop},
doi={10.1109/ICSTW.2014.6},
ISSN={},
month={March},}
@INPROCEEDINGS{6742964,
author={Waszecki, Peter and Kauer, Matthias and Lukasiewycz, Martin and Chakraborty, Samarjit},
booktitle={2014 19th Asia and South Pacific Design Automation Conference (ASP-DAC)}, title={Implicit intermittent fault detection in distributed systems},
year={2014},
volume={},
number={},
pages={646-651},
abstract={This paper presents a novel approach to detect resources in distributed systems with an increased occurrence of intermittent faults that exceed the amount of unavoidable transient faults caused by environmental phenomena. Intermittent faults occur due to stressed resources and often are a precursor of permanent faults. The proposed early fault detection and diagnosis allows the use of precautionary measures before the permanent failure of a component in a distributed system occurs. In this paper, we present four methods that can implicitly detect intermittent faults by taking the distributed applications and their dependencies into account. Thus, explicit tests are not required which would lead to additional costs and resource load. On the other hand, the implicit approach may considerably reduce the number of plausibility tests compared to the conservative solution with one test per resource. We analyzed and evaluated implementations of the proposed fault detection principle. The experimental results give evidence of the feasibility of our approach and show a comparison of the implemented methods in terms of runtime and detection rate.},
keywords={Equations;Fault detection;Transient analysis;Mathematical model;Vectors;Runtime;Reliability},
doi={10.1109/ASPDAC.2014.6742964},
ISSN={2153-697X},
month={Jan},}
@ARTICLE{6740851,
author={Rengifo, Fabio Andrade and Romeral, Luis and Cusidó, Jordi and Cárdenas, Juan J.},
journal={IEEE Transactions on Energy Conversion}, title={New Model of a Converter-Based Generator Using Electrostatic Synchronous Machine Concept},
year={2014},
volume={29},
number={2},
pages={344-353},
abstract={In this paper, a new method for modeling converter-based power generators in ac-distributed systems is proposed. It is based on the concept of electrostatic synchronous machines. With this new concept, it is possible to establish a simple relationship between the dc and ac side and to study stability in both the small and large signals of the microgrid by considering a dc-link dynamic and high variation in the power supplied. Also, for the purpose of illustration, a mathematical and electrical simulation is presented, based on MATLAB and PSCAD software. Finally, an experimental test is performed in order to validate the new model.},
keywords={Mathematical model;Electrostatics;Generators;Equations;Microgrids;Rotors;Stators;Microgrid control;microgrid model;stability of microgrids},
doi={10.1109/TEC.2014.2303827},
ISSN={1558-0059},
month={June},}
@ARTICLE{6701172,
author={Kelkar, Supriya and Kamal, Raj},
journal={IEEE Transactions on Industrial Electronics}, title={Adaptive Fault Diagnosis Algorithm for Controller Area Network},
year={2014},
volume={61},
number={10},
pages={5527-5537},
abstract={A controller area network (CAN)-based distributed system may develop faults at run-time. These faults need to be detected and diagnosed. This paper proposes a new algorithm named adaptive fault diagnosis algorithm for CAN (AFDCAN). It is designed for low-cost resource-constrained distributed embedded systems. The proposed algorithm detects all faulty nodes on the CAN. It allows new node entry and reentry of repaired faulty nodes during a diagnostic cycle. AFDCAN is found to provide high fault tolerance and to ensure reliable communication. It uses single-channel communication deploying the bus-based standard CAN protocol. A hardware implementation of the proposed algorithm has been used to obtain the results. The results show that the proposed algorithm diagnoses all faults in the system. Analysis of the proposed algorithm proves that the algorithm uses a definite and bounded number of testing rounds and messages to complete one diagnostic cycle.},
keywords={Algorithm design and analysis;Fault diagnosis;Adaptive systems;Protocols;Automotive engineering;Fault detection;Principal component analysis;Adaptive algorithms;automotive applications;controller area network (CAN) protocol;distributed networks;distributed systems;fault diagnosis;real-time systems},
doi={10.1109/TIE.2013.2297296},
ISSN={1557-9948},
month={Oct},}
@ARTICLE{6645368,
author={Leungwattanakit, Watcharin and Artho, Cyrille and Hagiya, Masami and Tanabe, Yoshinori and Yamamoto, Mitsuharu and Takahashi, Koichi},
journal={IEEE Transactions on Software Engineering}, title={Modular Software Model Checking for Distributed Systems},
year={2014},
volume={40},
number={5},
pages={483-501},
abstract={Distributed systems are complex, being usually composed of several subsystems running in parallel. Concurrent execution and inter-process communication in these systems are prone to errors that are difficult to detect by traditional testing, which does not cover every possible program execution. Unlike testing, model checking can detect such faults in a concurrent system by exploring every possible state of the system. However, most model-checking techniques require that a system be described in a modeling language. Although this simplifies verification, faults may be introduced in the implementation. Recently, some model checkers verify program code at runtime but tend to be limited to stand-alone programs. This paper proposes cache-based model checking, which relaxes this limitation to some extent by verifying one process at a time and running other processes in another execution environment. This approach has been implemented as an extension of Java PathFinder, a Java model checker. It is a scalable and promising technique to handle distributed systems. To support a larger class of distributed systems, a checkpointing tool is also integrated into the verification system. Experimental results on various distributed systems show the capability and scalability of cache-based model checking.},
keywords={Model checking;Software;Java;Checkpointing;Synchronization;Scalability;Message systems;Software model checking;software verification;distributed systems;checkpointing},
doi={10.1109/TSE.2013.49},
ISSN={1939-3520},
month={May},}
@ARTICLE{6478809,
author={Han, Son N. and Lee, Gyu Myoung and Crespi, Noel},
journal={IEEE Transactions on Industrial Informatics}, title={Semantic Context-Aware Service Composition for Building Automation System},
year={2014},
volume={10},
number={1},
pages={752-761},
abstract={Service-oriented architecture (SOA) is realized by independent, standardized, and self-describing units known as services. This architecture has been widely used and verified for automatic, dynamic, and self-configuring distributed systems such as in building automation. This paper presents a building automation system adopting SOA paradigm with devices implemented by device profile for web service (DPWS) in which context information is collected, processed, and sent to a composition engine to coordinate appropriate devices/services based on the context, composition plan, and predefined policy rules. A six-phased composition process is proposed to carry out the task. In addition, two other components are designed to support the composition process: building ontology as a schema for representing semantic data and composition plan description language to describe context-based composite services in form of composition plans. A prototype consisting of a DPWSim simulator and SamBAS is developed to illustrate and test the proposed idea. Comparison analysis and experimental results imply the feasibility and scalability of the system.},
keywords={Buildings;Context;Automation;Databases;Sensors;Semantics;Service-oriented architecture;Building automation;device profile for web service (DPWS);semantic web;service composition;service-oriented architecture (SOA)},
doi={10.1109/TII.2013.2252356},
ISSN={1941-0050},
month={Feb},}
@INPROCEEDINGS{6920745,
author={Mohd Anuar, Nur Ifwah and Yusof, Umi Kalsom and Nor Akmal Khalid, Mohd},
booktitle={2013 13th International Conference on Intellient Systems Design and Applications}, title={An artificial immune system algorithm for optimizing the distributed production scheduling in the semiconductor assembly industry},
year={2013},
volume={},
number={},
pages={259-264},
abstract={The semiconductor industry faces one of the most challenging and rapidly evolving problem in each passing time. The importance of satisfying customer demands and achieving higher profits have increased the attention towards production scheduling in semiconductor assembly areas. Combination of productivity and flexibility provided by semiconductor assembly areas have attracted many research efforts for several years. Additionally, recent globalization events have encouraged decentralized efforts in the semiconductor assembly industry by implementing distributed systems. With respect to the previously mentioned problems, several approaches have been introduced namely Petri nets, Ant Colony, Genetic Algorithm, Intelligent Agents, and hybrids of the latter. A new model, using artificial immune system (AIS) algorithm, slightly modified to conform to the manufacturing constraints, has been proposed and tested with an industrial dataset to establish its effectiveness. AIS algorithm is chosen due to its inherent self-learning capability and memory incubation features. The results from the experiments conducted show that the proposed algorithm is effective for the aforementioned problems where the production schedule efficiency increased from 30% to 60%.},
keywords={Production;Schedules;Immune system;Logic gates;Semiconductor industry;distributed production scheduling;artificial immune system},
doi={10.1109/ISDA.2013.6920745},
ISSN={2164-7151},
month={Dec},}
@INPROCEEDINGS{6913213,
author={Qiu, Weiwei and Zheng, Zibin and Wang, Xinyu and Yang, Xiaohu},
booktitle={16th IEEE International Symposium on Object/component/service-oriented Real-time distributed Computing (ISORC 2013)}, title={An efficient fault-tolerant scheduling algorithm for periodic real-time tasks in heterogeneous platforms},
year={2013},
volume={},
number={},
pages={1-7},
abstract={Fault-tolerant real-time scheduling algorithm is one of the most important means to ensure the timeliness and high availability characteristics of fault-tolerant real-time systems. Existing scheduling models for periodic real-time task in heterogeneous platforms typically require the number of processors in the systems to be determined in advance; hence prohibit the scalability and the performance of distributed systems. The algorithms based on these models also require a large number of schedubility tests which lead to long execution time. To address these problems, we propose a primary and backup replica partition based fault-tolerant scheduling algorithm (PBPFT) based on a scalable scheduling model using heterogeneity that does not have to determine the scale of the distributed system in advance. The PBPFT approach also takes advantage of backup copy overlapping and phasing delay techniques to minimize system redundancy, and adopts the processor grouping technique to simplify algorithm complexity. Comprehensive experiments are conducted, and the results validate high resource utilization and commendable performance of our proposed approach.},
keywords={Program processors;Fault tolerance;Fault tolerant systems;Real-time systems;Scheduling algorithms;Partitioning algorithms;Scheduling algorithm;Fault-tolerant;Real-time task;Primary and backup copy;Heterogeneous distributed system},
doi={10.1109/ISORC.2013.6913213},
ISSN={2375-5261},
month={June},}
@INPROCEEDINGS{6825336,
author={Freitas, Allan Edgard Silva and Macêdo, Raimundo José de Araújo},
booktitle={2013 III Brazilian Symposium on Computing Systems Engineering}, title={Performance Evaluation in Hybrid and Dynamic Distributed Systems},
year={2013},
volume={},
number={},
pages={17-22},
abstract={Distributed systems can be characterized by processes that communicate with each other by message passing, through communication channels, and may be located at several computers spread over a communication network. These processes and communication channels are usually characterized by synchronous or asynchronous timeliness behavior, according to the characteristics of underlying system (operating system and communication sub-system). Unlike conventional systems, the timeliness characteristics of dynamic and hybrid distributed systems may vary over time, according to the availability of resources and occurrence of failures. Such systems are becoming common today because of the increasing diversity and heterogeneity of computer networks and associated devices. Due their high complexity, these systems are difficult to test or verify. In this paper, we introduce a simulation tool for such environments, where distinct fault models and timeliness properties can be dynamically assigned to processes and communication channels. Such a tool is meant not only for protocol evaluation but also for prototyping, allowing code reuse in real applications. Besides presenting the simulation tool, we show a few experiments that validate the simulator behavior.},
keywords={Communication channels;Quality of service;Computational modeling;Analytical models;Protocols;Clocks;Computers;hybrid and dynamic distributed systems;simulation;performance evaluation},
doi={10.1109/SBESC.2013.20},
ISSN={2324-7894},
month={Dec},}
@INPROCEEDINGS{6819184,
author={Soelistio, Yustinus Eko},
booktitle={2013 International Conference on Computer, Control, Informatics and Its Applications (IC3INA)}, title={Application distribution model in volunteer computing environment using peer-to-peer torrent like approach},
year={2013},
volume={},
number={},
pages={261-266},
abstract={Volunteer computing has been known as an alternative solution to solve complex problems. It is acknowledged for its simplicity and its ability to work on multiple operating systems. Nonetheless, setting up a server for volunteer computing can be time consuming and relatively complex to be implemented. This paper offer a model which can ease the effort of setting up a server by making the agent works two ways, as seeder and leecher, like P2P torrent approaches. The model consists of measurement units to manage applications to be distributed, system hierarchy, and basic procedures for the server and the agent. The model has been tested in four scenarios using 2,000,000 to 3,000,000 integer data employing up to six nodes. The tests demonstrate speedup in three of the scenarios.},
keywords={Servers;Computational modeling;Availability;Complexity theory;Adaptation models;Measurement units;Connectors;volunteer computing;torrent;P2P},
doi={10.1109/IC3INA.2013.6819184},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6799806,
author={Henke, Christian and Trächtler, Ansgar},
booktitle={2013 International Conference on Connected Vehicles and Expo (ICCVE)}, title={Autonomously driven railway cabin convoys — Communication, control design and experimentation},
year={2013},
volume={},
number={},
pages={277-282},
abstract={The subject of the RailCab project is the development of autonomous railway cabins, which can dynamically group to convoys without mechanical coupling. This enables an on-demand use of these vehicles while retaining the cost and ecological advantages of public transport. In this paper, we present (1) the system concept with focus on the convoy operation mode. Furthermore we explain (2) the system and communication architecture, which is the basis for the coordination and organization of locally distributed systems. The main contribution of this paper is the convoy control system (3). The proposed methods allow automatic and dynamic operation of vehicle convoys. Experimental results from real-life tests (4) validate the system, the communication structure and the control synthesis, thereby ensuring safe and reliable operation.},
keywords={Vehicles;Vehicle dynamics;Merging;Stability analysis;Control systems;Acceleration;Aerodynamics;Intelligent Transportation System;Vehicle control;Convoy stability},
doi={10.1109/ICCVE.2013.6799806},
ISSN={2378-1297},
month={Dec},}
@INPROCEEDINGS{6779898,
author={Al-Rayis, Ektemal and Kurdi, Heba},
booktitle={2013 European Modelling Symposium}, title={Performance Analysis of Load Balancing Architectures in Cloud Computing},
year={2013},
volume={},
number={},
pages={520-524},
abstract={The Cloud computing is a rapidly emerging distributed system paradigm that offers a huge amount of IT resources as utility services at a reduced cost and flexible schemes. The key of such flexibility is an efficient load balancer that offers better management and utilization of virtualized underlying cloud infrastructures. However, most of the existing load balancers in cloud computing are based on either centralized or fully distributed architectures while the idea of harnessing multiple load balancers in a hierarchical structure to improve the sever load and job response time is still under studied. Therefore, this paper, aims at bridging this gap by providing a comparative study between the three load balancing architectures in cloud computing: centralized, decentralized and hierarchical load balancers. The experimental results suggest that the hierarchical architecture for load balancers best suits the public cloud environment and call for further research to test whether these results can be generalized for other types of clouds.},
keywords={Load management;Computer architecture;Cloud computing;Servers;Load modeling;Time factors;Algorithm design and analysis;component;cloud computing;load balancing;simulation},
doi={10.1109/EMS.2013.10},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6741292,
author={Zhi Li and Bachmayer, Ralf},
booktitle={2013 OCEANS - San Diego}, title={The development of a robust Autonomous Surface Craft for deployment in harsh ocean environment},
year={2013},
volume={},
number={},
pages={1-7},
abstract={In this paper, a robust Autonomous Surface Craft (ASC) that is capable of operating in harsh ocean environments near the coastal waters of Newfoundland and Labrador is introduced. The reliable Controller Area Network (CAN) protocol is implemented to build the onboard communication and control system. As a distributed system, the time synchronization between different CAN nodes is resolved using the Time Reference Message (TRM). This ASC integrates a long-distance wireless modem for wireless data logging and supervisory command updates. In addition, a MATLAB based user interface is tentatively used as the ASC control and data display terminal on the dock-side computer. Full-scale resistance and self-propulsion tests are performed at the tow tank of Memorial University, and the drag coefficient and a bilinear thruster model are generated. The sea trials are performed at Holyrood Arm, Conception Bay, Newfoundland, for the validation of the tow tank experimental results.},
keywords={Drag;Oceans;Transmission line measurements;Wireless communication;Robustness;Protocols;Control systems;Autonomous Surface Craft (ASC);Controller Area Network (CAN);resistance test;self-propulsion test},
doi={10.23919/OCEANS.2013.6741292},
ISSN={0197-7385},
month={Sep.},}
@INPROCEEDINGS{6734948,
author={Mansour, Wassim and Marques-Costa, Greicy and Velazco, Raoul},
booktitle={2013 25th International Conference on Microelectronics (ICM)}, title={Robustness with respect to SEU of a hardware-implemented self-converging algorithm},
year={2013},
volume={},
number={},
pages={1-4},
abstract={Self-convergence is a property that allows distributed systems, when perturbed or badly initialized, to recover a correct operation within finite number of calculation steps. In this paper, an FPGA implementation of this algorithm is presented. The intrinsic robustness of the studied implementation with respect to soft errors resulting from radiation effects is explored by means of a fault-injected method. Obtained results put in evidence the fault-tolerance capabilities and robustness of the tested hardware-implemented algorithm.},
keywords={Robustness;Field programmable gate arrays;Random access memory;Indexes;Adders;Clocks;Hardware;Self-stabilization;Single Event Upsets;Fault Injection;Hardware Description Language;FPGA},
doi={10.1109/ICM.2013.6734948},
ISSN={2159-1679},
month={Dec},}
@INPROCEEDINGS{6726905,
author={Higashihata, Daiki and Defago, Xavier},
booktitle={2013 First International Symposium on Computing and Networking}, title={Decreasing the Locks by Isolating the Concurrent Execution in Microprotocol Framework},
year={2013},
volume={},
number={},
pages={247-251},
abstract={We propose the methodology to reduce the range of isolating the event flows by concurrent execution in micro protocol framework which is used in developing distributed system. A micro protocol is a software component to reduce the complexity of designing, developing and testing distributed systems. A distributed system is decomposed into micro protocols based on communication which provides the reusable and flexible composition. There are two types of event flows in a distributed system which are "send", "broadcast" and "receive" event among remote nodes and local node. There is concurrency not only between "send" and "receive" events but also among "send" events and among "receive" events. There is a potential race condition in this kind of distributed environment. Due to providing dynamic composition, the complexity is high in the existing micro protocol framework which supports concurrent execution. Although there is software transactional memory (STM) of which the methodology supports the concurrency, locking resources is necessary. Since the micro protocol is a resource, it implies that locking all related micro protocols is necessary too. In the complex composition of micro protocols, since there are some cases that several microprotols take part in only one event flow, there are few concurrent execution flows due to locking several micro protocols. We propose micro protocol framework which increases concurrent execution as many as possible by means of locking only the possible areas of race condition by concurrency. We evaluate our selective locking methodology compared to locking all micro protocols.},
keywords={Protocols;Distributed algorithms;Complexity theory;Software;Clocks;Concurrent computing;Context;Distributed system;microprotocol framework},
doi={10.1109/CANDAR.2013.43},
ISSN={2379-1896},
month={Dec},}
@INPROCEEDINGS{6722224,
author={Giefing, Gerd-Jürgen},
booktitle={2013 IEEE International Conference on Systems, Man, and Cybernetics}, title={Matrix Transfer Protocol a Unified Communication Framework for Distributed Modules in Cognitive Robotics},
year={2013},
volume={},
number={},
pages={2759-2764},
abstract={This paper introduces the Matrix Transfer Protocol (MTP) as a communication framework for distributed cognitive modules and systems suitable for cognitive robotics applications. It uses a unified intra-module information representation considering findings in neuroscience, robotics, computer networks communication and results in automation. The protocol data unit contains a matrix style data object and supports distributed system states contributing to the global workspace. A demonstrator is presented proving the applicability of the proposed protocol. This demonstrator incorporates augmented reality techniques with a focus on 3D stereo visual computing. Cognitive modules intended for a humanoid service robot head are task-driven integrated. A basic set of tasks has been implemented and tested.},
keywords={Protocols;Robot kinematics;Head;Cameras;Robot vision systems;Cognitive robotics;matrix transfer protocol;information representation;communication framework;unified message format;human-machine system;augmented reality;3D stereo visual computing},
doi={10.1109/SMC.2013.471},
ISSN={1062-922X},
month={Oct},}
@INPROCEEDINGS{6705174,
author={Lotfi, Nasser and Acan, Adnan},
booktitle={2013 IEEE 14th International Symposium on Computational Intelligence and Informatics (CINTI)}, title={Solving multiprocessor scheduling problem using multi-objective mean field annealing},
year={2013},
volume={},
number={},
pages={113-118},
abstract={Multiprocessor scheduling problem is one of the most important issues regarding to parallel programming and distributed system environments. Multiprocessor scheduling is known as a NP-hard problem, hence, applying an exact solution method is not recommended at all. Single-objective type of multiprocessor scheduling problem has already been solved by evolutionary algorithms like genetic algorithms, ant colony optimization, particle swarm optimization, mean field annealing and so on. This paper presents a mean field annealing approach for solving the multi-objective type of this problem. We introduce multi-objective multiprocessor scheduling problem with three objectives and then solve it using mean field annealing approach. Finally, the proposed algorithm is tested over some benchmarks and its effectiveness is compared to NSGA2 and MOGA algorithms. Obtained results show that mean field annealing method leads better Pareto fronts within reasonable computation times.},
keywords={Processor scheduling;Annealing;Sociology;Statistics;Biological cells;Optimization;Schedules;Multiprocessor scheduling;Mean field annealing;Energy function;Multi-objective optimization},
doi={10.1109/CINTI.2013.6705174},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6693058,
author={Hellerstein, Joseph M.},
booktitle={2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)}, title={BOOM: Experiences in language and tool design for distributed systems (keynote)},
year={2013},
volume={},
number={},
pages={1-1},
abstract={With the rapid expansion of cloud infrastructure and mobile devices, distributed systems have quickly emerged as a dominant computing platform. Distributed systems bring significant complexity to programming, due to platform issues including asynchrony, concurrency, and partial failure. Meanwhile, scalable distributed infrastructure—notably “NoSQL” systems—have put additional burdens on programmers by sacrificing traditional infrastructure contracts like linearizable or transactional I/O in favor of high availability. A growing segment of the developer community needs to deal with these issues today, and for the most part developers are still using languages and tools designed for sequential computation on tightly coupled architectures. This has led to software that is increasingly hard to test and hard to trust. Over the past 5 years, the BOOM project at Berkeley has focused on making it easier to write correct and maintainable code for distributed systems. Our work has taken a number of forms, including the development of the Bloom programming language for distributed systems, tools for testing and checking distributed programs, and the CALM Theorem, which connects programmer level concerns of determinism to system-level concerns about the need for distributed coordination. This talk will reflect on this work, and highlight opportunities for improved collaboration between the software engineering and distributed systems research communities.},
keywords={},
doi={10.1109/ASE.2013.6693058},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6699433,
author={Zhabelova, Gulnara and Vyatkin, Valeriy and Zhang, Ziang and Chow, Mo-Yuen},
booktitle={IECON 2013 - 39th Annual Conference of the IEEE Industrial Electronics Society}, title={Agent-based distributed consensus algorithm for decentralized economic dispatch in Smart Grid},
year={2013},
volume={},
number={},
pages={1968-1973},
abstract={The agent based solutions are successfully applied in research and have impressive results. However, at the present, the agent technology is the realm of theory and laboratory simulation. The paper presents an attempt to bridge the gap. To facilitate migration of agent technology to the field, we propose to apply industrial standards such as IEC 61499 to design and implement the agent based algorithms. IEC 61499 is reference architecture for development of distributed systems. As an example of agent based solution to power system problem we consider Incremental Cost Consensus (ICC) algorithm for solving economic dispatch problem. The original system was simulated in Matlab. The agent based 5 node system was developed using IEC 61499 and deployed to 5 industrial controllers, then tested. With the proposed approach the agent algorithms can be designed, tested and deployed within one framework without the overhead of re-coding the solution for different stages. The proposed approach enables directly executable agent solutions and paves the way to the industrial adoption of the agent technology in power system domain.},
keywords={IEC standards;Algorithm design and analysis;MATLAB;Smart grids;Mathematical model;Smart Grid;Distribtued cotnrol;multi-agent system;Automation;power system;IEC61499;consensus algorithm},
doi={10.1109/IECON.2013.6699433},
ISSN={1553-572X},
month={Nov},}
@INPROCEEDINGS{6700055,
author={Dinita, Razvan-Ioan and Wilson, George and Winckles, Adrian and Cirstea, Marcian and Rowsell, Tim},
booktitle={IECON 2013 - 39th Annual Conference of the IEEE Industrial Electronics Society}, title={A novel autonomous management distributed system for cloud computing environments},
year={2013},
volume={},
number={},
pages={5620-5625},
abstract={This paper describes a novel modular design of an autonomous management distributed system (AMDS) for cloud computing environments and it presents its implementation with the Scala programming language. The AMDS was designed from the ground up with distributed deployment, modularity and security in mind, using a full object oriented approach. A key feature of this system is the ability to gather and store information from various networking and monitoring devices from within the same computing cluster. Another key feature is the ability to intelligently control VMWare vSphere local instances based on analysis of collected data and predefined parameters. vSphere in turn, once it receives commands from the AMDS, proceeds to issue instructions to multiple locally monitored ESXi severs in order to maximize energy efficiency, reduce the carbon footprint and minimize running costs. The predefined parameters are based on results from a previous paper written by the authors. The AMDS has been deployed on the authors' test bed and is currently running successfully. Test results show highly potential industrial applications in datacenter energy management and lowering of operating costs.},
keywords={Databases;Power demand;Servers;Java;Hardware;Cloud computing;Monitoring;cloud;distributed;energy;optimisation;software},
doi={10.1109/IECON.2013.6700055},
ISSN={1553-572X},
month={Nov},}
@INPROCEEDINGS{6686389,
author={Sharma, Rashmi and Nitin},
booktitle={2013 Third International Conference on Advances in Computing and Communications}, title={Visualization of Information Theoretic Maximum Entropy Model in Real Time Distributed System},
year={2013},
volume={},
number={},
pages={282-286},
abstract={There are various states of affairs in Natural Language Processing (NLP), Thermodynamics and Image processing, which illustrates the maximum entropy model (MEM) at different levels. An application of the principle of maximum entropy for approximation of fundamental probability distribution can depend on the variables that used for recitation of system. In Real Time Distributed System (RTDS) arrival time, worst-case execution time and deadline are some fundamental attributes of real time tasks that elucidate the activities of entire system. As we all know that for the acceptance test (in real time scheduling algorithms) of task scheduling and in order to govern the dynamics (task migration and duplication), utilization is the only parameter that brings into play in RTDS. All these methodologies need current utilization value (=1) of all participant processors of the system. To the best of our knowledge, this paper first time introduces one more contender that promises great affinity to replace utilization factor. This paper introduces MEM in RTDS arena that works analogous to the utilization factor with some additional advantages in terms of scalability and processor information. With the help of some mathematical derivations and theoretical examples, we would like to broadcast that entropy can be another parameter that is able to govern the dynamics in RTDS parallel to utilization factor.},
keywords={Entropy;Program processors;Real-time systems;Scheduling algorithms;Computational modeling;Uncertainty;Mathematical model;Real Time Distributed System (RTDS);Maximum Entropy Model (MEM);Task Scheduling;Information Theory},
doi={10.1109/ICACC.2013.60},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6681275,
author={Lostrie, K. and De Meulenaere, P. and Temmerman, M. and Van Remortel, N. and Beaumont, W.},
booktitle={2013 Eighth International Conference on P2P, Parallel, Grid, Cloud and Internet Computing}, title={Benchmarking of PCIe-Performance in MicroTCA-Equipment},
year={2013},
volume={},
number={},
pages={474-478},
abstract={In the development of advanced data acquisition, testing or telecommunication equipment, one often relies on modular off-the-shelf processor boards and I/O-boards that are being composed to a single distributed system. To support such architectures, the microTCA (micro Telecommunication Computing Architecture) offers standardized racks and standardized back panel communication protocols connecting the different boards. During the design of such distributed systems, the performance characteristics of the end-to-end communication between different boards in the system are often not sufficiently known. In this paper, we present a setup allowing for benchmarking the performance of the full PCIe-communication path between two microTCA FPGA-boards. The experimental setup is discussed, and it is shown that benchmarking figures for throughput, delay and delay variation of the PCIe end-to-end path can be retrieved.},
keywords={Delays;Benchmark testing;Throughput;Data acquisition;Field programmable gate arrays;Registers;Communications technology;microTCA;PCIe;throughput;delay},
doi={10.1109/3PGCIC.2013.81},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6681311,
author={Stoja, Sebasijan and Vukmirovic, Srdan and Jelacic, Bojan},
booktitle={2013 Eighth International Conference on P2P, Parallel, Grid, Cloud and Internet Computing}, title={Publisher/Subscriber Implementation in Cloud Environment},
year={2013},
volume={},
number={},
pages={677-682},
abstract={In modern-day systems there is a need for a system which allows indirect asynchronous communication during message exchange between clients and servers. This is known as Publisher/Subscriber system. Such system is important for transmitting data in distributed systems. This paper describes Publisher/Subscriber implementation in Microsoft Windows Azure environment for working services in distributed systems. This is useful for the large scale and big data processing because Microsoft Windows Azure is motivated by it. Pub/Sub system is implemented in every service/client in real-time distributed systems, tested with different distributed networks and the results of this type of implementation are shown in this paper.},
keywords={Fabrics;Subscriptions;Graphical user interfaces;Publishing;Real-time systems;Cloud computing;Asynchronous communication;Pub/Sub system;Cloud Computing;Windows Azure},
doi={10.1109/3PGCIC.2013.116},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6676012,
author={Magaña-Lemus, Ernesto and Medina-Ríos, Aurelio and Ramos-Paz, Antonio and Montesinos-González, Víctor H.},
booktitle={2013 10th International Conference on Electrical Engineering, Computing Science and Automatic Control (CCE)}, title={Periodic steady state determination of power systems using graphics processing units},
year={2013},
volume={},
number={},
pages={274-279},
abstract={Modern advances in computer technology will definitely have a great impact on the methodologies used in the expansion and operational planning of power systems. Parallel and distributed systems are the new tendencies in technologies that present a great potential for application in these areas. Graphics processing units (GPUs) have recently become popular in several fields of science due to their capability of performing massively large computational tasks. The use of a GPU for efficiently obtaining the periodic steady state solution in the time domain of large-scale electric power systems with nonlinear components is presented in this paper. Two programming models of the numerical differentiation process are proposed and implemented on a single GPU. The computation time of the simulation performed by co-processing GPU-CPU has been compared with the simulation time required when using a conventional CPU. Several test cases have been analyzed to demonstrate the acceleration of the GPU-CPU simulation and the speed-up achieved.},
keywords={Graphics processing units;Instruction sets;Parallel processing;Steady-state;Power systems;Programming;Computational modeling;CUDA;graphics processing unit;limit cycle;Newton techniques;steady state solution;transition matrix},
doi={10.1109/ICEEE.2013.6676012},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6664661,
author={Kumarswamy, Inamanamelluri and Sandipamu, Tara Kalyani and Prasanth, Venkata},
booktitle={2013 7th Asia Modelling Symposium}, title={Analysis of Islanding Detection in Distributed Generation Using Fuzzy Logic Technique},
year={2013},
volume={},
number={},
pages={3-7},
abstract={The advancement in new technology like fuel cell, wind turbine, photo voltaic and new innovation in power electronics, customer demands for better power quality and reliability are forcing the power industry to shift for distributed generations. Hence distributed generation (DG) has recently gained a lot of momentum in the power industry due to market deregulations and environmental concerns. Islanding occurs when a portion of the distribution system becomes electrically isolated from the remainder of the power system yet continues to be energized by distributed generators. An important requirement to interconnect a DG to power distributed system is the capability of the DG to detect islanding detection. The proposed method develops a fuzzy rule-based classifier that was tested using features for islanding detection in distributed generation. In the developed technique, the initial classification boundaries are found out by using the decision tree (DT). From the DT classification boundaries, the fuzzy membership functions (MFs) are developed and the corresponding rule base is formulated for islanding detection. But some of the fuzzy MFs are merged based upon similarity the measure for reducing the fuzzy MFs and simplifying the fuzzy rule base to make it more transparent. The developed fuzzy rule-based classifier is tested using features with noise up to a signal-to-noise ratio of 20 dB and provides classification results without misdetection, which shows the robustness of the proposed approach for islanding detection for distributed generations in the distribution network.},
keywords={Distributed power generation;Fuzzy logic;Feature extraction;Cranes;Power quality;Reactive power;Generators;Distributed generation; Fuzzy logic; Islanding detection; power quality},
doi={10.1109/AMS.2013.57},
ISSN={2376-1172},
month={July},}
@INPROCEEDINGS{6662704,
author={Sobotka, Jan and Novák, Jiří},
booktitle={2013 IEEE 7th International Conference on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS)}, title={Automation of automotive integration testing process},
year={2013},
volume={01},
number={},
pages={349-352},
abstract={Goal of this paper is to propose an improvement of an automotive distributed system testing process, namely the part called integration testing. The improvement is focused on test cases development, which is today done mostly manually by test engineers. The work is focused on deployment of Model-Based Testing method on field of integration testing.},
keywords={Testing;Automotive engineering;Formal specifications;Switches;Vehicles;Automata;Ignition;Automotive;Integration;Model-Based;Testing},
doi={10.1109/IDAACS.2013.6662704},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6662678,
author={Bergemann, Stephan and Sieck, Jürgen},
booktitle={2013 IEEE 7th International Conference on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS)}, title={Application infrastructure management system for media façades},
year={2013},
volume={01},
number={},
pages={231-234},
abstract={Media façades have becomn increasingly popular during the past years. They do not only decorate buildings but also transform them into interactively usable platforms. However they usually are either only temporary or displaying only static content respectivly only one specific application. Our main goal was to design a software and hardware infrastructure enabling us to run multiple interactive applications on our façade using different combinations of the display technologies provided by the façade. We implemented a flexbile distributed system with a launcher, website and different configurable output devices to specify and run applications on our media façade. Proving the functionallity of the system we implemented and tested different example applications.},
keywords={Media;Rendering (computer graphics);Buildings;Hardware;Protocols;Libraries;media façade;urban screen;interaction},
doi={10.1109/IDAACS.2013.6662678},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6655717,
author={Santone, Antonella and Intilangelo, Valentina and Raucci, Domenico},
booktitle={2013 IEEE Ninth World Congress on Services}, title={Efficient Formal Verification in Banking Processes},
year={2013},
volume={},
number={},
pages={325-332},
abstract={Model checking is a very useful method to verify concurrent and distributed systems which is traditionally applied to computer system design. We examine the applicability of model checking to validation of Business Processes that are mapped through the systems of Workflow Management. The use of model checking in business domain is affected by the state explosion problem, which says that the state space grows exponentially in the number of concurrent processes. In this paper we consider a property-based methodology developed to combat the state explosion problem. Our focus is two fold; firstly we show how model checking can be applied in the context of business modelling and analysis and secondly we evaluate and test the methodology using as a case study a real-world banking workflow of a loan origination process. Our investigations suggest that the business community, especially in the banking field, can benefit from this efficient methodology developed in formal methods since it can detect errors that were missed by traditional verification techniques, and being cost-efficient, it can be adopted as a standard quality assurance procedure. We show and discuss the experimental results obtained.},
keywords={Banking;Business;Model checking;Explosions;Standards;Calculus;Syntactics;Business Process Management; Formal Methods; CCS; Workflow Verification; Banking Process},
doi={10.1109/SERVICES.2013.79},
ISSN={2378-3818},
month={June},}
@INPROCEEDINGS{6644773,
author={Riesch, Christian and Marinescu, Cristian and Rudigier, Manfred},
booktitle={2013 IEEE International Symposium on Precision Clock Synchronization for Measurement, Control and Communication (ISPCS) Proceedings}, title={Measurement of egress and ingress delays of PTP clocks},
year={2013},
volume={},
number={},
pages={113-118},
abstract={Distributed systems evolved to state-of-the-art solutions in many areas, like test, measurement and automation control systems. The protocol defined by the IEEE 1588 standard was designed to achieve clock synchronization among the different components of a distributed system using an unreliable communication network. Since any asymmetry between master and slave impacts the synchronization accuracy, PTP software has to compensate such asymmetries as much as possible. Our paper studies the effects of the hardware propagation delays on the synchronization. We present a setup that allows the direct measurement of these time delays and show results for a PTP clock that is generating the timestamps in the PHY and for a second one that is creating them on a MAC level. We discuss the characteristics of the time delays, and demonstrate the feasibility of the setup by using the measured compensation values in a synchronization experiment.},
keywords={Delays;Synchronization;Oscilloscopes;Clocks;Propagation delay;Hardware;Semiconductor device measurement;IEEE 1588;hardware propagation delay;PTP;synchronization},
doi={10.1109/ISPCS.2013.6644773},
ISSN={1949-0313},
month={Sep.},}
@INPROCEEDINGS{6642191,
author={Ammar, Mohammed E. and Dumont, Guy},
booktitle={2013 5th International Conference on Modelling, Identification and Control (ICMIC)}, title={Identification of paper machines cross-directional models in closed-loop},
year={2013},
volume={},
number={},
pages={3-9},
abstract={Paper machines cross-directional (CD) processes are a class of spatially distributed systems. Due to economic constraints, identification experiments are usually severely limited making the identification of this large dimension multi-variable model challenging. The industrial identification technique uses bump test data where a few actuators are stepped while the CD process is running in open-loop. This paper presents a technique for the identification of paper machines CD models in closed-loop. The spatial interaction matrix is replaced by a noncausal spatial finite impulse response (FIR) model to account for the actuator response in the cross-direction (CD). The non-causal FIR model is identified in a prediction error frame using least squares. Least squares identification delivers parameter uncertainty bounds that translate to bounds on the uncertainties in the spatial interaction matrix which are less than the values assumed in industrial practice. Identifying the spatial model from a rich spatial input signal provides accurate CD response models from limited scans in a low signal-to-noise ratio (SNR). The proposed techniques are illustrated by identification experiments conducted on an industrial paper machine simulator.},
keywords={Actuators;Data models;Mathematical model;Finite impulse response filters;Process control;Biological system modeling;Predictive models;Identification of CD models;Closed-Loop Identification;Noncausal spatial model},
doi={},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6641444,
author={Benosman, Ridha and Barkaoui, Kamel and Albrieux, Yves},
booktitle={2013 International Conference on High Performance Computing Simulation (HPCS)}, title={A new dynamic IPC-memory allocator based on a paging approach},
year={2013},
volume={},
number={},
pages={382-389},
abstract={In a distributed system, multiple processes can communicate and exchange data. These processes can be either local (on the same host machine), or remote (on different ones). The POSIX IPC standard provides a set of mechanisms (such as shared memory, message queues and semaphores) for data sharing and synchronization between local processes. In particular, the shared memory mechanism allows multiple local processes/threads to share the same address space. However, this mechanism only allows the sharing of data that belongs to the basic types (such as integer, float, char, ...) and does not support natively the share of pointers. In addition, the shared memory area presents in the form of a contiguous space, which can lead to fragmentation (internal/external) and compaction problems during use. In this paper, we present a new dynamic shared memory allocator (DSMA) addressing these limitations. Other aspects are also taken into account by the allocator, such as portability, performance (response time for an allocation request) or the consistency of shared data in a concurrent access to the same shared memory space. The allocator is in the form of a C/C++ API, compiled and tested on three operating systems: Linux, Windows and Mac-OS.},
keywords={Compaction;Resource management;Dynamic scheduling;Heuristic algorithms;Operating systems;Standards;Memory management},
doi={10.1109/HPCSim.2013.6641444},
ISSN={},
month={July},}
@INPROCEEDINGS{6613395,
author={Astapov, Sergei and Berdnikova, Julia and Preden, Jürgo-Sören},
booktitle={Proceedings of the 20th International Conference Mixed Design of Integrated Circuits and Systems - MIXDES 2013}, title={A method of initial search region reduction for acoustic localization in distributed systems},
year={2013},
volume={},
number={},
pages={451-456},
abstract={Acoustic localization by means of sensor arrays has a variety of applications, such as environment monitoring and object tracking. These tasks are appealing for implementation on embedded systems, however large dataflows and computational complexity of multi-channel signal processing impede the development of such systems. This paper proposes a method of acoustic localization in distributed systems, such as Wireless Sensor Networks (WSN). The method builds on an optimized localization algorithm of Steered Response Power with Phase Transform (SRP-PHAT) and simplifies it further by reducing the initial search region, in which the sound source is contained. The sensor array is partitioned into sub-blocks, which may be implemented as independent nodes of WSN. For the region reduction two approaches are handled. One is based on Direction of Arrival estimation and the other - on multilateration. Both approaches are tested on real signals for speaker localization and industrial machinery monitoring applications. Experiment results indicate the method's potency in both these tasks.},
keywords={Direction-of-arrival estimation;Acoustics;Monitoring;Wireless sensor networks;Noise;Microphone arrays;Acoustic localization;wireless sensor networks;direction of arrival;multilateration;SRP-PHAT},
doi={},
ISSN={},
month={June},}
@INPROCEEDINGS{6621340,
author={Astoul, A. and Filliter, C. and Rau-Chaplin, A. and Shridhar, K. and Varghese, B. and Varshney, N.},
booktitle={2013 24th International Workshop on Database and Expert Systems Applications}, title={Risk Analytics for Estimating and Validating Magnitude of Earthquake Losses},
year={2013},
volume={},
number={},
pages={26-31},
abstract={The support for data ingestion and data integration from multiple data sources for rapid loss estimation and visualisation is highly desirable in post-event catastrophe modelling systems. In this paper, the design and development of a distributed system for real-time estimation and visualisation of insured losses incurred due to earthquakes is presented. The system incorporates a model for estimating losses due to earthquakes in near real-time and a geo-browser for visualisation of hazard, exposure and loss data. Preliminary validation of the system is performed using as test cases six global earthquakes and their associated industry loss data. The key result is that the system can generate reasonable order of magnitude estimates with limited data.},
keywords={Earthquakes;Data visualization;Estimation;Real-time systems;Hazards;Cities and towns;Portfolios;rapid data processing;rapid loss estimation;catastrophe losses;validation technique},
doi={10.1109/DEXA.2013.9},
ISSN={2378-3915},
month={Aug},}
@INPROCEEDINGS{6610613,
author={Noumeir, Rita and Rose, Jose},
booktitle={2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, title={Testing of Electronic Healthcare Record images and reports viewer},
year={2013},
volume={},
number={},
pages={4767-4770},
abstract={Electronic Health Record (EHR) is a distributed system that results from the cooperation of several heterogeneous and autonomous subsystems. It improves health care by enabling access to prior diagnostic information to assist in health decisions. We focus on the image and imaging report visualization component that needs to interoperate with several other systems to enable healthcare professionals visualize a patient's imaging record. We propose and describe an environment that has been built and used to facilitate the development of the viewer component. This environment has also been used to test and verify the interoperability of the viewer component with other EHR components in accordance with the Integrating the Healthcare Enterprise (IHE) technical framework. It has also been used to demonstrate functionalities, to educate end users, and to train maintenance and test engineers. Moreover, it has been used for acceptance testing as part of an EHR deployment project. We also discuss the challenges we faced in constructing the testing data and describe the software developed to automatically populate the test environment with valid data.},
keywords={Testing;DICOM;Software;Servers;Medical services;Interoperability},
doi={10.1109/EMBC.2013.6610613},
ISSN={1558-4615},
month={July},}
@INPROCEEDINGS{6605877,
author={Yang, Yukai and Abe, Hirotake and Baba, Ken-ich and Shimojo, Shinji},
booktitle={2013 IEEE 37th Annual Computer Software and Applications Conference Workshops}, title={A Scalable Approach to Avoid Incast Problem from Application Layer},
year={2013},
volume={},
number={},
pages={713-718},
abstract={With the development of distributed computing technology and cloud computing technology, the amount of data is rising dramatically, as well as the number of datacenters, where we keep these data for long periods. In these datacenters, TCP protocol is still widely used in most of the network traffic. On the other hand, TCP protocol can also result in severe goodput collapse in a high bandwidth and low latency datacenter environment. One of the problems is called incast problem. In order to efficiently utilize the bandwidth of distributed systems such as datacenter environments, some traffic control mechanisms should be very necessary. In this paper, we discussed about an application layer control mechanism to improve the performance of certain traffic pattern which may cause incast problem. The main idea is to make the data flows be staggered and transferred in serialized manner. We conducted both simulation-based and real-machine-based experiments. In the real-machine-based experiments, we conducted both small scale test and large scale test. We find out that the proposed approach, staggered flows, is able to avoid incast problem and to make the performance better in most cases. Also, this paper shows the potential of scalability that this approach can keep the performance well with the increment of node quantity.},
keywords={Servers;Receivers;Mathematical model;Protocols;Bandwidth;Educational institutions;Equations;datacenter;TCP;congestion control;staggered flows;incast},
doi={10.1109/COMPSACW.2013.106},
ISSN={},
month={July},}
@INPROCEEDINGS{6595505,
author={Bergen, Andreas and Yazir, Yağız Onat and Müller, Hausi A. and Coady, Yvonne},
booktitle={2013 8th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS)}, title={RPC automation: Making legacy code relevant},
year={2013},
volume={},
number={},
pages={175-180},
abstract={Due to the well-known issues with Remote Procedure Calls (RPC), the rather simple idea of modifying legacy applications - that have low spatial locality to the data they need to process - to execute all of their procedures via RPC is not a feasible option. A more realistic and feasible alternative is to provide a self-management mechanism that can dynamically monitor and alter the execution of an existing application by selectively modifying certain procedures to execute remotely when it is necessary to improve spatial locality. In this paper we describe the motivations behind such a self-management mechanism, and outline an initial design. In addition, we introduce our vision for the required profiling component of these applications. As such, we introduce the Automated Legacy system Remote Procedure Call mechanism (ALRPC). It automatically converts existing monolithic C applications into a distributed system semi-automatically. Thus automation is a key criterion for successfully competing with existing remote procedure tools for legacy applications and with newer solutions such as SOAP and REST [12], [21]. ALRPC is the core component to convert monolithic applications into distributable self-adaptive RPC systems. The empirical results collected from our initial experiments show that our mechanism's level of automation outperforms existing industry strength tools and improves development time. At the same time our mechanism is able to correctly function with a significant code base and shows acceptable performance in initial tests.},
keywords={Servers;Automation;Measurement;Monitoring;Bandwidth;Manuals;Service-oriented architecture;Self-managing system;cloud computing;large data;remote procedure calls},
doi={10.1109/SEAMS.2013.6595505},
ISSN={2157-2321},
month={May},}
@INPROCEEDINGS{6580408,
author={Pattison, Richard C. and Baldea, Michael},
booktitle={2013 American Control Conference}, title={Latent storage-enhanced distributed temperature control in hydrogen microreactors},
year={2013},
volume={},
number={},
pages={3735-3740},
abstract={Microchannel catalytic plate reactors are a promising route for converting methane from geographically distributed sources (e.g., shale gas deposits) to hydrogen or liquid transportation fuels. Their capacity is easily scalable by increasing the number of units and thus well suited to distributed production needs. However, miniaturization inevitably reduces the number of available actuators and sensors, and the control of these inherently distributed systems presents challenges. In the present paper, we concentrate on autothermal microchannel reactors producing hydrogen via methane-steam reforming, and introduce a novel temperature control strategy based on the use of a layer of phase-change material (PCM) confined between the reactor plates. The PCM layer, which mitigates temperature excursions through melting-solidification occurring due to fluctuations in hydrogen production rate, acts as the distributed tier of a hierarchical control structure. The supervisory layer consists of a model-based feedforward controller. We also introduce a novel stochastic optimization method for selecting the PCM layer thickness (i.e., for distributed controller “tuning”). The proposed approach is tested in simulations carried out on a detailed 2D reactor model, showing excellent disturbance rejection performance.},
keywords={Inductors;Phase change materials;Mathematical model;Optimization;Heating;Combustion},
doi={10.1109/ACC.2013.6580408},
ISSN={2378-5861},
month={June},}
@INPROCEEDINGS{6576448,
author={Konnov, Igor and Kashina, Olga and Laitinen, Erkki},
booktitle={2013 11th International Symposium and Workshops on Modeling and Optimization in Mobile, Ad Hoc and Wireless Networks (WiOpt)}, title={Vector resource allocation problems in communication networks},
year={2013},
volume={},
number={},
pages={304-309},
abstract={We consider a problem of optimal allocation of a homogeneous resource in spatially distributed systems such as communication networks, where both utilities of users and network expenses must be taken into account. The network is divided into zones which leads to a two-level vector optimization problem and involves non-differentiable functions whose values are computed algorithmically. We propose several approaches to find a solution. Also, new simple subgradient type methods for non-differentiable Pareto optimization problems are suggested. Their performance is illustrated by computational results on test problems.},
keywords={Optimization;Ad hoc networks;Mobile computing;Mobile communication;Wireless networks;Resource management;Conferences;Resource allocation;spatial systems;communication networks;multi-objective optimization;non-differentiable functions;subgradient methods},
doi={},
ISSN={},
month={May},}
@INPROCEEDINGS{6571829,
author={Kundu, Soumya and Backhaus, Scott and Hiskens, Ian A.},
booktitle={2013 IEEE International Symposium on Circuits and Systems (ISCAS)}, title={Distributed control of reactive power from photovoltaic inverters},
year={2013},
volume={},
number={},
pages={249-252},
abstract={As new devices and technologies enter the electrical distribution grid, decentralized control algorithms will become increasingly important. Unlike centralized control where standard optimization procedures can ensure optimal system performance, control algorithms for distributed systems may take a variety of forms. This paper derives a decentralized algorithm that regulates the reactive power output from highly distributed photovoltaic (PV) sources. An objective function is constructed that minimizes voltage deviations and line losses. It is shown that this objective function is minimized by a local control law that regulates the reactive power output of PV inverters. Optimality of the derived control law is tested against central optimization solutions.},
keywords={Reactive power;Inverters;Correlation;Voltage control;Photovoltaic systems;Optimization},
doi={10.1109/ISCAS.2013.6571829},
ISSN={2158-1525},
month={May},}
@INPROCEEDINGS{6569716,
author={Alégroth, Emil and Feldt, Robert and Olsson, Helena H.},
booktitle={2013 IEEE Sixth International Conference on Software Testing, Verification and Validation}, title={Transitioning Manual System Test Suites to Automated Testing: An Industrial Case Study},
year={2013},
volume={},
number={},
pages={56-65},
abstract={Visual GUI testing (VGT) is an emerging technique that provides software companies with the capability to automate previously time-consuming, tedious, and fault prone manual system and acceptance tests. Previous work on VGT has shown that the technique is industrially applicable, but has not addressed the real-world applicability of the technique when used by practitioners on industrial grade systems. This paper presents a case study performed during an industrial project with the goal to transition from manual to automated system testing using VGT. Results of the study show that the VGT transition was successful and that VGT could be applied in the industrial context when performed by practitioners but that there were several problems that first had to be solved, e.g. testing of a distributed system, tool volatility. These problems and solutions have been presented together with qualitative, and quantitative, data about the benefits of the technique compared to manual testing, e.g. greatly improved execution speed, feasible transition and maintenance costs, improved bug finding ability. The study thereby provides valuable, and previously missing, contributions about VGT to both practitioners and researchers.},
keywords={Testing;Graphical user interfaces;Manuals;Companies;Automation;Image recognition;Interviews;Visual GUI testing;Test Automation;Test Maintenance;Empirical;Industrial case study},
doi={10.1109/ICST.2013.14},
ISSN={2159-4848},
month={March},}
@INPROCEEDINGS{6559650,
author={Therdphapiyanak, Jakrarin and Piromsopa, Krerk},
booktitle={2013 10th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology}, title={An analysis of suitable parameters for efficiently applying K-means clustering to large TCPdump data set using Hadoop framework},
year={2013},
volume={},
number={},
pages={1-6},
abstract={In this paper, we determined the appropriate number of clusters and the proper amount of entries for applying K-means clustering to TCPdump data set using Apache Mahout/Hadoop framework. We aim at finding suitable configuration for efficiently analyzing large data set in limited amount of time. Our implementation applied Hadoop for large-scale log analysis with data set from KDD'99 competition as test data. With the distributed system framework, we can analyze a whole data set of KDD'99 by first applying our preprocessing. In addition, we use an anomaly detection model for log analysis. A key challenge is to make anomaly detection work more accurately. For the Kmeans algorithm, a key challenge is to set the appropriate number of the initial cluster (K). Moreover, we discuss whether the number of entries in log files affects the accuracy and detection rate of the system or not. Therefore, our implementation and experimental results describe the appropriate number of cluster and the proper amount of entries in log files. Finally, we show the result of our experiments with accuracy rate and number of initial cluster (K) graph, ROC curve and detection rate and false alarm rate table.},
keywords={Clustering algorithms;Accuracy;Indexes;Intrusion detection;Algorithm design and analysis;Partitioning algorithms;Distributed databases;Log analysis;K-means algorithm;Hadoop;Mahout;Distributed log analysis;KDD'99;Security;Intrusion Detection System;IDS},
doi={10.1109/ECTICon.2013.6559650},
ISSN={},
month={May},}
@INPROCEEDINGS{6555533,
author={Stachel, Joshua R. and Sejdić, Ervin and Ogirala, Ajay and Mickle, Marlin H.},
booktitle={2013 IEEE International Instrumentation and Measurement Technology Conference (I2MTC)}, title={The impact of the internet of Things on implanted medical devices including pacemakers, and ICDs},
year={2013},
volume={},
number={},
pages={839-844},
abstract={The Internet of Things describes multiple distributed systems where all (or most) everyday items include embedded systems in order to connect to the internet. This paradigm has the potential to revolutionize global industry and daily life. Healthcare is once such industry where the Internet of Things may provide great advantages to patients, care givers, and medical institutions. As the number of radio frequency emitters increases under this new paradigm public health and safety must also be taken into account. This paper explores the electromagnetic interference on implantable cardiac rhythm management devices caused by RFID interrogators. A standard electromagnetic compatibility test framework is proposed in order to diagnose the possibility of interference. Also, a mitigation method is proposed and tested. It is shown that the proposed method can reduce the incidence of clinically significant interference by nearly 60%.},
keywords={Radiofrequency identification;Electromagnetic interference;Pacemakers;Internet;Radio frequency;Testing;Pacemaker;Defibrillator;RFID;electromagnetic interference},
doi={10.1109/I2MTC.2013.6555533},
ISSN={1091-5281},
month={May},}
@INPROCEEDINGS{6546075,
author={Olteanu, Alexandru-Corneliu and Tapus, Nicolae and Iosup, Alexandru},
booktitle={2013 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing}, title={Extending the Capabilities of Mobile Devices for Online Social Applications through Cloud Offloading},
year={2013},
volume={},
number={},
pages={160-163},
abstract={Handheld devices are becoming an attractive option for users to interact with their social network, through online social applications. We are witnessing a rapid adoption of smarter devices all around us, which brings with it orders of magnitude in heterogeneity. Thus, researchers in the field of distributed systems are faced with new challenges: How to optimize performance for devices that are so diverse in terms of energy consumption, processing power and communication capabilities? My PhD research focuses on this challenge, adopting techniques for offloading operations from mobile to more powerful cloud-based infrastructure, and brings a three-fold contribution. First, we have characterized and modeled workloads of online social applications, and empirically validated them using traces of hundreds of real applications. Second, we are currently investigating offloading mechanisms, including: communication offloading, lossy performance offloading, and loss less performance offloading. We have been testing and evaluating these mechanisms with several mobile applications, measuring performance and energy consumption. Third, we will create an integrated cloud based offloading system that aims to improve the performance of online social applications. We will empirically evaluate this system using both simulations and open-source real-world applications.},
keywords={Mobile communication;Educational institutions;Energy consumption;Smart phones;Data models;Home automation;mobile;cloud offloading;workload characterization;workload modeling;scheduling;allocation},
doi={10.1109/CCGrid.2013.52},
ISSN={},
month={May},}
@INPROCEEDINGS{6546062,
author={Lübke, Robert and Schuster, Daniel and Schill, Alexander},
booktitle={2013 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing}, title={Reproducing Network Conditions for Tests of Large-Scale Distributed Systems},
year={2013},
volume={},
number={},
pages={74-77},
abstract={Testing distributed systems is difficult, error-prone and time-consuming. Especially scalability tests are hard to perform because of the large number of connected devices. The heterogeneity of these devices (PCs, smart phones, tablets) and their various network access technologies (DSL, cable, WiFi, 3G/4G) even worsen the situation. We argue that the reproduction of network conditions is necessary to provide an adequate test bed for experiments with distributed systems. We therefore propose an architecture that allows performing automated tests of large scale distributed systems including the emulation of network and application behavior.},
keywords={Emulation;Servers;Mobile communication;Network topology;Scalability;Delays;Accuracy},
doi={10.1109/CCGrid.2013.70},
ISSN={},
month={May},}
@INPROCEEDINGS{6513490,
author={Carvajal, Gonzalo and Fischmeister, Sebastian},
booktitle={2013 Design, Automation Test in Europe Conference Exhibition (DATE)}, title={An open platform for mixed-criticality Real-time Ethernet},
year={2013},
volume={},
number={},
pages={153-156},
abstract={For more than one decade, researchers have considered Ethernet as a natural replacement to legacy fieldbuses in modern distributed applications. However, Ethernet components require special modifications and hardware support to provide strict timing guarantees. In general, the high-cost of deploying hardware components limits the experimental validation of proposed solutions in real-world applications. Despite the vast literature, only a few solutions report real implementations, and they are all closed to the research community, hindering further development for constantly evolving applications. This paper introduces Atacama, an on-going effort on deploying the first hardware-accelerated and open-source framework for mixed-criticality communication on multi-hop networks. Specialized modules exploit the principles of traditional fieldbus systems to coordinate communication tasks on real-time stations, and can be easily integrated to and coexist with Commercial Off The Shelf (COTS) devices operating with best-effort traffic. Experimental characterization of implemented prototypes report minimal jitter on 1Gbps links, and show that real-time guarantees are resilient to injected best-effort traffic. The framework is available as an open-source project, enabling researchers to verify the results, explore, test, and deploy new networking solutions for modern distributed systems in real-world scenarios.},
keywords={Real-time systems;Schedules;Ports (Computers);Prototypes;Timing;Jitter;Robustness},
doi={10.7873/DATE.2013.044},
ISSN={1530-1591},
month={March},}
@INPROCEEDINGS{6496368,
author={Tsutsumi, Tomoaki and Koizumi, Minoru and Ebata, Tomoichi and Ohshima, Kohta and Terada, Matsuaki},
booktitle={The International Conference on Information Networking 2013 (ICOIN)}, title={Performance evaluation of synchronous distributed wireless network emulator for high-speed mobility},
year={2013},
volume={},
number={},
pages={151-156},
abstract={The IEEE 1588 Precision Time Protocol Version 2 (PTP v2) standard was developed to establish precise synchronization protocols for distributed nodes, and thus facilitate the development of distributed time-based systems. As an example of an applied system, we developed a distributed wireless network emulator that permits precise and stable synchronized time-based wireless link scheduling. In our system, each agent node simply emulates schedule-based wireless links without exchanging control messages with any other nodes. This emulator is designed to test IP-based protocols for high scalability and high-speed mobility while maintaining high accuracy levels over extended periods of time. In our previous work, we showed the existence of a relationship between packet handling size and accuracy in a developed emulator system. In this study, we evaluate the performance of agent node loads, and their relationship to processing time accuracy, by obtaining time information from PTP v2. These factors are important when deciding future extension policy. From our experimental results, we determined that performance has two kind of influence. Specifically, processing time increases as loading increases, and some processing elements become excessively long under high load conditions. These factors have a significant influence on time-based distributed systems, such as our distributed mobile network emulator, which is designed to handle processes at microsecond speeds.},
keywords={Emulation;Delays;Accuracy;Synchronization;Mobile communication;Wireless networks;Protocols;Distributed wireless network emulator;Scalability;IEEE 1588 PTP v2},
doi={10.1109/ICOIN.2013.6496368},
ISSN={2332-5658},
month={Jan},}
@INPROCEEDINGS{6488549,
author={Hongsen Yu and Wijeratne, Vindya and Zhihong Cai},
booktitle={2013 IEEE 10th Consumer Communications and Networking Conference (CCNC)}, title={An energy-efficient network implemented with Junos SDK},
year={2013},
volume={},
number={},
pages={797-800},
abstract={Nowadays, although the performance of network devices has improved a lot, they consume more and more energy. Therefore, network energy efficiency is increasingly important in this era. In this paper, a software approach to energy-efficient networks is proposed and an application, which can make mid-scale networks energy efficient is demonstrated. In the proposed approach, some idle network interfaces of routers are disabled when the network load is low. Traffic flow converges on certain network paths, as disabled interfaces make some paths unavailable. When network load gets high, disabled interfaces can be re-enabled to ease the traffic pressure and network traffic disperses to different paths. A dynamic routing protocol is configured in order to keep connectivity when the topology changes. This approach is realized by an application, which is developed using Juniper Junos Software Development Kit (Junos SDK) and operates on Juniper programmable routers. Multiple entities of the application operating on different routers in the network form a distributed system. With regards to functionality, the application senses network traffic load; communicates among entities on other routers; determines and executes interface disabling and enabling. A virtual network built by Juniper virtual network simulator is used to test this application. Results reveal that a reasonable amount of energy can be saved by disabling idle interfaces while maintaining the connectivity.},
keywords={Energy efficiency;Routing protocols;Routing;Telecommunication traffic;Topology;Operating systems;energy-efficient;Junos SDK;applicable software;computer networks},
doi={10.1109/CCNC.2013.6488549},
ISSN={2331-9860},
month={Jan},}
@ARTICLE{6468063,
author={Chouvardas, Symeon and Slavakis, Konstantinos and Theodoridis, Sergios},
journal={IEEE Journal of Selected Topics in Signal Processing}, title={Trading off Complexity With Communication Costs in Distributed Adaptive Learning via Krylov Subspaces for Dimensionality Reduction},
year={2013},
volume={7},
number={2},
pages={257-273},
abstract={In this paper, the problem of dimensionality reduction in adaptive distributed learning is studied. We consider a network obeying the ad-hoc topology, in which the nodes sense an amount of data and cooperate with each other, by exchanging information, in order to estimate an unknown, common, parameter vector. The algorithm, to be presented here, follows the set-theoretic estimation rationale; i.e., at each time instant and at each node of the network, a closed convex set is constructed based on the received measurements, and this defines the region in which the solution is searched for. In this paper, these closed convex sets, known as property sets, take the form of hyperslabs. Moreover, in order to reduce the number of transmitted coefficients, which is dictated by the dimension of the unknown vector, we seek for possible solutions in a subspace of lower dimension; the technique will be developed around the Krylov subspace rationale. Our goal is to find a point that belongs to the intersection of this infinite number of hyperslabs and the respective Krylov subspaces. This is achieved via a sequence of projections onto the property sets and the Krylov subspaces. The case of highly correlated inputs that degrades the performance of the algorithm is also considered. This is overcome via a transformation which whitens the input. The proposed schemes are brought in a decentralized form by adopting the combine-adapt cooperation strategy among the nodes. Full convergence analysis is carried out and numerical tests verify the validity of the proposed schemes in different scenarios in the context of the adaptive distributed system identification task.},
keywords={Vectors;Tin;Noise;Topology;Estimation;Equations;Matrix decomposition;Adaptive distributed learning;diffusion;Krylov subspaces;Projections},
doi={10.1109/JSTSP.2013.2246762},
ISSN={1941-0484},
month={April},}
@ARTICLE{6269083,
author={Bondavalli, Andrea and Brancati, Francesco and Flammini, Alessandra and Rinaldi, Stefano},
journal={IEEE Transactions on Instrumentation and Measurement}, title={Master Failure Detection Protocol in Internal Synchronization Environment},
year={2013},
volume={62},
number={1},
pages={4-12},
abstract={During the last decades, the wide advance in the networking technologies has allowed the development of distributed monitoring and control systems. These systems show advantages compared with centralized solutions: heterogeneous nodes can be easily integrated, new nodes can be easily added to the system, and no single point of failure. For these reasons, distributed systems have been adopted in different fields, such as industrial automation and telecommunication systems. Recently, due to technology improvements, distributed systems are also adopted in the control of power-grid and transport systems, i.e., the so-called large-scale complex critical infrastructures. Given the strict safety, security, reliability, and real-time requirements, using distributed systems for controlling such critical infrastructure demands that adequate mechanisms have to be established to share the same notion of time among the nodes. For this class of systems, a synchronization protocol, such as the IEEE 1588 standard, can be adopted. This type of synchronization protocol was designed to achieve very precise clock synchronization, but it may not be sufficient to ensure safety of the entire system. For example, instability of the local oscillator of a reference node, due to a failure of the node itself or to malicious attacks, could influence the quality of synchronization of all nodes. In recent years, a new software clock, the reliable and self-aware clock (R&SAClock), which is designed to estimate the quality of synchronization through statistical analysis, was developed and tested. This statistical instrument can be used to identify any anomalous conditions with respect to normal behavior. A careful analysis and classification of the main points of failure of IEEE 1588 standard suggests that the reference node, which is called master, is the weak point of the system. For this reason, this paper deals with the detection of faults of the reference node(s) of an of IEEE 1588 setup. This paper describes and evaluates the design of a protocol for timing failure detection for internal synchronization based on a revised version of the R&SAClock software suitably modified to cross-exploit the information on the quality of synchronization among all the nodes of the system. The experimental evaluation of this approach confirms the capability of the synchronization uncertainty, which is provided by R&SAClock, to reveal the anomalous behaviors either of the local node or of the reference node. In fact, it is shown that, through a proper configuration of the parameters of the protocol, the system is able to detect all the failures injected on the master in different experimental conditions and to correctly identify failures on slaves with a probability of 87%.},
keywords={Synchronization;Protocols;Reliability;Uncertainty;Standards;Local oscillators;Peer to peer computing;Failure detection;precision time protocol (PTP);reliable and self-aware clock (R&SAClock);synchronization uncertainty;synchronization},
doi={10.1109/TIM.2012.2209916},
ISSN={1557-9662},
month={Jan},}
@ARTICLE{8130919,
author={Dan, Haitao and Hierons, Robert M.},
journal={The Computer Journal}, title={Controllability Problems in MSC-Based Testing},
year={2012},
volume={55},
number={11},
pages={1270-1287},
abstract={In testing systems with distributed interfaces/ports, we may place a separate tester at each port. It is known that this approach can introduce controllability problems which have received much attention in testing from finite state machines. Message sequence charts (MSCs) form an alternative, commonly used, language for modelling distributed systems. However, controllability problems in testing from MSCs have not been thoroughly investigated. In this paper, controllability problems in MSC test cases are analysed with three notions of observability: local, tester and global. We identify two types of controllability problem in MSC-based testing. It transpires that each type of controllability problem is related to a type of MSC pathology. Controllability problems of timing are caused by races but not every race causes controllability problems; controllability problems of choice are caused by non-local choices and not every non-local choice causes controllability problems. We show that some controllability problems of timing are avoidable and some controllability problems of choice can be overcome when testers have better observational power. Algorithms are provided to tackle both types of controllability problems. Finally, we show how one can overcome controllability problems using a coordination service with status messages based on algorithms developed in this paper.},
keywords={testing;controllability problems;message sequence charts;race;non-local choice},
doi={10.1093/comjnl/bxr125},
ISSN={1460-2067},
month={Nov},}
@INPROCEEDINGS{7087783,
author={Eswari, R. and Nickolas, S.},
booktitle={Fourth International Conference on Advances in Recent Technologies in Communication and Computing (ARTCom2012)}, title={Solving multi-objective task scheduling for heterogeneous distributed systems using firefly algorithm},
year={2012},
volume={},
number={},
pages={57-60},
abstract={The task scheduling problem with makespan and reliability objectives is solved for heterogeneous distributed systems using a new nature inspired metaheuristic algorithm, firefly algorithm. The proposed method is tested for the real application graph and compared with the existing algorithms such as modified genetic algorithm (MGA) and biobjective genetic algorithm (BGA). The results show that the firefly based algorithm can be able to produce minimal completion time with maximal reliability within minimum number of generations. The convergence capability of the algorithm is also demonstrated.},
keywords={static task scheduling;heterogeneous distributed systems;firefly algorithm;multiple objectives},
doi={10.1049/cp.2012.2494},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6512435,
author={Abbas, Riasat},
booktitle={2012 International Conference on Multimedia Computing and Systems}, title={Human perceived quality-of-service for multimedia applications},
year={2012},
volume={},
number={},
pages={1-6},
abstract={Usage of multimedia applications is increasing day by day in human social life, medical, military and businesses. Providing the expected quality of service (QoS) to end users is a challenging work for multimedia experts and companies in high technology environment. Delivering the good quality of services (QoS) to end users in distributed systems is based on multiple reliable components of software and hardware. Furthermore, human perceived quality of services are also depended on multiple factors such as multimedia application platform, middleware components and quality of service component architectures. This work addresses the quality of service components architecture particularly assessing the quality of service as perceived by end users. This paper discussed major components of QoS architecture in sense of perceiving the quality to end user and described the role of objective and subjective methods. For testing the quality-of-service for multimedia application, this paper evaluated subjective and objective methods and selected the successfully subjective method for assessing the video quality by end users. Furthermore, described the experiment results and data which collected for assessing perceived quality of multimedia by end users.},
keywords={Quality-of-Service;User service components and middleware components;Platform level components},
doi={10.1109/ICMCS.2012.6512435},
ISSN={},
month={May},}
@INPROCEEDINGS{6495991,
author={Tyutlyaeva, Ekaterina and Kurin, Evgeny and Moskovsky, Alexander and Konuhov, Sergey},
booktitle={2012 SC Companion: High Performance Computing, Networking Storage and Analysis}, title={Poster: Using Active Storages for Seismic Data Processing},
year={2012},
volume={},
number={},
pages={1391-1391},
abstract={This poster presents an approach to distributed seismic data processing using Seismic Un*x Software and the Active Storage system based on TSim C++ template library and the Lustre file system. Active Storage concept implies the use of distributed system architecture, where each node has processing and storage capabilities. Data are distributed accross these nodes, computational tasks are submitted to the most suitable nodes, to reduce network traffic. The main benefits of our approach are the following: - Effective storing and processing of seismic data with minimal changes in CWP/SU modules - Performance testing shows that the Active Storage system is effective - Meta-programming with C++ templates permits a flexible implementation of scheduling algorithms The study analyzes performance results of the developed system as well as the usability of Active Storage and Seismic Unix integration. In the nearest future, we will get results for a 1.2 TB data file.},
keywords={active stoage;scheduling;load balancing},
doi={10.1109/SC.Companion.2012.208},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6489544,
author={Yang, Jinsong and Sandström, Kristian and Nolte, Thomas and Behnam, Moris},
booktitle={Proceedings of 2012 IEEE 17th International Conference on Emerging Technologies Factory Automation (ETFA 2012)}, title={Data Distribution Service for industrial automation},
year={2012},
volume={},
number={},
pages={1-8},
abstract={The IEC 61499 is an open standard for the next generation of distributed control and automation. Data Distribution Service for Real-Time Systems (DDS) is a specification of a publish/subscribe middleware for distributed systems, created by the Object Management Group (OMG) to standardize a data-centric publish-subscribe programming model for distributed systems. This paper evaluates the DDS communication performance based on a model built within the IEC 61499 standard and compares it with the traditional socket based solution for communication. According to the test results, the DDS communication has the potential to reduce the complexity and is suggested as a suitable solution for some classes of industrial control systems.},
keywords={},
doi={10.1109/ETFA.2012.6489544},
ISSN={1946-0759},
month={Sep.},}
@INPROCEEDINGS{6481042,
author={Pletea, Daniel and Pop, Florin and Cristea, Valentin},
booktitle={2012 14th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing}, title={Speculative Genetic Scheduling Method for Hadoop Environments},
year={2012},
volume={},
number={},
pages={281-286},
abstract={The existence of computational intensive applications with a high degree of granularity which have to be executed as quickly as possible or sometimes with budget limitations makes the scheduling problem to be very important in today's distributed systems. In the context of the distributed systems being used by a various community of users, the quality and precision of the scheduling must be as close to optimality as possible. We present in this paper a scheduling method based on a genetic algorithm which respects the deadline constraints. Based on this model we developed a scheduler for MapReduce applications which also takes into account the heterogeneity of distributed systems. The developed scheduler was created for Hadoop platform, an already successful framework, and it can be built in conjunction with other existing Hadoop's schedulers like: fair-scheduler or capacity-scheduler. We tested and validated this model by testing its scalability and by comparing it with existing solutions in Hadoop. The novelty of this solution is that compared to other planners in Hadoop it meets the deadline and budget constraints and thus provides trust for the Hadoop platform.},
keywords={Biological cells;Sociology;Statistics;Genetic algorithms;Processor scheduling;Program processors;Genetics;Scheduling;Heterogeneous Distributed Systems;MapReduce;Hadoop;Genetic Algorithm;Speculative Techniques},
doi={10.1109/SYNASC.2012.62},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6424672,
author={Vieira da Silva Filho, Elenilson and de Sousa e Silva, Glauco and Neves de Oliveira, Hugo and Alves Ferreira, Anderson Vinícius and Gomes de Melo, Erick Augusto and Aires Tavares, Tatiana and Matos Bezerra Motta, Gustavo Henrique and Lemos de Souza Filho, Guido},
booktitle={2012 IEEE International Symposium on Multimedia}, title={A Strategy of Multimedia Reflectors to Encryption and Codification in Real Time},
year={2012},
volume={},
number={},
pages={278-285},
abstract={The constant need of sharing data in information systems leads to the development of more complex and creative solutions to the physical or cost limitations of the nowadays technology. The main problems of a distributed system include: the huge information volume by time interval carried over the network infrastructure, and the confidentiality of the ongoing data. Going into the media transmission sub area, there are even more restrictions to be considered. Error or delay, for example, can drastically impact the user experience in real-time transmission. In this context, this paper proposes a tool for performing efficient and secure distribution and encryption of video streams. This tool was implemented and applied in several contexts. In order to validate the tool in a set of possible situations, tests of the video reflector were made using several sets of parameters evolving variations of video codecs and presence or absence of cryptography.},
keywords={Encryption;Streaming media;Media;Multimedia communication;Real-time systems;Educational institutions;reflector;streaming;real-time transmission;cryptography},
doi={10.1109/ISM.2012.59},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6418319,
author={Paralič, M. and Martončík, J.},
booktitle={2012 IEEE 10th International Conference on Emerging eLearning Technologies and Applications (ICETA)}, title={Support for management of programming assignments - Automated grading},
year={2012},
volume={},
number={},
pages={285-289},
abstract={In the paper we deal with question, how the practical skills in programming of complex - possibly distributed - systems can be achieved in an effective way. Based on our previous experience with the highly specialized system ShareMe, the generic system GLab is introduced. It offers learning environment that provides not only the description of the laboratory tasks to be implemented in Java, but also covers electronic submission, automatic grading and online access to grading and test results of any Java-based application. In distinction to ShareMe it is an open learning environment that could be used potentially for any software project implemented in Java. We focus on the subsystem called GLabTest that support automated grading and peer-to-peer system for sharing files ShareMe is used for its validation.},
keywords={Testing;Java;Peer to peer computing;Programming profession;Electronic learning;Educational institutions;Automated grading;E-learning;Framework;Programming Skills Distributed Systems},
doi={10.1109/ICETA.2012.6418319},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6419480,
author={Lutovac, Maja and Dimic, Zoran and Ferenc, Goran and Vidakovic, Jelena and Bucan, Mirko},
booktitle={2012 20th Telecommunications Forum (TELFOR)}, title={Virtual robot in distributed control system},
year={2012},
volume={},
number={},
pages={1401-1404},
abstract={Before the development of robotic systems, it is of great importance to perform verification of various types of scenarios within the machine work and testing. That is especially important in designing the machine with new control logic, before releasing the machine. This paper describes 3D simulator, as part of the offline system, within distributed system for robot control. It is developed in Lola Institute and it provides robot control and monitoring. Parts of the developed system communicate via CORBA protocol, allowing remote control of the robot system through an intuitive graphical user interface.},
keywords={Telecommunications;3D simulator;distribuirani sistem;upravljanjerobotima},
doi={10.1109/TELFOR.2012.6419480},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6384142,
author={Carrozza, Gabriella and Loffreda, Massimo and Manetti, Vittorio},
booktitle={2012 7th International Conference on System of Systems Engineering (SoSE)}, title={Exploiting Cloud Computing for enabling distributed testing of complex systems: The SELEX-SI roadmap},
year={2012},
volume={},
number={},
pages={350-355},
abstract={Complex systems are usually made up of several heterogeneous components glued together to get a System of System (SoS) demanding more and more effort in terms of integration, testing and maintenance due to the number of components, as well as to the several sources of failures that rise from heterogeneity. On the other hand, the strict reliability requirements of these systems ask for massive testing campaigns since they mostly fail due to software defects that can be either triggered systematically during system execution or manifest in a transient way during its operational phase. In the SELEX-SI scenario, where systems get developed and tested across different premises distributed all over Europe, performing traditional, manual, and on-site testing becomes dramatically expensive in terms of time and human resources. Cloud computing represents the most promising way for allowing the seamless access to distributed testbed from any site and for allowing remote testing activities, either at system and integration level. A cloud based infrastructure in charge of connecting all the company premises would allow to run testing experiments from anywhere and, more important, the possibility of reproducing distributed systems deployment scenarios to run integration testing in a pre-installation phase thus dramatically reducing company costs. This paper aims to describe the i) cloud research roadmap that SELEX-SI has been designing, ii) the architectural design of the cloud infrastructure and iii) the real ROI that the company expect from introducing such an innovation into the traditional software production process.},
keywords={Testing;Cloud computing;Companies;Virtual machining;Stress;cloud computing;Open Source;testing},
doi={10.1109/SYSoSE.2012.6384142},
ISSN={},
month={July},}
@INPROCEEDINGS{6397355,
author={Tomić, J. and Slankamenac, M. and Kušljević, M. and Živanov, M.},
booktitle={2012 15th International Power Electronics and Motion Control Conference (EPE/PEMC)}, title={A virtual laboratory for teaching frequency estimation techniques},
year={2012},
volume={},
number={},
pages={DS3e.1-1-DS3e.1-6},
abstract={Recent developments in virtual instrument technologies, remote measurement, distributed systems, and interactive educational environments have greatly changed the traditional approach to teaching and practical experimentation at any educational level, from technical high schools and undergraduate academic courses, to continuous education and training in the industry. This paper presents the education methods and practical realization of a virtual laboratory for students' education in the field of frequency estimation techniques. A virtual instrument has been developed using LabVIEW software package and the method based on measurement of three consecutive sample signals was applied to measure the unknown frequency of the electrical power network. Unwanted noise is eliminated from the useful signal using adaptive filtering, and greater accuracy of measurements is achieved. Students have the opportunity to change the speed of data acquisition and number of samples, in the aim of testing the proposed measurement method. Remote laboratory is developed in LabVIEW software package and the measurement data are sent to the Internet over TCP/IP protocol. The system consists of a server with measurement equipment and one or more clients which have independent access to the measurement data. The realized virtual instrument was used to do measurements in a real power system network.},
keywords={Instruments;Power harmonic filters;Frequency estimation;Laboratories;Harmonic analysis;Finite impulse response filter;Instrumentation and measurement education;frequency estimation;virtual laboratory},
doi={10.1109/EPEPEMC.2012.6397355},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6412307,
author={Firdhous, Mohamed and Hassan, Suhaidi and Ghazali, Osman},
booktitle={TENCON 2012 IEEE Region 10 Conference}, title={Hysteresis-based robust trust computing mechanism for cloud computing},
year={2012},
volume={},
number={},
pages={1-6},
abstract={Cloud computing has been the new paradigm in distributed systems where users can access computing resources and pay only for usage similar to other utilities like electricity, water, gas and telephony. Service Level Agreements signed at the beginning between the clients and service providers stipulate conditions of the services including the QoS requirements. Trust can be used to quantify the QoS levels of providers and rank them according to their performances. Hence trust management systems can play an important role in identifying the right service provider who would maintain the QoS at the levels required by the clients. Researchers have proposed several trust computing mechanisms based on different techniques and trust metrics on the literature. Almost all of these mechanisms increment or decrement the trust scores monotonously based on the inputs. This is a major vulnerability that can be exploited by adversaries to force the trust scores towards extreme values. In this paper, the authors propose a novel trust computing mechanism based on hysteresis function which requires extra efforts to force the output from one end to the other. Hysteresis functions are immune to small changes and hence can be used to protect the system from sporadic attacks. The proposed mechanism has been tested using simulations. The test results show that the trust scores computed using the proposed mechanism are more robust and stable in the face of attacks than other mechanisms.},
keywords={Quality of service;Cloud computing;Time factors;Magnetic hysteresis;Entropy;Hysteresis;Hardware;trust computing;cloud computing;hysteresis;quality of service},
doi={10.1109/TENCON.2012.6412307},
ISSN={2159-3450},
month={Nov},}
@INPROCEEDINGS{6407878,
author={Mangala, N. and Jacob, Abey and Prahlad Rao, B. B. and Chattopadhyay, Subrata},
booktitle={2012 Third International Conference on Emerging Applications of Information Technology}, title={Comprehensive testing methodology for the operational National Grid Computing infrastructure — GARUDA},
year={2012},
volume={},
number={},
pages={129-134},
abstract={Grid computing is a collaborative e-infrastructure to solve large multidisciplinary problems having interrelated demands of data, compute, scientific instruments and increased importance of collaboration. Grid is a distributed system which enables sharing, selection, and aggregation of heterogeneous geographically distributed autonomous resources dynamically depending on their availability, capability, performance, and users QoS requirements. Validating such a vast system has additional complexities of heterogeneity, multiple administrative policies, and issues of multi-layer software interactions. GARUDA is the Indian National Grid Computing Initiative connecting over 65 partnering institutes across 30 cities of India and aggregating over 70TF-15TB Compute-Storage power. The authors present the software engineering challenges and validation methodologies for GARUDA, to make it a production level grid. Integrated validation of GARUDA through Application testing, Health Monitoring, Middleware-Software testing, Performance benchmarking and Stress testing are described.},
keywords={Testing;Monitoring;Middleware;Availability;Grid computing;Stress;Testing;Validation;Grid Computing;Distributed Computing},
doi={10.1109/EAIT.2012.6407878},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6385066,
author={Guan, Qiang and Chiu, Chi-Chen and Fu, Song},
booktitle={2012 IEEE 18th Pacific Rim International Symposium on Dependable Computing}, title={CDA: A Cloud Dependability Analysis Framework for Characterizing System Dependability in Cloud Computing Infrastructures},
year={2012},
volume={},
number={},
pages={11-20},
abstract={Cloud computing has become increasingly popular by obviating the need for users to own and maintain complex computing infrastructure. However, due to their inherent complexity and large scale, production cloud computing systems are prone to various runtime problems caused by hardware and software failures. Dependability assurance is crucial for building sustainable cloud computing services. Although many techniques have been proposed to analyze and enhance reliability of distributed systems, there is little work on understanding the dependability of cloud computing environments. As virtualization has been an enabling technology for the cloud, it is imperative to investigate the impact of virtualization on the cloud dependability, which is the focus of this work. In this paper, we present a cloud dependability analysis (CDA) framework with mechanisms to characterize failure behavior in cloud computing infrastructures. We design the failure-metric DAGs (directed a cyclic graph) to analyze the correlation of various performance metrics with failure events in virtualized and non-virtualized systems. We study multiple types of failures. By comparing the generated DAGs in the two environments, we gain insight into the impact of virtualization on the cloud dependability. This paper is the first attempt to study this crucial issue. In addition, we exploit the identified metrics for failure detection. Experimental results from an on-campus cloud computing test bed show that our approach can achieve high detection accuracy while using a small number of performance metrics.},
keywords={Measurement;Cloud computing;Servers;Virtual machine monitors;Correlation;Virtual machining;Cloud Computing;Dependability;Evaluation;Virtualization;Failure Management},
doi={10.1109/PRDC.2012.10},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6384371,
author={Avadanii, A. and Nicolae, M.},
booktitle={2012 IEEE 18th International Symposium for Design and Technology in Electronic Packaging (SIITME)}, title={An embedded architecture for distributed processing: A practical solution for combining peripherals and digital processing},
year={2012},
volume={},
number={},
pages={177-181},
abstract={This paper covers the hardware and basic software design of a multifunctional module that offers the peripheric devices specific to a microcontroller, while having a digital signal processor core, in order to be used in robots or decentralized industrial applications. Considering the module has a field bus interface, a distributed system can be constructed by interconnecting a number of such devices. Unlike PLCs, one module offers all the functionality needed, thus making each system node able to assume other node's tasks in case of failure. A test application consisting of two PID controllers and a supervisor was implemented using three modules.},
keywords={Robots;Microcontrollers;Process control;Pulse width modulation;Electronics packaging;Computer architecture;Hardware;embedded;architecture;distributed processing},
doi={10.1109/SIITME.2012.6384371},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6382446,
author={Hall, Brendan and Driscoll, Kevin and Schweiker, Kevin},
booktitle={2012 IEEE/AIAA 31st Digital Avionics Systems Conference (DASC)}, title={Verification and validation of distributed flight critical systems},
year={2012},
volume={},
number={},
pages={9D3-1-9D3-12},
abstract={This paper describes Honeywell's work on a NASA-sponsored multi-year program for verification and validation (V&V) of flight critical systems. The motivation for this work comes from the increasing complexity of integrated systems and the fact that there is often a gap between formal theory and real-world systems. For example, Byzantine fault tolerance is often overlooked in real-world systems, even though such failures have caused system-level failures [1], and the theory behind tolerating these failures has been well explored. The goals of this effort include: (1) providing advanced analytical, architectural, and testing capabilities to enable sound assurance of safety critical properties for distributed systems; (2) establishing a collection of reusable models supporting the validation and verification of a broad array of distributed systems, which enable effective engineering trade-offs to resolve debates about “best” design approaches; (3) improving open standards such as the Architecture Analysis Design Language (AADL); and (4) advancing the state-of-the-art in formal analysis and modeling tool chains.},
keywords={Protocols;Fault tolerance;Fault tolerant systems;Topology;Analytical models;Safety},
doi={10.1109/DASC.2012.6382446},
ISSN={2155-7209},
month={Oct},}
@INPROCEEDINGS{6379239,
author={Alexandrescu, Adrian and Agavriloaei, Ioan and Craus, Mitica},
booktitle={2012 16th International Conference on System Theory, Control and Computing (ICSTCC)}, title={A task mapping simulation framework for comparing the performance of mapping heuristics in various scenarios},
year={2012},
volume={},
number={},
pages={1-6},
abstract={Heterogeneous high-computing distributed systems need to process tasks as efficiently as possible by mapping each task to the most suitable machine from the system. Mapping heuristics can be used to solve this problem, but the performance of these heuristics depend on the environment in question. In this paper we propose a highly-customizable Task Mapping Framework for comparing heuristics that can be used in various scenarios based on performance metrics. Our framework was used to test ten mapping heuristics in eight scenarios using four performance metrics: the makespan, the load imbalance, the algorithm's execution time and the success rate. The tasks used in the simulation had priorities and soft-deadlines, and the scenarios focused on comparing between a low and a high number of tasks, consistent and inconsistent ETC matrices, and a low and a high heterogeneity using a uniform and a gamma random distribution of the tasks' execution times. This framework proved to be an efficient tool for determining the best mapping heuristics in different scenarios.},
keywords={Heuristic algorithms;Measurement;Availability;Load modeling;XML;Abstracts;Computers},
doi={},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6375644,
author={Nguyen, Huu Nghia and Poizat, Pascal and Zaïdi, Fatiha},
booktitle={2012 IEEE 14th International Symposium on High-Assurance Systems Engineering}, title={Online Verification of Value-Passing Choreographies through Property-Oriented Passive Testing},
year={2012},
volume={},
number={},
pages={106-113},
abstract={Choreography supports the specification, with a global perspective, of the interactions between roles played by peers in a collaboration. Choreography conformance testing aims at verifying whether a set of distributed peers collaborates wrt. a choreography. Such collaborations are usually achieved through information exchange, thus taking data into account during the testing process is necessary. We address this issue by using a non-intrusive passive testing approach based on functional properties. A property can express a critical (positive or negative) behavior to be tested on an isolated peer (locally) or on a set of peers (globally). We support online verification of these kind of properties against local running traces of each peer in a distributed system where no global clock is needed. Our framework is fully tool supported.},
keywords={Testing;Context;Semantics;Collaboration;Clocks;Credit cards;choreography;conformance checking;passive testing;online verification;tools},
doi={10.1109/HASE.2012.15},
ISSN={1530-2059},
month={Oct},}
@INPROCEEDINGS{6375632,
author={Figueiras, João and GrønbæK, Jesper and Ceccarelli, Andrea and Schwefel, Hans-Peter},
booktitle={2012 IEEE 14th International Symposium on High-Assurance Systems Engineering}, title={GPS and Electronic Fence Data Fusion for Positioning within Railway Worksite Scenarios},
year={2012},
volume={},
number={},
pages={17-23},
abstract={Context-dependent decisions in safety-critical applications require careful consideration of accuracy and timeliness of the underlying context information. Relevant examples include location-dependent actions in mobile distributed systems. This paper considers localization functions for personalized warning systems for railway workers, where the safety aspects require timely and precise identification whether a worker is located in a dangerous (red) or safe (green) zone within the worksite. The paper proposes and analyzes a data fusion approach based on low-cost GPS receivers integrated on mobile devices, combined with electronic fences strategically placed in the adjacent boundaries between safe and unsafe geographic zones. An approach based on the combination of a Kalman Filter for GPS-based trajectory estimation and a Hidden Markov Model for inclusion of mobility constraints and fusion with information from the electronic fences is developed and analyzed. Different accuracy metrics are proposed and the benefit obtained from the fusion with electronic fences is quantitatively analyzed in the scenarios of a single mobile entity: By having fence information, the correct zone estimation can increase by 30%, while false alarms can be reduced one order of magnitude in the tested scenario.},
keywords={Hidden Markov models;Global Positioning System;Estimation;Accuracy;Trajectory;Noise;Mobile Positioning;Data Fusion;Kalman Filter;Hidden Markov Model;GPS;Electronic Fences},
doi={10.1109/HASE.2012.30},
ISSN={1530-2059},
month={Oct},}
@INPROCEEDINGS{6362954,
author={Zhuang, Yanyan and Tredger, Stephen and Matthews, Chris and McGeer, Rick and Coady, Yvonne},
booktitle={2012 Seventh International Conference on P2P, Parallel, Grid, Cloud and Internet Computing}, title={Distributed Systems in the Wild: The Theoretical Foundations and Experimental Perspectives},
year={2012},
volume={},
number={},
pages={87-94},
abstract={Modernizing experimentation in system-oriented courses such as computer networks and distributed systems is often challenging due to the raw and complex nature of infrastructure testing. In these practical courses, students not only need access to network layers and system kernels, but they often need to reason about consistency issues associated with the distributed nature of these experiments. This paper outlines the pros and cons of redesigning a traditional distributed systems course to incorporate modern experimental facilities for deploying distributed systems, such as Emulab, Seattle and Planet Lab. The possibility of giving students practical and relevant experience coupled with theoretical foundations is explored by considering traditional learning outcomes in the context of new course assignment objectives. A proposed set of experiments, along with their potential pitfalls and shortcomings, provide a basis for an evaluation of the trade-offs of studying distributed systems in the wild.},
keywords={Cloud computing;Servers;Programming;Educational institutions;Bandwidth;Debugging;Topology;Distributed systems;experimental facilities;course design},
doi={10.1109/3PGCIC.2012.32},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6303083,
author={Abduljalil, Fekri M.},
booktitle={2012 IEEE 13th International Conference on Information Reuse Integration (IRI)}, title={A framework for vehicular accident management using wireless networks},
year={2012},
volume={},
number={},
pages={727-729},
abstract={In this paper, a new framework for vehicular accident management using wireless networks is proposed and discussed. We present integrated wireless network architecture for next generation all IP wireless networks which supports our framework. In this architecture we show that different wireless Networks can provide Internet access for moving vehicles. A new distributed system is developed to manage vehicle accidents (anywhere, anytime) such that it communicates accident information with traffic police department, hospital, and Insurance Company. The proposed system is implemented using a test bed. Then the results are discussed.},
keywords={Vehicles;Accidents;Computers;Wireless networks;Servers;Internet;Wireless Networks;Accident Management;Vehicular Ad hoc Networks},
doi={10.1109/IRI.2012.6303083},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6303291,
author={Lin, Xiaojin and Wu, Beibei and Zhang, Junjun and Niu, Chenhui},
booktitle={IEEE PES Innovative Smart Grid Technologies}, title={Design and development of a mobile LVRT testing device},
year={2012},
volume={},
number={},
pages={1-4},
abstract={A new type LVRT test device, basing on power electronics technology, has been developed. Compared with traditional reactor VGS, this device overcomes many drawbacks, such as bulky size, difficult to control precision, and difficult to continuously testing. Considering the characteristics of distributed system accessing to the grid by photovoltaic power station, the designed testing platform has the capability of flexible and mobile performance. In this paper, working principle, hardware and software structures, and features have been introduced. Meanwhile, Testing verification and data analysis also have been conducted in laboratory. Based on those procedures, the device has been verified for having excellent effectiveness and practicality. Over that period, the device has been applied and tested on site in the Qinghai, which had got the ideal result as well as in laboratory.},
keywords={Testing;Photovoltaic systems;Voltage fluctuations;Voltage control;Power systems;photovoltaic power station;LVRT;voltage dip;on-site testing;mobile testing},
doi={10.1109/ISGT-Asia.2012.6303291},
ISSN={2378-8542},
month={May},}
@INPROCEEDINGS{6299348,
author={Grzech, Adam and Juszczyszyn, Krzysztof and Swiatek, Pawel and Mazurek, Cezary and Sochan, Arkadiusz},
booktitle={2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing}, title={Applications of the Future Internet Engineering Project},
year={2012},
volume={},
number={},
pages={635-642},
abstract={The paper gain is to present at the glance applications proposed, deployed and tested within Future Internet Engineering (FIE) project. The elaborated applications utilize the general concept of IP Multimedia Subsystem (IMS) according to which the IMS core servers (Parallel Internets) are treated as a docking station for any kind of application servers. It also means that the applications may be running as long as IMS provides standardized and extended interfaces. The mentioned applications have been selected as representative for process-oriented business organizations characterized by service- and process-oriented models and implementation (SOA, Web Services, Semantic Web, Web of data, content- and context aware applications, etc.) based on service-oriented architectures, pervasive computing and communication-enabled applications paradigms. In the applications' architectures complex, "autonomic" self-organizing distributed systems/workflows with autonomous components are specified, realized, simulated and evaluated as a based on active component technologies. Application-driven content- and context-aware networks support process- and service-oriented processes. A combination of the above mentioned models (the push and pull model), architecture and approaches are possible and attractive from research, application and business perspectives. All the shortly presented applications may be considered as Over-The-Top (OTT) applications which effectively commoditize intelligent network services.},
keywords={Internet;Streaming media;Monitoring;Quality of service;Multimedia communication;Security;Future Internet;content and context aware applications;service-oriented architecture;pervasive computing;communication-enabled applications;service centric architectures},
doi={10.1109/SNPD.2012.61},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6274170,
author={Shi, Ruisheng and Zhang, Yang and Chen, Junliang and Cheng, Bo and Qiao, Xiuquan and Wu, Budan},
booktitle={2012 IEEE Ninth International Conference on Services Computing}, title={Summary Instance: Scalable Event Priority Determination Engine for Large Scale Distributed Event-Based System},
year={2012},
volume={},
number={},
pages={400-406},
abstract={With the advent of ubiquitous sensor-rich environments and location-based services, distributed event-based systems based on the publish/subscribe communication paradigm are becoming very important and crucial. In many large-scale distributed mission critical areas, publish/subscribe systems must support a large number of geographically distributed publishers and subscribers and ensure real-time event data to be delivered timely. The concern is that a large number of low priority events may clog the channel thereby causing high priority events to get delayed. The challenge raised for the event-based middleware in large-scale distributed system is that event priority determination engine must be efficient and scalable in term of priority rule size and event throughputs. Checking every rule for priority on-the-fly is not friendly to the cache. This paper proposed an innovative approach based on Bloom filter technique. This approach is cache friendly and the online computation time of event priority determination is independent of the rule size. This approach is based on two ideas: make the online queries as simple as possible and exploit the power of cache on the broker. The rule instantiation engine (RIE) is made offline while online part is the priority determination engine (PDE). A Bloom filter data structure is used by RIE to store the rule instances and their priorities. The complex rule evaluation is reduced to set membership testing as queries on Bloom filters. The PDE makes the querying simplified by event discretization. Results are then cached in the broker caches. Finally, we have an analysis on the system complexity and some open issues. The further evaluation with prototype experiments are under development and the results may be found in our later publication.},
keywords={Engines;Vectors;Filtering theory;System performance;Handheld computers;Radiation detectors;Data structures;Publish/Subscibe system;Distributed event-based system;Rule matching;Bloom Filter;Priority Determination Engine;Cache friendly;Rule instantiation;Event signature generation;Event discretizaiton},
doi={10.1109/SCC.2012.44},
ISSN={},
month={June},}
@INPROCEEDINGS{6280213,
author={Patil, Prithviraj and Pawar, Sunil},
booktitle={World Congress on Internet Security (WorldCIS-2012)}, title={Remote agent based automated framework for threat modelling, vulnerability testing of SOA solutions and web services},
year={2012},
volume={},
number={},
pages={127-131},
abstract={Web services are a widely touted technology that aims to provide tangible benefits to both business and IT. Their increasing use in the enterprise sector, for the integration of distributed systems and business critical functions, dictates the need for diverse security assurances. Existing security frameworks do provide comprehensive security testing, but are not flexible enough to handle complex, user defined threat scenarios. This paper identifies and details an approach for providing an automated mechanism, which has the capability to allow users to create their own complex threat scenarios and test them against highly distributed web services. This mechanism provides the user with the tools and information necessary to generate and implement user defined security tests. This mechanism should however be considered only as a user driven extension to existing web service security testing frameworks.},
keywords={Security;Testing;Simple object access protocol;Java;Tutorials;Monitoring;web services;security testing;extensible automated framework;user driven test scenarios},
doi={},
ISSN={},
month={June},}
@INPROCEEDINGS{6274055,
author={Li, Hua and Xing, Yi and Xue, Yu and Ye, Xinming},
booktitle={2012 IEEE Eighth World Congress on Services}, title={Study of Interaction Property Testing of Distributed System Based on Petri Net},
year={2012},
volume={},
number={},
pages={227-233},
abstract={Interaction property is a natural feature of many distributed systems which finish their work on network. But most of them face state explosion when the test work is analyzed and developed with a formal model such as Petri net. In order to avoid such problems, test based on interaction property is considered. It can focus on the interesting features of a system and ignore the other irrelevant part. In this paper, some definitions related to interaction property are given. Petri net is employed to model the IUT to make use of its reachability graph. Furthermore a test sequence generation algorithm is proposed and analyzed in detail. The coverage of a test sequence is discussed according to the interaction property and the corresponding verdict methods are given. During the process of test generation, the feasibility of test execution is simultaneously considered. In order to speed up the test execution, the generated test sequences are classified by the test scenarios. The approach is illustrated on several examples.},
keywords={Testing;Educational institutions;Firing;Peer to peer computing;Complexity theory;Computer science;Electronic mail;interaction property;test;Petri net},
doi={10.1109/SERVICES.2012.40},
ISSN={2378-3818},
month={June},}
@INPROCEEDINGS{6270794,
author={Rivière, Etienne},
booktitle={2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops PhD Forum}, title={Simplifying Hands-On Teaching of Distributed Algorithms with SPLAY},
year={2012},
volume={},
number={},
pages={1311-1316},
abstract={Teaching distributed algorithms using a learning-by-doing approach is usually associated with a slow and cumbersome learning process for students. In order to test and evaluate even simple protocols, students need to learn how to set up and operate a test bed, and to write scripts for deploying, running their program and finally retrieving some logs for parsing and observation of the results. Moreover, the amount of code required in languages such as C# or Java for implementing even a simple protocol is often one order of magnitude more than the length of the protocol pseudo-code description as discussed in class or in research papers. Nevertheless, teaching distributed algorithms and protocols using real deployed code on real conditions is highly desirable and can not always be replaced by the use of simulators. We present in this paper our experience of using SPLAY, a distributed systems evaluation framework that greatly simplifies the work of both instructors and students for hands-on learning of distributed systems. SPLAY simplifies the writing, deployment and observation of distributed algorithms and the management of test environments, narrowing the complexity gap between pseudo-code descriptions and executable implementations. In addition, SPLAY's features and the focus kept on the algorithms and their evaluation, allow students to evaluate their protocols in a variety of conditions, by controlling the experiments and their running conditions, or by allowing running them on multiple test beds at no additional costs and with minimal administration complexity.},
keywords={Distributed algorithms;Protocols;Education;Complexity theory;Libraries;Programming;Peer to peer computing;distributed systems;distributed algorithms;education;deployment;evaluation;learning-by-doing},
doi={10.1109/IPDPSW.2012.162},
ISSN={},
month={May},}
@INPROCEEDINGS{6265675,
author={Bolea, Y. and Blesa, J. and Puig, V.},
booktitle={2012 20th Mediterranean Conference on Control Automation (MED)}, title={LPV modelling and identification of an open-flow canal for control},
year={2012},
volume={},
number={},
pages={427-432},
abstract={Open-flow canals are large distributed systems characterized by non-linear, time-varying and dependent with the operating point behavior. These systems can be suitably represented for control by linear parameter varying (LPV) models. In this paper, two ways of obtaining an LPV model for a single reach open canal are proposed and compared: an LPV integrator delay zero (IDZ) model based on hydraulic laws and an LPV model based on identification techniques. The first approach is a white-box modeling approach while the latter is a grey-box methodology using experimental data. Finally, they will be applied and compared in a test-bench canal.},
keywords={Irrigation;Mathematical model;Delay;Logic gates;Least squares approximation;Equations},
doi={10.1109/MED.2012.6265675},
ISSN={},
month={July},}
@INPROCEEDINGS{6258534,
author={Bruneau, Julien and Consel, Charles and OMalley, Marcia and Taha, Walid and Hannourah, Wail Masry},
booktitle={2012 Eighth International Conference on Intelligent Environments}, title={Virtual Testing for Smart Buildings},
year={2012},
volume={},
number={},
pages={282-289},
abstract={Smart buildings promise to revolutionize the way we live. Applications ranging from climate control to fire management can have significant impact on the quality and cost of these services. However, smart buildings and any technology with direct effect on human safety and life must undergo extensive testing. Virtual testing by means of computer simulation can significantly reduce the cost of testing and, as a result, accelerate the development of novel applications. Unfortunately, building physically-accurate simulation codes can be labor intensive. To address this problem, we propose a framework for rapid, physically-accurate virtual testing. The proposed framework supports analytical modeling of both a discrete distributed system as well as the physical environment that hosts it. The discrete models supported are accurate enough to allow the automatic generation of a dedicated programming framework that will help the developer in the implementation of these systems. The physical environment models supported are equational specifications that are accurate enough to produce running simulation codes. Combined, these two frameworks enable simulating both active systems and physical environments. These simulations can be used to monitor the behavior and gather statistics about the performance of an application in the context of precise virtual experiments. To illustrate the approach, we present models of Heating, Ventilating and Air-Conditioning (HVAC) systems. Using these models, we construct virtual experiments that illustrate how the approach can be used to optimize energy and cost of climate control for a building.},
keywords={Mathematical model;Testing;Smart buildings;Heating;Atmospheric modeling;Context;Virtual testing;Smart buildings;HVAC;Pervasive computing},
doi={10.1109/IE.2012.24},
ISSN={},
month={June},}
@INPROCEEDINGS{6249820,
author={Cecílio, José and Martins, Pedro and Costa, João and Furtado, Pedro},
booktitle={2012 IEEE 16th International Conference on Intelligent Engineering Systems (INES)}, title={A configurable middleware for processing in heterogeneous industrial intelligent sensors},
year={2012},
volume={},
number={},
pages={145-150},
abstract={Sensor and Actuator Networks (SAN) are distributed systems deployed to sense, monitor and act on the environment. They may include any of the following: sensors, wiring, embedded system components, wireless communication and backbone servers. Given current technological advances, costs and flexibility advantages of wireless sensor networks technology, wireless SAN (WSAN) deployments are being considered in such environments. WSAN nodes are small embedded devices that provide sensing, acting and (limited) in-network processing capabilities. From a system point-of-view the whole industrial network is a hardware and software heterogeneous distributed system with some computation-capable nodes (WSAN nodes and servers). A significant research challenge is how to provide a single homogeneous interface that allows configuration and operation of any computation-capable node or group of nodes and hides node heterogeneity, as opposed to hand-coding every component of the system for every minor or major configuration change and/or having multiple configuration systems. In this paper we propose a homogeneous configuration approach over heterogeneous WSAN networks. Our approach was successfully tested in an industrial refinery setting, allowing deployers to configure and re-configure the system to meet specific needs.},
keywords={Wireless sensor networks;Protocols;Servers;Middleware;Wireless communication;Programming;Computer architecture},
doi={10.1109/INES.2012.6249820},
ISSN={1543-9259},
month={June},}
@INPROCEEDINGS{6240140,
author={Kim, Youngheum and Lee, Seungyong and Kim, Seungbeom},
booktitle={2012 IEEE 75th Vehicular Technology Conference (VTC Spring)}, title={Virtualization for Testing in Model-Driven Distributed System},
year={2012},
volume={},
number={},
pages={1-5},
abstract={Most automotive companies are facing key challenges of improving quality and reducing time to market with limited resources. One of the methods for achieving key challenges is virtual simulation test in early development phase. The methodology of creating a model-driven distributed system with communication database as well as automatically generating models from specifications is proposed. In order to detect faults of functions and inconsistency of communication signals between specifications and vehicle network, the manual and automated test methods which can verify a model-driven distributed system is represented. This was applied successfully in the production project. The usefulness of the developed methodology and the tool is well ensured through the discovery of defects.},
keywords={Vehicles;Testing;Graphical user interfaces;Manuals;Automation;Time division multiplexing;Databases},
doi={10.1109/VETECS.2012.6240140},
ISSN={1550-2252},
month={May},}
@INPROCEEDINGS{6240667,
author={Chitsaz, Behzad and Razzazi, Mohammadreza},
booktitle={2012 Proceedings of the 35th International Convention MIPRO}, title={Non-blocking roll-forward recovery for message passing systems},
year={2012},
volume={},
number={},
pages={339-344},
abstract={Due to the message transmission between processes in a distributed system, an error in a process might be propagated to another via faulty messages, which causes a global failure. In the absence of built-in fault detection methods, rollback recovery approach is not useful. To avoid error propagation and rollback overhead, roll-forward recovery schemes based on redundancy techniques such as N-Version Programming techniques have been presented. The disadvantage of using these schemes is that they need to block the receiver process until each received message is confirmed by the other version of the process, which results in high time overhead. In the case of variant response latencies, consisting of processing time and message transmission delay, these techniques would not be efficient. In this paper, a non-blocking roll-forward recovery approach with some changes to duplex system is proposed. This approach does not avoid fault propagation. But it performs an additional test using a copy of a failed module version to discover faulty process and replace its state with the fault-free process and mask the faults which are propagated to other processes; so it does not need to block processing or message transmission in any phases of the process. This scheme has lower execution time than existing roll-forward techniques.},
keywords={Fault tolerant systems;Software;Message passing;Redundancy;Delay;Fault detection;Causal Memory;Replicated Distributed Systems;Replication Consistency},
doi={},
ISSN={},
month={May},}
@INPROCEEDINGS{6237709,
author={Ignat, S. and Stancel, E. and Stoian, I.},
booktitle={Proceedings of 2012 IEEE International Conference on Automation, Quality and Testing, Robotics}, title={Support for condition based maintenance Operating equipment performances monitoring},
year={2012},
volume={},
number={},
pages={234-239},
abstract={Reliability and availability requirements for large scale distributed systems implemented in electric power units represent an important factor with impact in energy efficiency. The real time data information and online diagnostic capabilities and ability to set alarms provided by SCADA systems, ensure advanced notification of impending failure. The processing of their database and the decision support system offers better decisions that can prevent the equipment malfunction. Machine operation assistance functions, switchgear diagnosis functions (cumulative breaking current and number of operations, operating time charging time) implemented by electronic protection devices and partial discharging monitoring systems is another effective on-line predictive maintenance test for assessing the functional condition of motors and generators as well as for electrical distribution equipment.},
keywords={Monitoring;Condition monitoring;Availability;Circuit breakers;Vibrations;Predictive maintenance;maintenance management;predictive maintenance;cost-effective condition-based maintenance;decision support;diagnostics;prognostics;hazard function;risk of failure;performance metrics;performance health monitoring},
doi={10.1109/AQTR.2012.6237709},
ISSN={},
month={May},}
@INPROCEEDINGS{6236592,
author={Sweilam, N. H. and Moharram, H. M. and Ahmed, Sameh},
booktitle={2012 8th International Conference on Informatics and Systems (INFOS)}, title={On the parallel iterative finite difference algorithm for 2-D Poisson's equation with MPI cluster},
year={2012},
volume={},
number={},
pages={MM-78-MM-85},
abstract={In this paper, a parallel iterative finite difference method (PIFD) for solving 2D Poisson's equation on a distributed system using Message Passing Interface (MPI) is investigated. This method is based on the domain decomposition method, where the 2D domain is divided into multiple sub-domains using horizontal and/or vertical axis depending on the available number of computer nodes. For interior points Poisson's equation is solved implicitly by four iterative schemes in combining with the boundary conditions. At the interface points of interior subdomains, Poisson's equation is solved by explicit iterative schemes. The proposed approach fulfills the suitability for the implementation on Linux PC cluster through the minimization of inter-process communication by restricting the exchange of data to the interface between the sub-domains. To examine the efficiency and accuracy of the iterative algorithm, several numerical experiments using different number of nodes of the Linux PC cluster are tested. The performance metrics clearly show the benefit of using the proposed approach on the Linux PC cluster in terms of execution time reduction and speedup with respect to the sequential running in a single PC.},
keywords={Computers;Program processors;Poisson equations;Iterative methods;Finite difference methods;Educational institutions;Parallel processing;Domain decomposition;finite difference method;parallel computing;Linux PC cluster workstation},
doi={},
ISSN={},
month={May},}
@INPROCEEDINGS{6233173,
author={Corral-Soto, Eduardo R. and Tal, Ron and Wang, Langyue and Persad, Ravi and Chao, Luo and Solomon, Chan and Hou, Bob and Sohn, Gunho and Elder, James H.},
booktitle={2012 Ninth Conference on Computer and Robot Vision}, title={3D Town: The Automatic Urban Awareness Project},
year={2012},
volume={},
number={},
pages={433-440},
abstract={The 3DTown project is focused on the development of a distributed system for sensing, interpreting and visualizing the real-time dynamics of urban life within the 3D context of a city. At the heart of this technology lies a core of algorithms that automatically integrate 3D urban models with data from pan/tilt video cameras, environmental sensors and other real-time information sources. A key challenge is the "three-dimensionalization" of pedestrians and vehicles tracked in 2D camera video, which requires automatic real-time computation of camera pose relative to the 3D urban environment. In this paper we report preliminary results from a prototype system we call 3DTown, which is composed of discrete modules connected through pre-determined communication protocols. Currently, these modules consist of: 1) A 3D modeling module that allows for the efficient reconstruction of building models and integration with indoor architectural plans, 2) A GeoWeb server that indexes a 3D urban database to render perspective views of both outdoor and indoor environments from any requested vantage, 3) Sensor modules that receive and distribute real-time data, 4) Tracking modules that detect and track pedestrians and vehicles in urban spaces and access highways, 5) Camera pose modules that automatically estimate camera pose relative to the urban environment, 6) Three-dimensionalization modules that receive information from the GeoWeb server, tracking and camera pose modules in order to back-project image tracks to geolocate pedestrians and vehicles within the 3D model, 7) An animation module that represents geo-located dynamic agents as sprites, and 8) A web-based visualization module that allows a user to explore the resulting dynamic 3D visualization in a number of interesting ways. To demonstrate our system we have used a blend of automatic and semi-automatic methods to construct a rich and accurate 3D model of a university campus, including both outdoor and indoor detail. The demonstration allows web-based 3D visualization of recorded patterns of pedestrian and vehicle traffic on streets and highways, estimations of vehicle speed, and real-time (live) visualization of pedestrian traffic and temperature data at a particular test site. Having demonstrated the system for hundreds of people, we report our informal observations on the user reaction, potential application areas and on the main challenges that must be addressed to bring the system closer to deployment.},
keywords={Cameras;Solid modeling;Buildings;Transmission line matrix methods;Data models;Earth;Vehicles;mixed reality;visualization;distributed VR;tracking;camera calibration;recti?cation},
doi={10.1109/CRV.2012.64},
ISSN={},
month={May},}
@INPROCEEDINGS{6225763,
author={Sivieri, Alessandro and Mottola, Luca and Cugola, Gianpaolo},
booktitle={2012 Third International Workshop on Software Engineering for Sensor Network Applications (SESENA)}, title={Drop the phone and talk to the physical world: Programming the internet of things with Erlang},
year={2012},
volume={},
number={},
pages={8-14},
abstract={We present ELIOT, an Erlang-based development framework expressly conceived for heterogeneous and massively decentralized sensing/actuation systems: a vision commonly regarded as the “Internet of Things”. We choose Erlang due to the functional high-level programming model and the native support for concurrency and distributed programming. Both are assets when developing applications as well as system-level functionality in our target domain. Our design enriches the Erlang framework with a custom library for programming sensing/actuation distributed systems along with a dedicated run-time support, while we wipe off unnecessary language and run-time features. We couple the resulting platform with adhoc tools for simulation and testing, supporting developers throughout the development cycle. We assess our solution by implementing three sensor network distributed protocols. A comparison with traditional sensor network programming platforms demonstrates the advantages in terms of terseness of code, readability, and maintainability.},
keywords={Programming;Libraries;Protocols;Sensors;Hardware;Internet;Computational modeling;distributed systems;Internet of Things;programming languages;frameworks},
doi={10.1109/SESENA.2012.6225763},
ISSN={2327-1647},
month={June},}
@INPROCEEDINGS{6223553,
author={Xue, Yu and Xing, Yi and Li, Hua and Ye, Xinming},
booktitle={2012 International Conference on Systems and Informatics (ICSAI2012)}, title={Research on the interactive property testing based on Petri net},
year={2012},
volume={},
number={},
pages={2466-2470},
abstract={The aim of property based testing is to validate an IUT with respect to a target property, to observe whether the property is violated or not. Many distributed systems have a natural feature-Interactive property. But most distributed systems face state explosion when the test work is analyzed and developed with a formal model. In order to avoid such problems, this paper presents a testing method that is Interactive property based testing, which can focus on the interesting features of a system. Petri net is employed to model the IUT, so we can make use of its reachability graph to generate the executable test sequence. Furthermore a test sequence generation algorithm is proposed and analyzed. We also give a verdict method. The feasibility of the approach is illustrated on a BRP case study.},
keywords={Testing;Educational institutions;Computer science;Computational modeling;Firing;Analytical models;Observers;Interactive property;test;Petri net},
doi={10.1109/ICSAI.2012.6223553},
ISSN={},
month={May},}
@INPROCEEDINGS{6220928,
author={Izurieta, Clemente and Poole, Geoffrey and Payn, Robert A. and Griffith, Isaac and Nix, Ryan and Helton, Ashley and Bernhardt, Emily and Burgin, Amy J.},
booktitle={2012 International Conference on Information Science and Applications}, title={Development and Application of a Simulation Environment (NEO) for Integrating Empirical and Computational Investigations of System-Level Complexity},
year={2012},
volume={},
number={},
pages={1-6},
abstract={Network Exchange Objects (NEO) is a new software framework designed to facilitate development of complex natural or built distributed system models, where the system model is represented as a graph, through which currencies (e.g., coding information) flux. This paper introduces "system-level hypothesis (SLH) testing" as a form of computational thinking that will drive integration of computational and empirical sciences to promote efficient, self- correcting inquiry into the operations and behavior of complex systems. To demonstrate NEO, we examine the problem of maximizing the productivity of a software development organization by measuring growth in the total lines of code (LOC) contributed by developers. We develop a software framework (NEO) that allows rapid creation of model variants representing alternative SLHs. NEO is designed to investigate systems we describe as "complex adaptive hierarchical networks" (CAHNs - complex systems represented as networks that route and store multiple interactive currencies). Models built atop NEO, are organized collections of individual values (model variables) and algorithms (model logic). Modelers systematically combine algorithms to create alternative model formulations at runtime. Thus, NEO is a simulation framework that can be used in any domain of expertise, where systems are represented as interdependent entities that store and flux multiple currencies.},
keywords={Organizations;Heuristic algorithms;Mathematical model;Complexity theory;Object oriented modeling;Topology;Biological system modeling},
doi={10.1109/ICISA.2012.6220928},
ISSN={2162-9048},
month={May},}
@INPROCEEDINGS{6217441,
author={Tom´s, Luis and Caminero, Blanca and Carrión, Carmen},
booktitle={2012 12th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (ccgrid 2012)}, title={Improving Grid Resource Usage: Metrics for Measuring Fragmentation},
year={2012},
volume={},
number={},
pages={352-359},
abstract={In highly heterogeneous and distributed systems, like Grids, it is rather difficult to provide QoS to the users. As reservations of resources may not always be possible, another possible way of enhancing the perceived QoS is by performing meta-scheduling of jobs in advance, where jobs are scheduled some time before they are actually executed. Thank to this, it is more likely that the appropriate resources are available to execute the job when needed. When using this type of scheduling, fragmentation appears and may become the cause of poor resource utilization. Because of that, some techniques are needed to perform rescheduling of tasks that may reduce the existing fragmentation. To this end, knowing the status of the system is a must. However, how to measure and quantify the existing fragmentation in a Grid system is a challenging task. This paper proposes different metrics aiming at measuring that fragmentation not only at resource level but also taking into account all the resources of the Grid environment as a whole. Finally, a performance evaluation of the proposed metrics over a real test bed is presented.},
keywords={Quality of service;Resource management;Equations;Educational institutions;Proposals;Performance evaluation;Bag of Task;Fragmentation;Grid computing;Meta-Scheduling in Advance;Rescheduling of Jobs},
doi={10.1109/CCGrid.2012.63},
ISSN={},
month={May},}
@INPROCEEDINGS{6210013,
author={Chimisliu, Valentin and Wotawa, Franz},
booktitle={2012 IEEE International Conference on Industrial Technology}, title={Model based test case generation for distributed embedded systems},
year={2012},
volume={},
number={},
pages={656-661},
abstract={As test case creation activities consume an increasing amount of resources allocated to software development projects, the need to automate this task as much as possible becomes more and more stringent. In this article we report on the application of academic test case generation tools in an industrial context. We present an approach to generate test cases from reactive distributed systems specified as asynchronously communicating UML statecharts. We employ two approaches for the generation process. The first one is fully automated and generates test cases aimed at transition coverage. The second one requires the intervention of the tester in order to annotate states and/or transitions partially describing a test scenario. It is the job of the tool to compute test cases pertaining to the specified test scenario.},
keywords={Unified modeling language;Logic gates;Hazards;UML statecharts;LOTOS;test case generation},
doi={10.1109/ICIT.2012.6210013},
ISSN={},
month={March},}
@INPROCEEDINGS{6209967,
author={Faltinski, Sebastian and Niggemann, Oliver and Moriz, Natalia and Mankowski, André},
booktitle={2012 IEEE International Conference on Industrial Technology}, title={AutomationML: From data exchange to system planning and simulation},
year={2012},
volume={},
number={},
pages={378-383},
abstract={The planning, testing and integration of modern automation systems is becoming more and more a bottleneck in the construction of new production facilities. This is due to the facts that plants grow in complexity and that modern automation systems are highly distributed and comprise complex components. To cope with these challenges and to guarantee short implementation times and a small number of errors for the automation systems, modern development processes are needed. Such modern processes can be reduced to four main aspects: (i) A seamless process with corresponding seamless tools, (ii) a high level of model reuse and adaptability, (iii) executable models and early tests, and (iv) a system-wide planing process of the distributed system. Therefore, the established tool landscape with its specialized tools for each discipline of engineering has difficulties to keep up with these trends. The approach presented in this paper implements a development process including the aspects (i) (iv) using the new data exchange format AutomationML. AutomationML serves as an enabling technology and has the potential to change future development processes and may trigger the development of new, better integrated tools.},
keywords={Production facilities},
doi={10.1109/ICIT.2012.6209967},
ISSN={},
month={March},}
@INPROCEEDINGS{6209147,
author={Smith, Karl and Galloway, Jeffrey and Vrbsky, Susan},
booktitle={2012 Ninth International Conference on Information Technology - New Generations}, title={An Empirical Study on the Impact of an Idle Limit in Distributed Systems},
year={2012},
volume={},
number={},
pages={209-213},
abstract={In this paper we empirically investigate the impact of an idle limit on a distributed system. A series of experiments was conducted to examine the effect of introducing an idle limit into the power management system for a cluster. The experiments tested the total power consumption of the cluster, as well as the number of times the nodes were cycled between power states. The idle limit was not proven to have an impact on the total power consumption, however, it was shown to have an impact on the number of power transitions. In some cases the addition of the idle limit was able to reduce the number of power transitions by 80%.},
keywords={Power demand;Schedules;Distributed computing;Green products;Electricity;Hardware;Computers;Distributed Computing;Empirical;Power Management},
doi={10.1109/ITNG.2012.162},
ISSN={},
month={April},}
@INPROCEEDINGS{6200205,
author={Dustmann, Oscar Soria and Sasnauskas, Raimondas and Wehrle, Klaus},
booktitle={2012 IEEE Fifth International Conference on Software Testing, Verification and Validation}, title={Symbolic System Time in Distributed Systems Testing},
year={2012},
volume={},
number={},
pages={893-894},
abstract={We propose an extension of symbolic execution of distributed systems to test software parts related to timing. Currently, the execution model is limited to symbolic input for individual nodes, not capturing the important class of timing errors resulting from varying network conditions. In this paper, we introduce symbolic system time in order to systematically find timing-related bugs in distributed systems. Instead of executing time events at a concrete time, we execute them at a set of times and analyse possible event interleaving son demand. We detail on the resulting problem space, discuss possible algorithmic optimisations, and highlight our future research directions.},
keywords={Testing;Concrete;Wireless sensor networks;Computer bugs;Optimization;Engines;Conferences;Testing;Symbolic Execution;Distributed Systems;Networks},
doi={10.1109/ICST.2012.193},
ISSN={2159-4848},
month={April},}
@INPROCEEDINGS{6189481,
author={Agrawal, Pragati and Rao, Shrisha},
booktitle={2012 IEEE International Systems Conference SysCon 2012}, title={Energy-aware scheduling of distributed systems using cellular automata},
year={2012},
volume={},
number={},
pages={1-6},
abstract={In today's world of large distributed systems, the need for energy efficiency of individual components is complemented by the need for energy awareness of the complete system. Hence, energy-aware scheduling of tasks on systems has become very important. Our work addresses the problem of finding an energy-aware schedule for a given system which also satisfies the precedence constraints between tasks to be performed by the system. We present a method which uses cellular automata to find a near-optimal schedule for the system. The rules for cellular automata are learned using a genetic algorithm. Though the work presented in this paper is not limited to scheduling in computing environments only, the work is validated with a sample simulation on distributed computing systems, and tested with some standard program graphs.},
keywords={Schedules;Job shop scheduling;Genetic algorithms;Automata;Optimal scheduling;Learning automata;Scheduling algorithms},
doi={10.1109/SysCon.2012.6189481},
ISSN={},
month={March},}
@INPROCEEDINGS{6168333,
author={Khatri, Sunil Kumar and Kapur, P.K. and Johri, Prashant},
booktitle={2012 Second International Conference on Advanced Computing Communication Technologies}, title={Flexible Discrete Software Reliability Growth Model for Distributed Environment Incorporating Two Types of Imperfect Debugging},
year={2012},
volume={},
number={},
pages={57-63},
abstract={In literature we have several software reliability growth models developed to monitor the reliability growth during the testing phase of the software development. These models typically use the calendar / execution time and hence are known as continuous time SRGM. However, very little seems to have been done in the literature to develop discrete SRGM. Discrete SRGM uses test cases in computer test runs as a unit of testing. Debugging process is usually imperfect because during testing all software faults are not completely removed as they are difficult to locate or new faults might be introduced. In real software development environment, the number of failures observed need not be same as the number of errors removed. If the number of failures observed is more than the number of faults removed then we have the case of imperfect debugging. Due to the complexity of the software system and the incomplete understanding of the software requirements, specifications and structure, the testing team may not be able to remove the fault perfectly on detection of the failure and the original fault may remain or get replaced by another fault. In this paper, we discuss a discrete software reliability growth model for distributed system considering imperfect debugging that faults are not always corrected/removed when they are detected and fault generation. The proposed model assumes that the software system consists of a finite number of reused and newly developed sub-systems. The reused sub-systems do not involve the effect of severity of the faults on the software reliability growth phenomenon because they stabilize over a period of time i.e. the growth is uniform whereas, the newly developed subsystem does involve. For newly developed component, it is assumed that removal process follows logistic growth curve due to the fact that learning of removal team grows as testing progresses. The fault removal phenomena for reused and newly developed sub-systems have been modeled separately and are summed to obtain the total fault removal phenomenon of the software system. The model has been validated on two software data sets and it is shown that the proposed model fairs comparatively better than the existing one.},
keywords={Debugging;Mathematical model;Equations;Software reliability;Testing;Software systems;Probability Generating Function (PGF);Imperfect Debugging;Fault Removal Rate (FRR);Distributed Development Environment;Logistic removal rate},
doi={10.1109/ACCT.2012.54},
ISSN={2327-0659},
month={Jan},}
@INPROCEEDINGS{6149190,
author={Clarke, Andrew and Steele, Robert},
booktitle={2012 45th Hawaii International Conference on System Sciences}, title={Secure and Reliable Distributed Health Records: Achieving Query Assurance across Repositories of Encrypted Health Data},
year={2012},
volume={},
number={},
pages={3021-3029},
abstract={Future health information system architectures will intrinsically include distributed systems and data repositories across multiple organizations. As such it will become more important to provide a high level of query quality assurance for the organizations utilizing these distributed and shared data repositories. Query assurance is defined as the data source accurately responding to queries by meeting the requirements of correctness, completeness and freshness. Secure and private health information is a necessity and as one of the significant threats to this security is from insider activities, it will often be desirable that electronic health information be stored in an encrypted format to provide data confidentiality. Providing data confidentially and query assurance within the same approach will be a necessity, while simultaneously ensuring the usability of the health information is not substantially diminished. In this paper, we present a query assurance model that implements the three requirements of query assurance across sources of searchable encrypted data. Further, we consider the issue of freshness and data persistence in a multiple data-owner environment. This is a novel contribution to query assurance and one driven by and increasingly important in the specific context of emerging distributed health information systems. The approach is tested against a large dataset of Continuity of Care Records (CCR) in a key-value store and evaluation results are presented.},
keywords={Distributed databases;Encryption;Probabilistic logic;Organizations;Information systems;query assurance;searchable encryption;health data},
doi={10.1109/HICSS.2012.515},
ISSN={1530-1605},
month={Jan},}
@ARTICLE{6112667,
author={Lim, Sung-Hun and Kim, Jin-Seok and Kim, Myong-Hyon and Kim, Jae-Chul},
journal={IEEE Transactions on Applied Superconductivity}, title={Improvement of Protection Coordination of Protective Devices Through Application of a SFCL in a Power Distribution System With a Dispersed Generation},
year={2012},
volume={22},
number={3},
pages={5601004-5601004},
abstract={The introduction of various dispersed generation (DG) in a power distribution system has caused the short-circuit current to increase, which can make the operation of the protective device deviate from its original set value. Especially, the increase of a short-circuit current in a power distribution system due to the increase of the DG is expected to be required for the related power utilities such as the circuit breaker to be replaced with a larger one. Among the countermeasures to solve the short-circuit problem in a power distribution system considering the increase of the DG, the superconducting fault current limiter (SFCL) has been noticed as one of the promising devices. In this paper, the protection coordination of the protective de- vices due to the application of a SFCL in a power distribution system where the DG was introduced into its bus line was analysed. The experimental circuit to simulate the DG and the protective de- vices was designed and realized by using power electronic switches. The short-circuit tests for the power distributed system assembled with the DG, the protective devices and the SFCL were carried out. Through the analysis on the results from the short-circuit tests, the application of the SFCL with its proper resistance amplitude in a power distribution system with the DG could be confirmed to be contributed to the improvement of the protection coordination of the protective devices.},
keywords={Power distribution;Resistance;Fault current limiters;Circuit faults;Circuit breakers;Voltage control;Relays;Dispersed generation (DG);protection coordination;protective device;short-circuit current;superconducting fault current limiter (SFCL)},
doi={10.1109/TASC.2011.2181930},
ISSN={1558-2515},
month={June},}
@ARTICLE{6101561,
author={Lim, Sung-Hun and Kim, Jae-Chul},
journal={IEEE Transactions on Applied Superconductivity}, title={Analysis on Protection Coordination of Protective Devices With a SFCL Due to the Application Location of a Dispersed Generation in a Power Distribution System},
year={2012},
volume={22},
number={3},
pages={5601104-5601104},
abstract={The increase of various dispersed generations (DGs) such as wind power and solar cell in a power distribution system has been reported to increase the short-circuit current. In addition, the introduction of the DG in a power distribution system is expected to affect the operation of the protective device as well. The superconducting fault current limiter (SFCL), which has been continuously studied for the reduction of the circuit breaker's power burden, recently starts to be noticed as effective method to solve the fault current problem due to the introduction of the DG. Since the fault current limiting operation of the SFCL and the operations of the protective devices such as the over-current relay or the recloser are affected by the application location of the DG, the study on the protection cooperation of the protective devices with the SFCL considering the application location of the DG in a power distribution system is firstly required. In this paper, the protection coordination of the protective devices with a SFCL due to the application location of a DG in a power distribution system was analyzed. The experimental circuit to simulate the DG and the protective devices was designed and realized by using the power electronic switches. With the power distributed system assembled with the DG, the protective devices and the SFCL, the short-circuit tests were carried out. The lower resistance generation of the SFCL due to the decrease of the feeder current in case that the DG was applied into the middle point of the fault feeder was analyzed to cause the fault current not to decrease, which the lock-out operation of the recloser was confirmed to be less delayed compared to other application locations of the DG in a power distribution system.},
keywords={Power distribution;Resistance;Circuit faults;Fault currents;Optical character recognition software;Fault current limiters;Limiting;Dispersed generation (DG);protection coordination;short-circuit current;superconducting fault current limiter (SFCL)},
doi={10.1109/TASC.2011.2179509},
ISSN={1558-2515},
month={June},}
@ARTICLE{6095526,
author={Duarte, Elias Procopio and Weber, Andrea and Fonseca, Keiko V. O.},
journal={IEEE Transactions on Parallel and Distributed Systems}, title={Distributed Diagnosis of Dynamic Events in Partitionable Arbitrary Topology Networks},
year={2012},
volume={23},
number={8},
pages={1415-1426},
abstract={This work introduces the Distributed Network Reachability (DNR) algorithm, a distributed system-level diagnosis algorithm that allows every node of a partitionable arbitrary topology network to determine which portions of the network are reachable and unreachable. DNR is the first distributed diagnosis algorithm that works in the presence of network partitions and healings caused by dynamic fault and repair events. Both crash and timing faults are assumed, and a faulty node is indistinguishable of a network partition. Every link is alternately tested by one of its adjacent nodes at subsequent testing intervals. Upon the detection of a new event, the new diagnostic information is disseminated to reachable nodes. New events can occur before the dissemination completes. Any time a new event is detected or informed, a working node may compute the network reachability using local diagnostic information. The bounded correctness of DNR is proved, including the bounded diagnostic latency, bounded startup and accuracy. Simulation results are presented for several random and regular topologies, showing the performance of the algorithm under highly dynamic fault situations.},
keywords={Heuristic algorithms;Testing;Clocks;Network topology;Topology;Partitioning algorithms;Timing;Network reachability;distributed diagnosis;multiprocessor systems;dynamic fault diagnosis;bounded correctness.},
doi={10.1109/TPDS.2011.284},
ISSN={1558-2183},
month={Aug},}
@ARTICLE{6051438,
author={Cassou, Damien and Bruneau, Julien and Consel, Charles and Balland, Emilie},
journal={IEEE Transactions on Software Engineering}, title={Toward a Tool-Based Development Methodology for Pervasive Computing Applications},
year={2012},
volume={38},
number={6},
pages={1445-1463},
abstract={Despite much progress, developing a pervasive computing application remains a challenge because of a lack of conceptual frameworks and supporting tools. This challenge involves coping with heterogeneous devices, overcoming the intricacies of distributed systems technologies, working out an architecture for the application, encoding it in a program, writing specific code to test the application, and finally deploying it. This paper presents a design language and a tool suite covering the development life-cycle of a pervasive computing application. The design language allows us to define a taxonomy of area-specific building-blocks, abstracting over their heterogeneity. This language also includes a layer to define the architecture of an application, following an architectural pattern commonly used in the pervasive computing domain. Our underlying methodology assigns roles to the stakeholders, providing separation of concerns. Our tool suite includes a compiler that takes design artifacts written in our language as input and generates a programming framework that supports the subsequent development stages, namely, implementation, testing, and deployment. Our methodology has been applied on a wide spectrum of areas. Based on these experiments, we assess our approach through three criteria: expressiveness, usability, and productivity.},
keywords={Pervasive computing;Taxonomy;Computer architecture;Programming;Domain specific languages;Computational modeling;Software architecture;Methodology;domain-specific language;generative programming;pervasive computing;toolkit;programming support;simulation},
doi={10.1109/TSE.2011.107},
ISSN={1939-3520},
month={Nov},}
@INPROCEEDINGS{6199258,
author={Xiaohui Zou},
booktitle={Proceedings 2011 International Conference on Transportation, Mechanical, and Electrical Engineering (TMEE)}, title={Design, implementation of the Parallel C Language based on C/S mode in distributed systems},
year={2011},
volume={},
number={},
pages={532-535},
abstract={In this paper, we designed Parallel C Language, a kind of parallel programming Language, to implement parallel computing in distributed systems. It is to add some special identification statements on ANSI C Language. The parallel computing is based on C/S mode, incorporating multithread and RPC(remote procedure call). Multithread is used to cut tasks and concurrent the subtasks, and RPC is used to deploy and execute the subtasks on remote server nodes. The allocation of remote server nodes is dependent on load balancing mechanism. We also implemented the Pre-compiler of this Parallel C Language program, which parses the Parallel C Language program into several RPC application files. The other RPC application files are constructed by RPCGEN. The result of test shows that using this Parallel C Language to program in distributed systems reduces the codes size and makes full use of the system resources and improves efficiency.},
keywords={Servers;Load management;Parallel processing;Protocols;Instruction sets;Programming;Presses;Parallel C Language;Pre-compiler;template;RPC;multithread;RPCGEN;load balancing},
doi={10.1109/TMEE.2011.6199258},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6169591,
author={Dinu, Cristian-Mircea and Pop, Florin and Cristea, Valentin},
booktitle={2011 13th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing}, title={Pattern Detection Model for Monitoring Distributed Systems},
year={2011},
volume={},
number={},
pages={268-275},
abstract={The ever-increasing size, variety and complexity of distributed systems necessitate the development of highly automated and intelligent solutions for monitoring system parameters. In the context of Large Scale Distributed Systems, automatically detecting events and activity patterns will provide self-organization abilities and increase the dependability of these systems. We present in this paper a model for representing a wide variety of patterns in the parallel time series describing the distributed system parameters and states. Based on this model, we outline an application architecture for a system that employs advanced machine learning techniques for detecting and learning patterns in a distributed system with only minimal user input. The application is implemented as an add-on to the highly successful MonALISA monitoring framework for distributed systems. We test and validate the proposed model in real-time using the large amount of monitoring data provided by the MonALISA system. The novelty of this solution consists of the expressiveness of the model and the advanced automated data analysis for pattern learning and recognition in a long-time monitored system.},
keywords={Program processors;Monitoring;Time series analysis;Feature extraction;Shape;Computer architecture;Machine learning;Large-Scale Distributed Systems;Pattern Detection;Machine Learning;Monitoring;Resource Allocation;MonALISA},
doi={10.1109/SYNASC.2011.22},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6158278,
author={Li Zhang and Yinghui Chen and Fan Tang and Xiong Ao},
booktitle={2011 6th International ICST Conference on Communications and Networking in China (CHINACOM)}, title={Design and implementation of cloud-based performance testing system for web services},
year={2011},
volume={},
number={},
pages={875-880},
abstract={Nowadays testing plays an important role in software development process. While software testing is expensive and time-consuming, sufficient testing is hard, especially for a distributed system using web services technique in the real circumstance. The development of cloud computing provides us new ideas to solve these testing problems. To address these issues, we propose a framework which integrates cloud computing and performance testing technologies. In this paper, first we present the architecture of the cloud-based performance testing system for web services (CPTS), which is a portable, extensible and easy-to-use framework for generating and submitting test workloads to computing clouds. Then, we show the process how to use CPTS to run a performance test and present the concept of dynamic migration in CPTS. Finally, we present our experiences with CPTS in Amazon EC2. We found that the CPTS allows a user to easily set up and test a web services system on the cloud and improve test effectively.},
keywords={Servers;Testing;Cloud computing;Monitoring;Dispatching;Dynamic scheduling;cloud computing;performance testing;web services;dynamic migration;virtual machine},
doi={10.1109/ChinaCom.2011.6158278},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6155967,
author={Liu, Ziluan and Jin, Yuehui and Cui, Yidong and Wang, Qiyao},
booktitle={2011 4th IEEE International Conference on Broadband Network and Multimedia Technology}, title={Design and implementation of a line simplification algorithm for network measurement system},
year={2011},
volume={},
number={},
pages={412-416},
abstract={This paper is based on an existing distributed system TRAK for network performance measurement. In TRAK system, SVG is used to draw a series of result line charts, however, it's impossible to display all the data in just one single webpage. In this paper we introduce an improved line simplification algorithm to reduce the result data set without changing the perceptual characteristics of the result line as much as possible, and retain the special characteristics of network performance measurement. Moreover, when the result charts are partial magnified, we design an anti-simplification procedure to add details to that. This paper takes One-way Delay result chart for example to introduce these procedures in detail, meanwhile we carry out experimental tests to evaluate the improved algorithm and finally illustrate that the improved line simplification algorithm is both computationally efficient and keep the characteristics of the original line well.},
keywords={Algorithm design and analysis;Delay;Memory management;Vectors;Area measurement;Equations;Computers;Line Simplification;Ramer-Douglas-Peucker algorithm;angular selection;partial magnification},
doi={10.1109/ICBNMT.2011.6155967},
ISSN={},
month={Oct},}
@INPROCEEDINGS{6154331,
author={Musico, P. and Bellini, V. and Capogni, M. and Cisbani, E. and Colilli, S. and De Leo, R. and De Oliveira, R. and De Persio, F. and De Smet, V. and Fratoni, R. and Frullani, S. and Giuliani, F. and Gricia, M. and Librizzi, F. and Lucentini, M. and Mammoliti, F. and Meddi, F. and Minutoli, S. and Noto, F. and Perrino, R. and Santavenere, F. and Sutera, C. and Urciuoli, G. M.},
booktitle={2011 IEEE Nuclear Science Symposium Conference Record}, title={Hybrid silicon μstrip and GEM tracker for JLab hall-a high luminosity experiments},
year={2011},
volume={},
number={},
pages={1306-1308},
abstract={A new hybrid silicon μstrip and large area GEM (Gas Electron Multiplier) tracker is under development for the upcoming high luminosity (up to 1039 /s/cm2) experiments at the Hall-A of the JLab 12 GeV electron beam facility. The system consists of 2 small 10×20 cm2 silicon planes placed near the scattering chamber and 18 40×50 cm2 GEM modules that form larger chambers with variable active area depending on the experimental needs. Rather general purpose readout electronics has been designed for both detectors and can be adopted in other equipment. It consists of two active components: front-end cards, directly connected to the detector channels and a multi-purpose digitizer board (MPD). The front-end is based on existing 128 channels APV25-S1 chip developed in the framework of LHC experiments. The MPD handles 16 front-end cards (for a total of 2048 channels) and can be used in VME environments (also VME64x or VXS). It also provides: optical, Ethernet, USB. These resources permit to use MPD cards in different frameworks, ranging from small bench-top to large on-detector distributed systems. The GEM project is part of the CERN/RD51 collaboration activities. The system has been beam tested in late 2010 and 2011, some results are presented here.},
keywords={Soldering;Detectors;Protocols;SDRAM;Geometry},
doi={10.1109/NSSMIC.2011.6154331},
ISSN={1082-3654},
month={Oct},}
@INPROCEEDINGS{6146859,
author={Pradhan, Buddhadeb and Hemant Reddy, K. and Roy, D. S. and Mohanta, D. K.},
booktitle={2011 International Conference on Recent Trends in Information Systems}, title={Intentional islanding of electric power systems in a grid computing framework: A graph-theoretic approach},
year={2011},
volume={},
number={},
pages={156-160},
abstract={Incorporation of deregulation and increase in renewable sources of generation has changed the nature of existing electric power systems to a more geographically distributed system. This had led to significant challenges towards on-line monitoring and control. Contingency set identification is an essential step in monitoring the power system security level since outage of a few among the combinatorially huge number of power distribution lines can lead to a catastrophe, even blackouts. Intentional islanding is a counter approach. The most critical criterion in intentional islanding is to minimize the real power imbalance within the islands to benefit subsequent restoration. Grid computing is being seen as a viable solution to cater to the changing needs of data-intensive computing, including monitoring and control of power systems. In this paper, a grid service for intentional islanding for power systems protection has been developed and demonstrated on a test bed using IEEE 30 bus test system as a case study. Grid gain 2.0 has been employed as middleware to setup the test bed.},
keywords={Power system stability;Monitoring;Generators;Grid computing;Power transmission lines;Vectors;Grid Computing;Power System Islanding;Graph Partitioning},
doi={10.1109/ReTIS.2011.6146859},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6134806,
author={Bahmanifirouzi, Bahman and Niknam, Taher and Taheri, Seyed Iman},
booktitle={2011 IEEE Power Engineering and Automation Conference}, title={A new evolutionary algorithm for placement of distributed generation},
year={2011},
volume={1},
number={},
pages={104-107},
abstract={The impacts of DG on various aspects of distribution system operation depend highly on location and size of DG. In this paper, a multi-objective optimization algorithm for the siting and sizing of distributed generation is proposed. The objectives consist of minimization costs and losses of distributed system and optimization of voltage profile. This multi-objective optimization is solved by the improved honey bee mating optimization (HBMO) algorithm. HBMO is modified on mating step because of it's converges to local optima. In many cases, these objectives contradict each other and cannot be handled by conventional single or multi objective optimization techniques. For this reason, a fuzzy system is used. This algorithm is executed on a typical 70-bus test system. Results show the proper siting and sizing of DGs are important to improve the voltage profile, reduce costs and losses of distribution system. Accuracy and speed to achieved aims are the most important advantage of this algorithm.},
keywords={Optimization;Turbines;Fuel cells;Heuristic algorithms;Distributed power generation;Minimization;Probability;Distributed generation (DG) placement;multi-objective;honey bee mating optimization},
doi={10.1109/PEAM.2011.6134806},
ISSN={},
month={Sep.},}
@ARTICLE{6078026,
author={Zhang, Fan and Cao, Junwei and Liu, Lianchen and Wu, Cheng},
journal={Tsinghua Science and Technology}, title={Performance improvement of distributed systems by autotuning of the configuration parameters},
year={2011},
volume={16},
number={4},
pages={440-448},
abstract={The performance of distributed computing systems is partially dependent on configuration parameters recorded in configuration files. Evolutionary strategies, with their ability to have a global view of the structural information, have been shown to effectively improve performance. However, most of these methods consume too much measurement time. This paper introduces an ordinal optimization based strategy combined with a back propagation neural network for autotuning of the configuration parameters. The strategy was first proposed in the automation community for complex manufacturing system optimization and is customized here for improving distributed system performance. The method is compared with the covariance matrix algorithm. Tests using a real distributed system with three-tier servers show that the strategy reduces the testing time by 400/0 on average at a reasonable performance cost.},
keywords={Noise level;Noise;Servers;Computational modeling;System performance;Optimization;Noise measurement;distributed systems;performance evaluation;autotune configuration parameters;ordinal optimization;covariance matrix algorithm},
doi={10.1016/S1007-0214(11)70063-3},
ISSN={1007-0214},
month={Aug},}
@INPROCEEDINGS{6131789,
author={McLoughlin, Ian Vince},
booktitle={2011 Second International Conference on Networking and Computing}, title={Virtualized Development and Testing of Embedded Computing Clusters},
year={2011},
volume={},
number={},
pages={17-26},
abstract={Design of customised embedded cluster processing solutions can be difficult for reasons including the large number of unknowns at design time in terms of quantity and type of processors, interconnectivity characteristics and code partitioning, and for reasons relating to inaccessible hardware for software developers. Many of these issues are faced in conventional development projects for generic embedded systems, or for generic distributed systems, but the issues are exacerbated when both factors are combined. This paper identifies and characterises the issues relating to the development of customised embedded cluster processing systems in general, before focussing on a specific example of a space-borne cluster computer using ARM processors running embedded Linux. This early example is representative of a growing number of systems in the pervasive and ambient computing fields, and illustrates the difficulty of co-developing hardware and software for specific tasks. One solution to many of the issues faced in conducting development tasks using such platforms is the use of distributed virtualization to simulate the final design. This paper presents an ARM cluster simulator, constructed using QEMU and virtual networking, used for the development and validation of distributed embedded processing tasks on a cluster-type architecture. As embedded clusters, embedded pervasive networks, and embedded clouds become more popular, such a virtualized simulation system can provide a useful development and testing resource.},
keywords={Hardware;Program processors;Field programmable gate arrays;Computer architecture;Linux;Reliability;Embedded cluster;virtualization;hardware-software codesign;distributed embedded computing},
doi={10.1109/ICNC.2011.13},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6127961,
author={Yang, Zhi and Wu, Budan and Yu, Nan and Yu, Gang and Chen, Junliang},
booktitle={2011 IEEE Asia-Pacific Services Computing Conference}, title={A Three-Step Service Experience Approach with Feedback for Service Provider},
year={2011},
volume={},
number={},
pages={188-194},
abstract={Web Service is one of implementation modes of distributed systems and is becoming the next generation web-based application. As the amount of Web services is increasing, service experience problem is emerging. After Web services are implemented and published, service providers do not know their running effect and user satisfactory degree. So in order to provide better services and improve user experience quality, this paper proposes a three-step service experience approach with feedback to service accessing, in which service experience coefficient is used to measure the running effect of service and user satisfaction degree. At the same time, the test data of the feedback are sent to service providers to create new services or improve services in order to provide better services. With the data of the web site WS-DREAM, the service experience approach to service accessing is designed and implemented. The result indicates that the approach is useful to improve better services.},
keywords={Web services;Indexes;Reliability;Time factors;Quality of service;Multimedia communication;XML;Service Experience;Service Experience Coefficient;Service Accessing},
doi={10.1109/APSCC.2011.47},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6119091,
author={Tangadpalliwar, Snehal and Sandrasegaran, Kumbesan and Raymond, Malcolm and Moitra, Ajanta and Madani, Faisal},
booktitle={2011 IEEE Ninth International Conference on Dependable, Autonomic and Secure Computing}, title={Benchmarking Embedded Devices for Broadband Performance Testing},
year={2011},
volume={},
number={},
pages={321-327},
abstract={Real time monitoring of broadband performance parameters is critical for estimating the user experience of new broadband services like VoIP, IPTV, Gaming and Video. This information is of interest to service providers themselves for efficient network design and maintenance and government regulatory bodies for analyzing ISPs, regions and national benchmarking. A web-based system TRUEE (Tool for Real-time User Experience Estimation) is a distributed system that incorporates independent modules such as standalone measurement devices installed at customer premises, data centers, test servers and web-clients for remote monitoring and management of the system. The focus of this paper is to discuss the process of benchmarking three commercial embedded devices with PC as reference device representing an end user system for accessing broadband services. This work is part of the ongoing development process of TRUEE. This benchmarking process is of significant importance for making an informed decision on the suitability of an embedded device capable of providing desired accuracy and consistency in estimation of the broadband performance parameters. Based on literature review, online forum reviews and cost analysis three devices based on ARM viz. SheevaPlug, Texas Instrument's BeagleBoard-xM and Gumstix Overo are selected for benchmarking. Results show that Marvell's SheevaPlug outperforms the other two devices in accurately measuring the broadband parameters on its network interface.},
keywords={Performance evaluation;Benchmark testing;Broadband communication;Bandwidth;Throughput;Jitter;Linux;Broadband;Performance Testing;Embedded device;Benchmarking;Network Monitoring},
doi={10.1109/DASC.2011.71},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6115162,
author={Skodzik, Jan and Danielis, Peter and Altmann, Vlado and Rohrbeck, Jens and Timmermann, Dirk and Bahls, Thomas and Duchow, Daniel},
booktitle={2011 IEEE 36th Conference on Local Computer Networks}, title={DuDE: A distributed computing system using a decentralized P2P environment},
year={2011},
volume={},
number={},
pages={1048-1055},
abstract={The P2P-based system for the distributed computing of statistics called DuDE is presented. High scalability and failure resilience features of P2P are exploited to achieve a high- performance distributed system, which avoids the bottlenecks of a centralized computing system. To ensure high data availability, a sophisticated algorithm for distributed data storage is integrated. Furthermore, an algorithm for global peer discovery is presented, which allows for finding all data assigned to peers without the need for a central instance. For the realization of DuDE, common working stages of distributed computing are extended to enable a highly scalable computing system based on P2P technology. Generated results from a test system show a nearly perfect linear speedup for distributed computing as well as high processor and memory relief compared to a centralized solution.},
keywords={Peer to peer computing;Distributed databases;Protocols;Availability;Buffer storage;P2P;DHT;Kademlia;Distributed Computing;Distributed Storing},
doi={10.1109/LCN.2011.6115162},
ISSN={0742-1303},
month={Oct},}
@INPROCEEDINGS{6092700,
author={Wang, Zhuo and Feng, Xiao-ning},
booktitle={2011 International Conference on Virtual Reality and Visualization}, title={A Cooperative Simulation System for AUV Based on Multi-agent},
year={2011},
volume={},
number={},
pages={109-114},
abstract={Advances in distributed system technology have created new possibilities for innovation in simulation and the creation of new tools and facilities that could improve the productivity of simulation. This paper describes a multi agents based collaborative simulation system for autonomous undersea vehicles. Multi-agent and the collaborative module of agents are used to resolve the problem of the existing simulation system. The detail of every agent in the system is described. In addition, the collaboration of agents and the decision rules are also introduced. The paper then presents results of autonomous underwater vehicle (AUV) simulation tests on the system.},
keywords={Computational modeling;Data models;Collaboration;Synchronization;Sonar navigation;Mathematical model;Load modeling;AUV;software structure;collaborative simulation;multi agents},
doi={10.1109/ICVRV.2011.48},
ISSN={},
month={Nov},}
@INPROCEEDINGS{6076775,
author={Yabandeh, Maysam and Anand, Abhishek and Canini, Marco and Kostic, Dejan},
booktitle={2011 IEEE 30th International Symposium on Reliable Distributed Systems}, title={Finding Almost-Invariants in Distributed Systems},
year={2011},
volume={},
number={},
pages={177-182},
abstract={It is notoriously hard to develop dependable distributed systems. This is partly due to the difficulties in foreseeing various corner cases and failure scenarios while implementing a system that will be deployed over an asynchronous network. In contrast, reasoning about the desired distributed system behavior and the corresponding invariants is easier than reasoning about the code itself. Further, the invariants can be used for testing, theorem proving, and runtime enforcement. In this paper, we propose an approach to observe the system behavior and automatically infer invariants which reveal implementation bugs. Using our tool, Avenger, we automatically generate a large number of potentially relevant properties, check them within the time and spatial domains using traces of system executions, and filter out all but a few properties before reporting them to the developer. Our key insight in filtering is that a good candidate for an invariant is the one that holds in all but a few cases, i.e., an "almost-invariant". Our experimental results with the XORP BGP implementation demonstrate Avenger's ability to identify the almost-invariants that lead the developer to programming errors.},
keywords={Containers;Routing;Cognition;Inspection;Data structures;Noise measurement;Complexity theory},
doi={10.1109/SRDS.2011.29},
ISSN={1060-9857},
month={Oct},}
@INPROCEEDINGS{6078271,
author={Eleftherakis, George and Kefalas, Petros and Kehris, Evangelos},
booktitle={2011 Federated Conference on Computer Science and Information Systems (FedCSIS)}, title={A methodology for developing component-based agent systems focusing on component quality},
year={2011},
volume={},
number={},
pages={561-568},
abstract={Formal development of agent systems with inherent high complexity is not a trivial task, especially if a formal method used is not accompanied by an appropriate methodology. X-machines is a formal method that resembles Finite State Machines but has two important extensions, namely internal memory structure and functions. In this paper, we present a disciplined methodology for developing agent systems using communicating X-machine agents and we demonstrate its applicability through an example. In practice, the development of a communicating system model can be based on a number of well-defined distinct steps, i.e. development of types of X-machine models, agents as instances of those types, communication between agents, and testing as well as model checking each of these agents individually. To each of the steps a set of appropriate tools is employed. Therefore the proposed methodology utilises a priori techniques to avoid any flaws in the early stages of the development together with a posteriori techniques to discover any undiscovered flaws in later stages. This way it makes the best use of the development effort to achieve highest confidence in the quality of the developed agents. We use this methodology for modelling naturally distributed systems, such as multi-agent systems. We use a generalized example in order to demonstrate the methodology and explain in detail how each activity is carried out. We briefly present the theory behind communicating X-machine agents and then we describe in detail the practical issues related using the same example throughout.},
keywords={Testing;Mathematical model;Computational modeling;Buffer storage;Memory management;Syntactics;Multiagent systems},
doi={},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{6063388,
author={Roalter, Luis and Moller, Andreas and Diewald, Stefan and Kranz, Matthias},
booktitle={2011 Seventh International Conference on Intelligent Environments}, title={Developing Intelligent Environments: A Development Tool Chain for Creation, Testing and Simulation of Smart and Intelligent Environments},
year={2011},
volume={},
number={},
pages={214-221},
abstract={Advances in the fields of distributed computing, wireless sensor networks, ubiquitous computing and many more have facilitated the development of a plenitude of Intelligent Environments (IE). The development processes, underlying tools and methods have however not yet been in the focus of researchers. A significant amount of time has to be spent for building a novel ecology of distributed systems to form an Intelligent Environment to conduct novel research in. A sophisticated middleware can shift the focus from building to research. Yet, a middleware is only one of the many requirements for leveraging the development process of Intelligent Environments. In this paper, we present a complete tool chain to simplify research on Intelligent Environments and to ease their creation. This tool chain consists out of middleware, simulation, visualization and prototyping tools. We start by modeling the physical environment using a CAD software and visualization tool including physical properties. The tool chain is complemented by a middleware and a sophisticated simulation environment allowing e.g. to simulate mobile phone interactions triggering, and the deployment to the real-world events. For developers and researchers of Intelligent Environments, creation and maintenance of Intelligent Environments is significantly improved. Development is speeded up as algorithms, interaction can be already evaluated and tested in simulation before the deployment to the real world.},
keywords={Middleware;Solid modeling;Actuators;Robot sensing systems;Development Process;Smart Spaces;Intelligent Environments;Mobile Devices;Android;ROS;Development;Middleware},
doi={10.1109/IE.2011.43},
ISSN={},
month={July},}
@INPROCEEDINGS{6062900,
author={Chen, Jheng-Cheng and Lai, Chia-Jui and Tsai, Chang-Hsiung},
booktitle={2011 14th IEEE International Conference on Computational Science and Engineering}, title={An Adaptive Diagnostic Algorithm for a Distributed System Modeled by Dual-cubes},
year={2011},
volume={},
number={},
pages={366-370},
abstract={Problem diagnosis in large distributed computer systems and networks is a challenging task that requires fast and accurate inferences from huge data volumes. In this paper, the PMC diagnostic model are considered and the diagnosis approach in this model is based on end-to-end probing technology. A probe is a test transaction whose outcome depends on some of the system's component, diagnosis is performed by appropriately selecting the probes and analyzing the results. In the PMC mode, every computer can execute a probe to test dedicated system's components. The key point of the PMC model is that any test result reported by a faulty probe station is unreliable and the test result reported by fault-free probe station is always correct. The aim of the diagnosis is to locate all faulty components in the system based on the collection of the test results. The fault diagnosis probem in an unstructured network has been shown to be NP-hard. We address an special structured network, namely dual cubes, in this paper. An n-dimensional dual-cube DC(n) is an (n+1)-regular spanning subgraph of a (2n+1)-dimensional hypercube. It uses n-dimensional hypercubes as building blocks and keeps the main desired properties of the hypercube so that it is suitable to be used as a topology of distributed systems. In this paper, we first show that the diagnosability of DC(n) is n+1 and then show that adaptive diagnosis is possible using at most N+n tests for an N-nodes distributed system modeled by dual-cubes DC(n) in which at most n+1 processes are faulty, where N=22n+1 and n ≥ 1.},
keywords={Hypercubes;Probes;Adaptive systems;Computers;Computational modeling;Adaptation models;Copper;Distributed systems;Reliable system design;Problem diagnosis;Adaptive diagnosis;Dual-cubes},
doi={10.1109/CSE.2011.70},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6052290,
author={Pagdon, K. and Gentile, C. and Cohen, A. and Ascione, G. and Baker, G.},
booktitle={2011 IEEE/NPSS 24th Symposium on Fusion Engineering}, title={Production of Tc-99m from naturally occurring molybdenum absent uranium},
year={2011},
volume={},
number={},
pages={1-4},
abstract={Technetium-99m (Tc-99m) is the world's most widely used medical isotope. Current production methods involve the irradiation of highly enriched uranium (HEU) and low enriched uranium (LEU) targets in nuclear reactors. Molybdenum-99 (Mo-99) is then extracted from these targets, which decays to Tc-99m. Currently, this process is centralized, as there are very few companies that manufacture Mo-99. In an effort to eradicate the need for uranium to produce this medical isotope, naturally occurring molybdenum was studied to produce Mo-99. Preliminary testing at Princeton Plasma Physics Laboratory included irradiating naturally occurring Mo coupons for varying amounts of time using a D-T neutron generator producing 1.5×108 n/sec to produce Mo-99. Exploiting this technique, Tc-99m was successfully produced. Proof of principle testing is also underway to confirm the ability to produce Mo-99 from Mo-100 using high-energy gamma rays. Future work consists of creating a mobile device that is able to produce Tc-99m on demand, allowing for a distributed system of the medical isotope in hospitals and radio pharmacies worldwide.},
keywords={Neutrons;Head;Generators;Isotopes;Polyethylene;neutron activation;high energy gamma;medical isotope},
doi={10.1109/SOFE.2011.6052290},
ISSN={2155-9953},
month={June},}
@INPROCEEDINGS{6045966,
author={Ravindran, Kaliappa and Ding, Gwangyu},
booktitle={2011 Sixth International Conference on Availability, Reliability and Security}, title={Observer-based Testing of Distributed Protocols Designed for Harsh Environments},
year={2011},
volume={},
number={},
pages={218-223},
abstract={Testing a distributed protocol P employed in a high assurance system involves the modeling of two distinct aspects: i) the intended goals G(P) to be satisfied by P, and ii) the external environment E(P) under which P operates. In complex distributed systems, a wide range of environment behaviors need to be captured by E(P) and/or diverse application requirements need to be stipulated in G(P). This model-based behavior generation is then used as a reference to compare with the actual behavior of a target system protocol. Our approach embodies two functional elements: first, an 'observer' at the service interface of P to the application that monitors the compliance to the conditions stipulated in G(P), and second, an 'injector' that subjects P to a variety of external disturbances as stipulated in E(P). The latter are either actual occurrences of system-level anomalies during an execution of P or computer-generated error conditions as aided by production rules that depict E(P). The modular decomposition that underlies our 3-tier approach (i.e., the observer and perturber modules interweaving with the target system module) enables a flexible testing of distributed protocols in various operating environments and under various application-level goals. The paper describes a language-based tool for generating events that drive the execution of P, and compare the observed behavior of P with a reference desired behavior.},
keywords={Protocols;Safety;Receivers;Testing;Interface states;Reliability;Complexity theory;Protocol service interface;safety/liveness checks;compositional design;hostile environment;structural complexity},
doi={10.1109/ARES.2011.38},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6042058,
author={Bhateja, Puneet},
booktitle={2011 Fifth International Conference on Theoretical Aspects of Software Engineering}, title={A Tagging Protocol for Asynchronous Testing},
year={2011},
volume={},
number={},
pages={11-18},
abstract={Conformance testing has a rich underlying theory popularly called IOCO-test theory. In the realm of IOCO-test theory, this paper addresses the issue of testing a component of an asynchronously communicating distributed system. Testing a system which communicates asynchronously (i.e., through some medium) with its environment is more difficult than testing a system which communicates synchronously (i.e., directly without any medium). What impedes asynchronous testing is that the actual behavior of the implementation under test (IUT) appears distorted and infinite to the tester. This impediment consequently renders the problem of generating a complete test suite, from the given specification of the IUT, infeasible. To this end, this paper contributes by proposing a tagging protocol which when implemented by the asynchronously communicating distributed system will make the problem of generating a complete test suite, from the specification of any of its component, feasible. Further, this paper describes how to generate the test suite from the given specification of the component.},
keywords={Testing;Tagging;Context;Artificial intelligence;Companies;Routing protocols;Synchronous testing;asynchronous testing;queue context;tagging protocol},
doi={10.1109/TASE.2011.41},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6037742,
author={Wu Shukui and Wang Jiali and Zhao Jian},
booktitle={IEEE 2011 10th International Conference on Electronic Measurement Instruments}, title={FPGA-based high-precision network time synchronization research and implementation},
year={2011},
volume={1},
number={},
pages={329-332},
abstract={This study is based on the 1588 network clock synchronization technology. Clock synchronization of distributed systems is a very important element in the communications, automation, and testing equipment especially for the current LXI Bus. Now there are some synchronization protocols like NTP. But it just has millisecond synchronization accuracy. In many areas such as instrument system, that accuracy is not enough. Therefore, higher synchronization accuracy is required. This paper presents a model for high accuracy of the clock, detailed analysis of the factors that affect clock synchronization and proposed to solve the key technology and solution. These technologies not only changed the clock operation mode, but also access to technology to improve the timestamp. The results show that the synchronization accuracy can be achieved nanosecond.},
keywords={Synchronization;Delay;Accuracy;Clocks;Adders;Field programmable gate arrays;Algorithm design and analysis;IEEE1588;clock synchronization;ABC Clock},
doi={10.1109/ICEMI.2011.6037742},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6022424,
author={Wen, Gaojin and Feng, Shengzhong and Wan, Yanyi and Jiang, Pingchuang and Zhang, Senlin},
booktitle={2011 Seventh International Conference on Natural Computation}, title={Energy-aware application scheduling based on genetic algorithm},
year={2011},
volume={4},
number={},
pages={2050-2053},
abstract={As cloud computing is expected to expand rapidly in the coming years, the large-scale computing and data centers are becoming more and more widespread in the world. Energy consumption of these distributed systems has become a urgent problem and received much attention. Application Scheduling can alleviate this problem by reducing the number of running nodes and effectively maximizing total system efficiency. This paper focuses on scheduling applications in large-scale data centers using genetic algorithm. Specifically, we present the design and implementation of the cost function, the modification of the genetic operators and the choice of the data transition weight. The algorithm is studied via simulation and implementation in a large-scale data center. Test results and performance discussion justify the feasibility of the scheduling algorithm. From the results, we know that the proposed application scheduling method can be useful in practice, which can reduce the running nodes and minimize the cost of data transferred among the nodes efficiently.},
keywords={Genetic algorithms;Virtual machining;Genetics;Scheduling;Scheduling algorithm;Cost function},
doi={10.1109/ICNC.2011.6022424},
ISSN={2157-9563},
month={July},}
@INPROCEEDINGS{6019874,
author={Chen, Jheng-Cheng and Lai, Chia-Jui and Tsai, Chang-Hsiung},
booktitle={2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)}, title={Optimal adaptive parallel diagnosis for a distributed system modeled by dual-cubes},
year={2011},
volume={3},
number={},
pages={1989-1993},
abstract={Problem diagnosis in large distributed computer systems and networks is a challenging task that requires fast and accurate inferences from huge data volumes. In this paper, the PMC diagnostic model are considered and the diagnosis approach in this model is based on end-to-end probing technology. A probe is a test transaction whose outcome depends on some of the system's component; diagnosis is performed by appropriately selecting the probes and analyzing the results. In the PMC mode, every computer can execute a probe to test dedicated system's components. The key point of the PMC model is that any test result reported by a faulty probe station is unreliable and the test result reported by fault-free probe station is always correct. The aim of the diagnosis is to locate all faulty components in the system based on the collection of the test results. The fault diagnosis probem in an unstructured network has been shown to be NP-hard. We address an special structured network, namely dual cubes, in this paper. An n-dimensional dual-cube DC(n) is an (n+1)-regular spanning subgraph of a (2n+1)-dimensional hypercube. It uses n-dimensional hypercubes as building blocks and keeps the main desired properties of the hypercube so that it is suitable to be used as a topology of distributed systems. In this paper, we first show that the diagnosability of DC(n) is n+1 and then show that adaptive diagnosis is possible using at most N +n tests for an N-nodes distributed system modeled by dual-cubes DC(n) in which at most n + 1 processes are faulty, where N = 22n+1 and n ≥ 1.},
keywords={Hypercubes;Probes;Adaptive systems;Computational modeling;Computers;Adaptation models;Fault diagnosis},
doi={10.1109/FSKD.2011.6019874},
ISSN={},
month={July},}
@INPROCEEDINGS{6019579,
author={Ravichandran, K.S. and Saeed Alsheyuhi, Salem Saleh},
booktitle={2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)}, title={FELM based intelligent optimal switching capacitor placement},
year={2011},
volume={1},
number={},
pages={366-371},
abstract={Minimizing the total active power loss and voltage drop are the two important challenges for power transmission and distribution in an unbalanced power flow problem for any power distribution system. This can be achieved by installing the proper sized switching capacitor at proper place in a power distribution system. This paper deals with the design of distributed power systems and optimal switching capacitor placements based on the Fuzzy-Extreme Learning Machine (FELM). The intelligent power automation system requires fast determination of site and size of the switchable capacitors in the system bus through the centralized control units based on the system load. But in case of conventional, ANN and other models available on the literature they are time consuming and less performing under a dynamic environment. To overcome this problem, we propose FELM mechanism to obtain an optimal switching capacitor placement in power distributed system. Fuzzy logic and its inference systems are used in ELM and the resultant FELM is to be used to find the site and size dynamically. Finally, the results are compared with a standard 34-bus test system with other models, with respect to the capacitor placement on the networks, savings and the computational time.},
keywords={Capacitors;Switches;Artificial neural networks;Load flow;Training;Load modeling;Fuzzy Logic;Extreme Learning Machine;Artificial Neural Networks (ANN);Power Flow Problem;Intelligent power automation system component},
doi={10.1109/FSKD.2011.6019579},
ISSN={},
month={July},}
@INPROCEEDINGS{6012656,
author={El Ioini, Nabil},
booktitle={2011 IEEE World Congress on Services}, title={Web Services Open Test Suites},
year={2011},
volume={},
number={},
pages={77-80},
abstract={In modern software solutions vendors are racing to keep pace with their customers' needs for more and more distributed systems enable to enhance collaboration and reduce their time to market. Service Oriented Architecture (SOA) is a promising software methodology that addresses this problem. SOA advocates the integration of loosely coupled systems across platforms. SOA adds a new level of abstraction to the existing stack of technologies and development methodologies, however, it adds also new challenges. The most popular way to implement SOA is using Web Services (WS). WS is an XML technology to integrate loosely coupled systems. The challenge facing WS is testing. From the client prospective, a Web Service is a black box that needs to be tested before being used, mainly to confirm the claims of services' providers about the quality of their services. By calling a WS, we delegate part of our business logic to an external provider to do it for us. Thus, we have no control of what could happen during the execution of that part of the business logic. To this end, many testing approaches and techniques have been proposed in the literature to address various aspects related to WS testing. In our effort to improve WS testing infrastructures, we propose a framework for service integrators to collaborate during Web Services testing by making test suites open to the public and share testing results.},
keywords={Testing;Servers;Communities;Service oriented architecture;XML;Business;Web Services;Testing;Test cases},
doi={10.1109/SERVICES.2011.47},
ISSN={2378-3818},
month={July},}
@INPROCEEDINGS{5985916,
author={Velazco, R. and Foucard, G. and Pancher, Fabrice and Mansour, Wassim and Marques-Costa, G. and Sohier, D. and Bui, A.},
booktitle={2011 12th Latin American Test Workshop (LATW)}, title={Robustness with respect to SEUs of a self-converging algorithm},
year={2011},
volume={},
number={},
pages={1-5},
abstract={Self-convergence is a property of distributed systems, allowing a system, when it was perturbed or badly initialised, to recover a correct operation in a finite number of calculation steps. In this paper is explored the intrinsic robustness of a self converging algorithm with respect to soft errors resulting from SEU (Single Event Upset) phenomena. This study was performed by fault injection using a devoted test platform. A self-converging benchmark program was executed by a LEON3 processor implemented in an FPGA. The low number of observed errors puts in evidence the fault tolerance of the tested algorithm.},
keywords={Self-stabilization;fault injection;SEU;fault tolerance},
doi={10.1109/LATW.2011.5985916},
ISSN={2373-0862},
month={March},}
@INPROCEEDINGS{5982296,
author={Alavieh Sadat Alavizadeh and Afradi, Amirhossein},
booktitle={2011 IEEE 2nd International Conference on Software Engineering and Service Science}, title={Automated method for verification of Stochastic Graph Transformation Systems},
year={2011},
volume={},
number={},
pages={228-234},
abstract={Stochastic Graph Transformation Systems (SGTS) have been introduced to model dynamic distributed systems. To insure correctness of a SGTS, we should employ verification concept. There are different methods for verifying such systems, for example simulation, test, deductive verification and model checking of which the most common is model checking. But most of the researches in the field of model checking come back to construct of a suitable model from SGTS and there is not done a complete research and a practical work in the field of model verification methods. Therefore, in this paper, we have implemented a SGTS to PRISM (a stochastic model checker) translator to automatically verify SGTS.},
keywords={Markov processes;Mathematical model;Mercury (metals);Safety;Analytical models;Indexes;Stochastic graph transformation system;Verification;Model checking;Prism},
doi={10.1109/ICSESS.2011.5982296},
ISSN={2327-0594},
month={July},}
@INPROCEEDINGS{5972342,
author={Baskaran, Santhi and Thambidurai, P.},
booktitle={2011 International Conference on Recent Trends in Information Technology (ICRTIT)}, title={Energy efficient scheduling simulator for distributed real-time systems},
year={2011},
volume={},
number={},
pages={1066-1071},
abstract={Energy consumption is a critical design issue in realtime systems, especially in battery-operated systems. The focus is to provide a scheduling simulator to check if a real-time application meets its temporal constraint while minimizing system energy consumption. With this simulator tool, a real-time application is defined by a set of processors, tasks, shared resources and messages. Simulator provides feasibility tests in the case of multiprocessor and distributed systems. The flexible simulation engine also allows the designer to describe and run simulations of specific real-time systems.},
keywords={Program processors;Real time systems;Processor scheduling;Energy efficiency;Algorithm design and analysis;Power demand;Energy consumption;Dynamic voltage scaling;real-time scheduling;simulation tool;voltage scalable processor},
doi={10.1109/ICRTIT.2011.5972342},
ISSN={},
month={June},}
@INPROCEEDINGS{5961714,
author={Sasnauskas, Raimondas and Dustmann, Oscar Soria and Kaminski, Benjamin Lucien and Wehrle, Klaus and Weise, Carsten and Kowalewski, Stefan},
booktitle={2011 31st International Conference on Distributed Computing Systems}, title={Scalable Symbolic Execution of Distributed Systems},
year={2011},
volume={},
number={},
pages={333-342},
abstract={Recent advances in symbolic execution have proposed a number of promising solutions to automatically achieve high-coverage and explore non-determinism during testing. This attractive testing technique of unmodified software assists developers with concrete inputs and deterministic schedules to analyze erroneous program paths. Being able to handle complex systems' software, these tools only consider single software instances and not their distributed execution which forms the core of distributed systems. The step to symbolic distributed execution is however steep, posing two core challenges: (1) additional state growth and (2) the state intra-dependencies resulting from communication. In this paper, we present SDE - a novel approach enabling scalable symbolic execution of distributed systems. The key contribution of our work is two-fold. First, we generalize the problem space of SDE and develop an algorithm significantly eliminating redundant states during testing. The key idea is to benefit from the nodes' local communication minimizing the number of states representing the distributed execution. Second, we demonstrate the practical applicability of SDE in testing with three sensor net scenarios running Contiki OS.},
keywords={History;Testing;Concrete;Software;Context;MIMICs;Receivers;symbolic execution;sensornet testing},
doi={10.1109/ICDCS.2011.28},
ISSN={1063-6927},
month={June},}
@INPROCEEDINGS{5958800,
author={Rao, Xiang and Wang, Huaimin and Shi, Dianxi and Chen, Zhenbang and Cai, Hua and Zhou, Qi and Sun, Tingtao},
booktitle={2011 IEEE/IFIP 41st International Conference on Dependable Systems and Networks Workshops (DSN-W)}, title={Identifying faults in large-scale distributed systems by filtering noisy error logs},
year={2011},
volume={},
number={},
pages={140-145},
abstract={Extracting fault features with the error logs of fault injection tests has been widely studied in the area of large scale distributed systems for decades. However, the process of extracting features is severely affected by a large amount of noisy logs. While the existing work tries to solve the problem by compressing logs in temporal and spatial views or removing the semantic redundancy between logs, they fail to consider the co-existence of other noisy faults that generate error logs instead of injected faults, for example, random hardware faults, unexpected bugs of softwares, system configuration faults or the error rank of a log severity. During a fault feature extraction process, those noisy faults generate error logs that are not related to a target fault, and will strongly mislead the resulted fault features. We call an error log that is not related to a target fault a noisy error log. To filter out noisy error logs, we present a similarity-based error log filtering method SBF, which consists of three integrated steps: (1) model error logs into time series and use haar wavelet transform to get the approximate time series; (2) divide the approximate time series into sub time series by valleys; (3) identify noisy error logs by comparing the similarity between the sub time series of target error logs and the template of noisy error logs. We apply our log filtering method in an enterprise cloud system and show its effectiveness. Compared with the existing work, we successfully filter out noisy error logs and increase the precision and the recall rate of fault feature extraction.},
keywords={Time series analysis;Noise measurement;Feature extraction;Wavelet transforms;Computer crashes;Complexity theory;Approximation methods;error log;event filtering;fault injection;large scale distributed system},
doi={10.1109/DSNW.2011.5958800},
ISSN={2325-6664},
month={June},}
@INPROCEEDINGS{5960057,
author={Nguyên, Toàn and Trifan, Laurentiu and Désidéri, Jean-Antoine},
booktitle={Proceedings of the 2011 15th International Conference on Computer Supported Cooperative Work in Design (CSCWD)}, title={Resilient workflows for cooperative design},
year={2011},
volume={},
number={},
pages={69-75},
abstract={This paper describes an approach to extend process modeling for engineering design applications with fault-tolerance and resilience capabilities. It is based on the requirements for application-level error handling, which is a requirement for petascale and exascale scientific computing. This complements the traditional fault-tolerance management features provided by the existing hardware and distributed systems. These are often based on data and operations duplication and migration, and on checkpoint-restart procedures. We show how they can be optimized for high-performance infrastructures. This approach is applied on a prototype tested against industrial testcases for optimization of engineering design artifacts.his electronic document is a “live” template. The various components of your paper [title, text, heads, etc.] are already defined on the style sheet, as illustrated by the portions given in this document.},
keywords={Software;Resilience;Hardware;Fault tolerance;Computational modeling;Fault tolerant systems;Optimization;Workflows;fault-tolerance;resilience;distributed systems;process modeling;high-performance computing;engineering design},
doi={10.1109/CSCWD.2011.5960057},
ISSN={},
month={June},}
@INPROCEEDINGS{5955311,
author={Wåhslén, Jonas and Orhan, Ibrahim and Lindh, Thomas},
booktitle={2011 International Conference on Body Sensor Networks}, title={Local Time Synchronization in Bluetooth Piconets for Data Fusion Using Mobile Phones},
year={2011},
volume={},
number={},
pages={133-138},
abstract={This paper presents a method to synchronize the clocks in a Bluetooth piconet from the application layer in a mobile phone. It adapts algorithms for time synchronization of distributed systems and the Internet to Bluetooth networks. The performance issues that cause problems for data synchronization between master and slaves in Bluetooth are highlighted. The tests show that the synchronization error is limited to one sampling time.},
keywords={Synchronization;Mobile handsets;Bluetooth;Clocks;Delay;Wireless sensor networks;Sleep;Time synchronization;mobile phone;wireless sensor networks;Bluetooth},
doi={10.1109/BSN.2011.11},
ISSN={2376-8894},
month={May},}
@INPROCEEDINGS{5951910,
author={Gui, Shenglin and Luo, Lei and Yu, Miao and Xu, Jianhua and Li, Yun},
booktitle={2011 IEEE Ninth International Symposium on Parallel and Distributed Processing with Applications}, title={Schedulability Analysis for Distributed Systems Using Network of Action Automata and Environment Automata},
year={2011},
volume={},
number={},
pages={225-231},
abstract={Recently, several formal approaches have been presented to address the problem of schedulability analysis of real-time systems by some varieties of timed automata, e.g., UPPAAL and TIMES. In this paper, we consider a more general and complicated formal computational model for distributed systems. To analyze the schedulability of tasks within this model by automata theory, we present a model, action automata, which is a class of suspension automata, to describe the execution semantics of tasks, we also define an environment model, environment automata, to describe the arrival patterns of tasks. One main result gives the scheduling policies under which the schedulability can be analyzed by our method correctly. To achieve this result, we translate the schedulability analysis to the reach ability analysis of the network of action automata and environment automata. Therefore, another main result of this paper is that we prove the reach ability of action automata is decidable. Based on these conclusions, we implement a prototype tool for schedulability analysis and test its performance under EDF policy.},
keywords={Automata;Clocks;Analytical models;Synchronization;Suspensions;Computational modeling;Processor scheduling;distributed systems;schedulability analysis;action automata;environment automata},
doi={10.1109/ISPA.2011.29},
ISSN={2158-9208},
month={May},}
@INPROCEEDINGS{5949774,
author={Falcon, Rafael and Almeida, Marcio and Nayak, Amiya},
booktitle={2011 IEEE Congress of Evolutionary Computation (CEC)}, title={Fault identification with binary adaptive fireflies in parallel and distributed systems},
year={2011},
volume={},
number={},
pages={1359-1366},
abstract={The efficient identification of hardware and software faults in parallel and distributed systems still remains a serious challenge in today's most prolific decentralized environments. System-level fault diagnosis is concerned with the detection of all faulty nodes in a set of interconnected units. This is accomplished by thoroughly examining the collection of outcomes of all tests carried out by the nodes under a particular test model. Such task has non-polynomial complexity and can be posed as a combinatorial optimization problem, whose optimal solution has been sought through bio-inspired methods like genetic algorithms, ant colonies and artificial immune systems. In this paper, we employ a swarm of artificial fireflies to quickly and reliably navigate across the search space of all feasible sets of faulty units under the invalidation and comparison test models. Our approach uses a binary encoding of the potential solutions (fireflies), an adaptive light absorption coefficient to accelerate the search and problem-specific knowledge to handle infeasible solutions. The empirical analysis confirms that the proposed algorithm outperforms existing techniques in terms of convergence speed and memory requirements, thus becoming a viable approach for real-time fault diagnosis in large-size systems.},
keywords={Fault diagnosis;Optimization;Fires;Mathematical model;Encoding;Complexity theory;Memory management;fault diagnosis;firefly optimization;swarm intelligence;invalidation model;comparison model},
doi={10.1109/CEC.2011.5949774},
ISSN={1941-0026},
month={June},}
@INPROCEEDINGS{5948613,
author={Frincu, Marc E. and Villegas, Norha M. and Petcu, Dana and Müller, Hausi A. and Rouvoy, Romain},
booktitle={2011 11th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing}, title={Self-Healing Distributed Scheduling Platform},
year={2011},
volume={},
number={},
pages={225-234},
abstract={Distributed systems require effective mechanisms to manage the reliable provisioning of computational resources from different and distributed providers. Moreover, the dynamic environment that affects the behaviour of such systems and the complexity of these dynamics demand autonomous capabilities to ensure the behaviour of distributed scheduling platforms and to achieve business and user objectives. In this paper we propose a self-adaptive distributed scheduling platform composed of multiple agents implemented as intelligent feedback control loops to support policy-based scheduling and expose self-healing capabilities. Our platform leverages distributed scheduling processes by (i) allowing each provider to maintain its own internal scheduling process, and (ii) implementing self-healing capabilities based on agent module recovery. Simulated tests are performed to determine the optimal number of agents to be used in the negotiation phase without affecting the scheduling cost function. Test results on a real-life platform are presented to evaluate recovery times and optimize platform parameters.},
keywords={Monitoring;Dynamic scheduling;Peer to peer computing;Context;Feedback loop;Processor scheduling;self-healing;scheduling platform;distributed systems},
doi={10.1109/CCGrid.2011.23},
ISSN={},
month={May},}
@INPROCEEDINGS{5944591,
author={Wu, T.-F. and Chen, Y.-K. and Yu, G.-R. and Chang, Y-C.},
booktitle={8th International Conference on Power Electronics - ECCE Asia}, title={Design and development of dc-distributed system with grid connection for residential applications},
year={2011},
volume={},
number={},
pages={235-241},
abstract={This paper presents design and development of a dc-distributed system with grid connection for residential applications. The system configuration is first described, including green power generator, energy storage element, dc appliance & equipment, and monitor & control center. For realizing the system, the kernel modules of bi-directional inverter, bi-directional charger/discharger, MPPT and dc appliance have been developed, and they are also introduced in this paper. A virtual house has been built in the CCU campus to demonstrate the system operation, from which test data and the critical issues being worth further study are presented.},
keywords={Inverters;Voltage control;Snubbers;Bidirectional control;Capacitors;Inductance;Batteries;dc-distribution system;grid connection},
doi={10.1109/ICPE.2011.5944591},
ISSN={2150-6086},
month={May},}
@INPROCEEDINGS{5929088,
author={Smith, Sandra and Beaulieu, Alain and Phillips, W. Greg},
booktitle={2011 IEEE International Systems Conference}, title={Modeling and verifying security protocols using UML 2},
year={2011},
volume={},
number={},
pages={72-79},
abstract={Large scale distributed systems often require security protocols to ensure high integrity. We present a modeling approach that uses UML 2 without extensions to support the design, composition and verification of security protocols. The approach assumes a strong threat model, in which an attacker can intercept, modify, and spoof all communications, with the exception of those protected by known-strong encryption. Through a series of models of extensively-studied protocols we demonstrate that the approach allows protocol properties to be accurately represented, and protocols to be automatically tested to detect potential security flaws. The approach benefits from the existing strong tool support for UML 2, allowing automatic generation of protocol implementations from the models.},
keywords={Protocols;Unified modeling language;Security;Object oriented modeling;Servers;Visualization;Software;Security Protocols;UML 2;Visual Modeling;Model Driven Development},
doi={10.1109/SYSCON.2011.5929088},
ISSN={},
month={April},}
@INPROCEEDINGS{5873010,
author={Popescu-Bodorin, N. and Balas, V.E.},
booktitle={2011 6th IEEE International Symposium on Applied Computational Intelligence and Informatics (SACI)}, title={Exploratory simulation of an Intelligent Iris Verifier Distributed System},
year={2011},
volume={},
number={},
pages={259-262},
abstract={This paper discusses some topics related to the latest trends in the field of evolutionary approaches to iris recognition. It presents the results of an exploratory experimental simulation whose goal was to analyze the possibility of establishing an Interchange Protocol for Digital Identities evolved in different geographic locations interconnected through and into an Intelligent Iris Verifier Distributed System (IIVDS) based on multi-enrollment. Finding a logically consistent model for the Interchange Protocol is the key factor in designing the future large-scale iris biometric networks. Therefore, the logical model of such a protocol is also investigated here. All tests are made on Bath Iris Database and prove that outstanding power of discrimination between the intra- and the inter-class comparisons can be achieved by an IIVDS, even when practicing 52.759.182 inter-class and 10.991.943 intra-class comparisons. Still, the test results confirm that inconsistent enrollment can change the logic of recognition from a fuzzified 2-valent consistent logic of biometric certitudes to a fuzzified 3-valent inconsistent possibilistic logic of biometric beliefs justified through experimentally determined probabilities, or to a fuzzified 8-valent logic which is almost consistent as a biometric theory - this quality being counterbalanced by an absolutely reasonable loss in the user comfort level.},
keywords={Iris recognition;Copper;Artificial intelligence;Biological system modeling;Boolean algebra;Safety},
doi={10.1109/SACI.2011.5873010},
ISSN={},
month={May},}
@ARTICLE{5771130,
author={Biteus, Jonas and Frisk, Erik and Nyberg, Mattias},
journal={IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans}, title={Distributed Diagnosis Using a Condensed Representation of Diagnoses With Application to an Automotive Vehicle},
year={2011},
volume={41},
number={6},
pages={1262-1267},
abstract={In fault detection and isolation, diagnostic test results are commonly used to compute a set of diagnoses, where each diagnosis points at a set of components which might behave abnormally. In distributed systems consisting of multiple control units, the test results in each unit can be used to compute local diagnoses while all test results in the complete system give the global diagnoses. It is an advantage for both repair and fault-tolerant control to have access to the global diagnoses in each unit since these diagnoses represent all test results in all units. However, when the diagnoses, for example, are to be used to repair a unit, only the components that are used by the unit are of interest. The reason for this is that it is only these components that could have caused the abnormal behavior. However, the global diagnoses might include components from the complete system and therefore often include components that are superfluous for the unit. Motivated by this observation, a new type of diagnosis is proposed, namely, the condensed diagnosis. Each unit has a unique set of condensed diagnoses which represents the global diagnoses. The benefit of the condensed diagnoses is that they only include components used by the unit while still representing the global diagnoses. The proposed method is applied to an automotive vehicle, and the results from the application study show the benefit of using condensed diagnoses compared to global diagnoses.},
keywords={Road vehicles;Fault location;Fault diagnosis;Distributed algorithms;Distributed algorithms;fault diagnosis;fault location;road vehicles},
doi={10.1109/TSMCA.2011.2147311},
ISSN={1558-2426},
month={Nov},}
@INPROCEEDINGS{5766858,
author={Farruggia, Alfonso and Lo Re, Giuseppe and Ortolani, Marco},
booktitle={2011 IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops)}, title={Detecting faulty wireless sensor nodes through Stochastic classification},
year={2011},
volume={},
number={},
pages={148-153},
abstract={In many distributed systems, the possibility to adapt the behavior of the involved resources in response to unforeseen failures is an important requirement in order to significantly reduce the costs of management. Autonomous detection of faulty entities, however, is often a challenging task, especially when no direct human intervention is possible, as is the case for many scenarios involving Wireless Sensor Networks (WSNs), which usually operate in inaccessible and hostile environments. This paper presents an unsupervised approach for identifying faulty sensor nodes within a WSN. The proposed algorithm uses a probabilistic approach based on Markov Random Fields, requiring exclusively an analysis of the sensor readings, thus avoiding additional control overhead. In particular, abnormal behavior of a sensor node will be inferred by analyzing the spatiotemporal correlation of its data with respect to its neighborhood. The algorithm is tested on a public dataset, over which different classes of faults were artificially superimposed.},
keywords={Wireless sensor networks;Equations;Correlation;Sensitivity;Markov random fields;Probabilistic logic;Algorithm design and analysis;Autonomic Computing;Markov Random Fields;Wireless Sensor Networks},
doi={10.1109/PERCOMW.2011.5766858},
ISSN={},
month={March},}
@INPROCEEDINGS{5765199,
author={Guangyi Chen and Wenfang Xie},
booktitle={International Conference on Information Science and Technology}, title={On a laxity-based real-time scheduling policy for fixed-priority tasks and its non-utilization bound},
year={2011},
volume={},
number={},
pages={7-10},
abstract={Understanding the end-to-end temporal behavior of distributed systems is a fundamental concern in real-time computing. Meeting timing requirements is critical to many computer applications. These real-time systems employ a schedulability test to determine whether each task can meet its deadline. In this paper, we extend Liu and Abdelzaher's work by introducing laxity as the priority of the real-time tasks. A Least-Laxity-First (LLF) scheduling algorithm assigns higher priority to a task with the least laxity, and has been proven to be optimal for uniprocessor systems. We compare this laxity-based scheduling policy with deadline monotonic scheduling (DMS), shortest-job-first scheduling (SJF), and velocity monotonic scheduling (VMS). Simulations show that the proposed laxity-based scheduling policy is a feasible approach for real-time admission control.},
keywords={Real time systems},
doi={10.1109/ICIST.2011.5765199},
ISSN={2164-4357},
month={March},}
@INPROCEEDINGS{5746053,
author={Dam, Martin and Pedersen, Jens Myrup},
booktitle={13th International Conference on Advanced Communication Technology (ICACT2011)}, title={A method for optimizing communication paths in distributed systems},
year={2011},
volume={},
number={},
pages={1342-1347},
abstract={One of the challenges in designing large distributed systems is to optimize the communication and dependencies between modules, where a trade-off between performance and ressource usage often must be made. This paper suggest a simple taxonomy for comparing potential designs, and provides also an algorithm for automatically suggesting good candidates. The algorithm is compared to previous work, where it is shown to be significantly faster, while providing comparable results. The results are also shown to be near-optimal in comparison with a brute-force algorithm which is only tested for small problem instances.},
keywords={Algorithm design and analysis;Complexity theory;Optimization;Delay;Heuristic algorithms;Taxonomy;Generators;Networks;internet;routing;load balanced routing;network testing;topology;traffic control (communication);feedback communication;probabilistic routing},
doi={},
ISSN={1738-9445},
month={Feb},}
@ARTICLE{5703091,
author={El-Hassan, Fadi and Ionescu, Dan},
journal={IEEE Transactions on Parallel and Distributed Systems}, title={SCBXP: An Efficient CAM-Based XML Parsing Technique in Hardware Environments},
year={2011},
volume={22},
number={11},
pages={1879-1887},
abstract={The underlying technologies of web information and distributed systems often require efficient XML parsing. Even though new software-based XML parsing techniques improve XML processing, the verbose nature of XML does not help to achieve the substantial improvements that are desired. In some systems, such as mobile devices, the restricted memory resources exacerbate the problems associated with XML processing. In this paper, we present a novel XML parsing technique-titled SCBXP-that is designed to achieve high performance in hardware-based environments. In addition, the parsing technique provides a natural way of checking for full well formedness and partial validation, thereby taking advantage of our CAM-based architecture and the inherent parallel features of the hardware. Furthermore, the efficiency of XML parsing is maintained even when memory resources are limited. The SCBXP technique architecture makes use of 1) a content-addressable memory that must be configured with a skeleton of the XML document being parsed, 2) a finite state machine that controls FIFOs, in order to align XML data properly, 3) multiple state machines acting on the multilevel nature of XML, and 4) dual-port memory modules. The results of testing the SCBXP technique, implemented on an FPGA, demonstrate that a processing rate of at least 2 bytes of XML data can be performed during each clock cycle.},
keywords={XML;Computer aided manufacturing;Skeleton;Loading;Clocks;Writing;Field programmable gate arrays;XML processing;XML parsing;field programmable gate arrays;content addressable memory.},
doi={10.1109/TPDS.2011.51},
ISSN={1558-2183},
month={Nov},}
@ARTICLE{9216682,
author={Henkel, Jörg},
journal={IEEE Design Test}, title={From the EIC: From Smartphones to Wearable Devices},
year={2020},
volume={37},
number={5},
pages={4-4},
abstract={ This issue of Design&Test focuses on design and test of distributed systems. According to the heterogeneous nature, such systems will need heterogeneous computing resources to satisfy a wide variety of often contradicting design constraints such as low power, which makes designing them a particularly complex task. Besides two surveys, one titled “A Survey on Energy Management for Mobile and loT Devices” and another titled “Dynamic Energy and Thermal Management of Multicore Mobile Platforms: A Survey,” the Guest Editors Umit Y. Ogras, Sudeep Pasricha, Michael Kishinevsky, and Raid Ayoub present us four regular special issue research articles. We would like to thank the Guest Editors for this timely special issue.},
keywords={},
doi={10.1109/MDAT.2020.3016585},
ISSN={2168-2364},
month={Oct},}
@ARTICLE{8648265,
author={Salinas, Edaena},
journal={IEEE Software}, title={Pat Helland on Failure and Resilience in Distributed Systems},
year={2019},
volume={36},
number={2},
pages={140-143},
abstract={Pat Helland of SalesForce talks with show-host Edaena Salinas about failure and resilience in distributed systems. The interview covers the growth of distributed computing on the public cloud, the origin of failure in the manufacturing process of hardware, the lifecycle of a server, why servers need to be replaced every three years, self-healing systems, replication, and the consistency, availability, and partition-tolerance (CAP) theorem. Sections that were not transcribed, due to space limits, cover how mutability makes life harder, append-only data, how electricity costs dominate, “cattle versus pets,” the rise of DevOps automation, testing, testing in production, and monitoring. },
keywords={Interviews;Distributed computing},
doi={10.1109/MS.2018.2883877},
ISSN={1937-4194},
month={March},}
@INPROCEEDINGS{8878477,
author={Han, Danyang and Yu, Jinsong and Song, Yue and Tang, Diyin and Dai, Jing},
booktitle={2019 IEEE AUTOTESTCON}, title={A distributed autonomic logistics system with parallel-computing diagnostic algorithm for aircrafts},
year={2019},
volume={},
number={},
pages={1-8},
abstract={The autonomic logistic system (ALS), first used by the U.S. military JSF, is a new conceptional system which supports prognostic and health management system of aircrafts, including such as real-time failure monitoring, remaining useful life prediction and maintenance decisions-making. However, the development of ALS faces some challenges. Firstly, current ALS is mainly based on client/server architecture, which is very complex in a large-scale aircraft control center and software is required to be reconfigured for every accessed node, which will increase the cost and decrease the expandability of deployment for large scale aircraft control centers. Secondly, interpretation of telemetry parameters from the aircraft is a tough task considering various real-time flight conditions, including instructions from controllers, work statements of single machines or machine groups, and intrinsic physical meaning of telemetry parameters. It is troublesome to meet the expectation of full representing the relationship between faults and tests without a standard model. Finally, typical diagnostic algorithms based on dependency matrix are inefficient, especially the temporal waste when dealing with thousands of test points and fault modes, for the reason that the time complexity will increase exponentially as dependency matrix expansion. Under this situation, this paper proposed a distributed ALS under complex operating conditions, which has the following contributions 1) Introducing a distributed system based on browser/server architecture, which is divided overall system into primary control system and diagnostic and health assessment platform; 2) Designing a novel interface for modelling the interpretation rules of telemetry parameters and the relationship between faults and tests in consideration of multiple elements of aircraft conditions; 3) Proposing a promoted diagnostic algorithm under parallel computing in order to decrease the computing time complexity. what's more, this paper develops a construction with 3D viewer of aircraft for user to locate fault points and presents repairment instructions for maintenance personnels based on Interactive Electronic Technical Manual, which supports both online and offline. A practice in a certain aircraft demonstrated the efficiency of improved diagnostic algorithm and proposed ALS.},
keywords={autonomic logistic;fault diagnosis;prognostics and health management},
doi={10.1109/AUTEST.2019.8878477},
ISSN={1558-4550},
month={Aug},}
@INPROCEEDINGS{8342406,
author={Vain, Jüuri and Kanter, Gert and Srinivasan, Seshadhri},
booktitle={2017 6th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)}, title={Model based testing of distributed time critical systems},
year={2017},
volume={},
number={},
pages={99-105},
abstract={Model-based testing incorporates steps such as test model construction, test purpose specification, test generation, deployment and execution. While the verification has not been traditionally an obligatory part of this process we extend the test development model by introducing model-based techniques, a tool and verification conditions of provably correct test development for time critical distributed systems. We demonstrate how Uppaal Timed Automata models and related tool family supports the development and verification of symbolic tests. Since distributed testing needs additional test deployment effort, we present the test controllability criteria for remote and distributed testing, provide an algorithm of distributing remote tests to improve the test performance and propose a technique of proving the correctness of distributed tests in terms of bisimulation equivalence between the remote and distributed tests.},
keywords={},
doi={10.1109/ICRITO.2017.8342406},
ISSN={},
month={Sep.},}
@INPROCEEDINGS{7371835,
author={},
booktitle={2015 IEEE 21st Pacific Rim International Symposium on Dependable Computing (PRDC)}, title={[Title page i]},
year={2015},
volume={},
number={},
pages={i-i},
abstract={The following topics are dealt with: fault injection; fault monitoring; software aspects; dependable computing; cyber physical systems; distributed computing; sensor networks; safety contracts; dependability modeling; dependability analysis; availability; integrity; resilience; distributed systems; dependable computer architectures; dependable storages; cloud systems; Web systems; testing; verification; recovery and real-time computing.},
keywords={},
doi={10.1109/PRDC.2015.1},
ISSN={},
month={Nov},}
@INPROCEEDINGS{7273301,
author={},
booktitle={2015 IEEE 39th Annual Computer Software and Applications Conference}, title={[Title page i - Part III]},
year={2015},
volume={3},
number={},
pages={i-i},
abstract={The following topics are dealt with: security aspects; computer forensics; software engineering; middleware; cybersecurity; cloud computing; internetworking; medical computing; software quality oriented reuse; lifelong mobile application software maintenance; user centered design and adaptive systems; social services; software cybernetics; Big Data management; Internet of Things; distributed systems; software testing; and data privacy.},
keywords={},
doi={10.1109/COMPSAC.2015.206},
ISSN={0730-3157},
month={July},}
@INPROCEEDINGS{6821074,
author={},
booktitle={2014 Tenth European Dependable Computing Conference}, title={Table of contents},
year={2014},
volume={},
number={},
pages={v-viii},
abstract={The following topics are dealt with: distributed systems; wireless sensor network; online fault and failure management; resilient systems modelling; testing, fault-injection and benchmarking; program compilation; safety-critical systems; space and aeronautics; security of data; and data privacy.},
keywords={},
doi={10.1109/EDCC.2014.10},
ISSN={},
month={May},}
@INPROCEEDINGS{6820830,
author={},
booktitle={2013 IEEE 19th Pacific Rim International Symposium on Dependable Computing}, title={Table of contents},
year={2013},
volume={},
number={},
pages={v-ix},
abstract={The following topics are dealt with: checkpointing and voting; fault injection; software testing; consensus algorithm; error detection and correction; distributed systems; security of data; safety system; and software reliability.},
keywords={},
doi={10.1109/PRDC.2013.4},
ISSN={},
month={Dec},}
@INPROCEEDINGS{6664494,
author={},
booktitle={2013 3rd Eastern European Regional Conference on the Engineering of Computer Based Systems}, title={Table of contents},
year={2013},
volume={},
number={},
pages={v-vii},
abstract={The following topics are dealt with: complex systems modeling; embedded systems; distributed systems; pattern recognition; computer-aided instruction; computer-based training; program testing; object-oriented programming; and mobile computing.},
keywords={},
doi={10.1109/ECBS-EERC.2013.4},
ISSN={},
month={Aug},}
@INPROCEEDINGS{6113864,
author={},
booktitle={2011 IEEE 13th International Symposium on High-Assurance Systems Engineering}, title={[Front cover]},
year={2011},
volume={},
number={},
pages={C1-C1},
abstract={The following topics are dealt with: software testing; software validation; verification; distributed systems; model checking; safety-critical systems; ocean systems; reliability; monitoring; bugs; fault localization; security; and privacy.},
keywords={},
doi={10.1109/HASE.2011.71},
ISSN={1530-2059},
month={Nov},}
@INPROCEEDINGS{6075043,
author={},
booktitle={2011 International Conference on Advances in ICT for Emerging Regions (ICTer)}, title={[Front cover]},
year={2011},
volume={},
number={},
pages={c1-c1},
abstract={The following topics are dealt with: social life networks; location aware queries; sensor networks; distributed systems; overlay networks; machine translation; biometrics computing; image morphing; automated vision-based recognition system; Sri Lankan Tamil sign language finger spelling; mobile interface design; audio environment; visually impaired; pattern independent fiducial marker detection; interactive public display; port decision system; South American sea ports; eTransformation; mTransformation; mobile information systems; mobile service portal; rural Fisher community development; Kenyan digital villages project; data security; forensic investigation; NoSQL query processing system; wireless ad-hoc networks; sensor networks; LEACH protocol; Sybil attack; Jakes channel scheme; UML; software engineering practice; knowledge extraction; semantic Web; Web mining; automated tool; test case generation; basis path testing; bag-of-keypoints approach; Tamil handwritten character recognition; SVM; digital watermarking; on-line broadcasting; ICT enabled childhood education; next generation mobile applications; MOPS; networked shared storage; Hypervisors; AskME; database abstraction; P control algorithm; PID control algorithm; nonholonomic multiagent mobile robots; data retrieval; dynamic data organizing solution; electronic trading systems; buddy-file-system; block level sharing; disk images; virtualization environments; intrinsic plagiarism detection; Kohonen self-organizing maps; Miyaesi; sound programming interface; automatic music transcription; RESTful email system; offline Web-based email client; e-Arogya; e-Health solution; and Sri Lankan government hospitals.},
keywords={},
doi={10.1109/ICTer.2011.6075043},
ISSN={},
month={Sep.},}
